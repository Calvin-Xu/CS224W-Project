{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fCt8t4Z7wp7Z","outputId":"e55dd56b-e3d6-4ec9-cf22-92603c70695a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: torch_geometric in /usr/local/lib/python3.10/dist-packages (2.6.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.11.9)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.10.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n","Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n","Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.6)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n","Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (0.2.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.18.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.8.30)\n","Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n"]}],"source":["!pip install torch\n","!pip install torch_geometric"]},{"cell_type":"code","source":["import os\n","import pickle\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.data import HeteroData\n","from torch_geometric.nn import HGTConv\n","from torch.utils.data import DataLoader\n","import numpy as np"],"metadata":{"id":"EgsdA02Tw21r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def is_colab():\n","    try:\n","        import google.colab\n","        return True\n","    except ImportError:\n","        return False\n","\n","IN_COLAB = is_colab()\n","\n","if IN_COLAB:\n","    from google.colab import drive\n","    drive.mount('/content/drive/')\n","    %cd \"/content/drive/My Drive/Colab Notebooks/CS224W/DRKG Project\"\n","\n","# Load the NetworkX graph\n","if os.path.exists(\"./ml_graph.pkl\"):\n","    G = pickle.load(open(\"./ml_graph.pkl\", \"rb\"))\n","else:\n","    print(\"Graph file not found!\")\n","    # raise FileNotFoundError(\"ml_graph.pkl not found.\")\n","\n","print(\"\\nChecking example edge attributes:\\n\")\n","print(f\"Gene::6434 -> Gene::27429: {G['Gene::6434']['Gene::27429']}\")\n","print(f\"Gene::27429 -> Gene::6434: {G['Gene::27429']['Gene::6434']}\")\n","print(f\"Gene::26092 -> Gene::5577: {G['Gene::26092']['Gene::5577']}\")\n","\n","print(\"Checking example edge attributes:\\n\")\n","print(\n","    f\"Edge from DB00004 to MESH:C063419: {G.get_edge_data('Compound::DB00004', 'Disease::MESH:C063419')}\"\n",")\n","print(\n","    f\"Edge from MESH:C004656 to MESH:C537014: {G.get_edge_data('Compound::MESH:C004656', 'Disease::MESH:C537014')}\"\n",")\n","print(\n","    f\"Edge from DB00997 to DOID:363: {G.get_edge_data('Compound::DB00997', 'Disease::DOID:363')}\\n\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0RR-UTUvxrnl","outputId":"def59956-c7e3-4d1a-cb3a-f37836483448"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n","/content/drive/My Drive/Colab Notebooks/CS224W/DRKG Project\n","\n","Checking example edge attributes:\n","\n","Gene::6434 -> Gene::27429: {'relation': 'STRING::OTHER::Gene:Gene', 'coexpression': 0.042, 'experimentally_determined_interaction': 0.047, 'automated_textmining': 0.692, 'combined_score': 0.694, 'rmsd_score': 'N/A', 'gene_gene_features': array([-0.47708675, -0.43628007,  1.306391  ,  0.5278838 ,  0.        ],\n","      dtype=float32), 'gene_gene_mask': array([0., 0., 0., 0., 1.], dtype=float32)}\n","Gene::27429 -> Gene::6434: {'relation': 'STRING::OTHER::Gene:Gene', 'coexpression': 0.042, 'experimentally_determined_interaction': 0.047, 'automated_textmining': 0.692, 'combined_score': 0.694, 'rmsd_score': 'N/A', 'gene_gene_features': array([-0.47708675, -0.43628007,  1.306391  ,  0.5278838 ,  0.        ],\n","      dtype=float32), 'gene_gene_mask': array([0., 0., 0., 0., 1.], dtype=float32)}\n","Gene::26092 -> Gene::5577: {'coexpression': 'N/A', 'experimentally_determined_interaction': 'N/A', 'automated_textmining': 'N/A', 'combined_score': 'N/A', 'rmsd_score': 57.85609014602144, 'gene_gene_features': array([0.       , 0.       , 0.       , 0.       , 1.1962671],\n","      dtype=float32), 'gene_gene_mask': array([1., 1., 1., 1., 0.], dtype=float32)}\n","Checking example edge attributes:\n","\n","Edge from DB00004 to MESH:C063419: {'relation': 'DRKG::Treats::Compound:Disease', 'treat_type': 'DRUGBANK::treats::Compound:Disease', 'treat_type_onehot': array([0., 0., 0., 0., 1., 0., 0.], dtype=float32)}\n","Edge from MESH:C004656 to MESH:C537014: {'relation': 'DRKG::Treats::Compound:Disease', 'treat_type': 'GNBR::T::Compound:Disease', 'treat_type_onehot': array([0., 0., 0., 0., 0., 0., 1.], dtype=float32)}\n","Edge from DB00997 to DOID:363: {'relation': 'DRKG::Treats::Compound:Disease', 'treat_type': 'Hetionet::CtD::Compound:Disease', 'treat_type_onehot': array([0., 0., 1., 0., 0., 0., 0.], dtype=float32)}\n","\n"]}]},{"cell_type":"code","source":["###################################################\n","# Convert the processed NetworkX graph to a PyG HeteroData\n","###################################################\n","\n","def get_node_type(node):\n","    if node.startswith(\"Compound::\"):\n","        return \"compound\"\n","    elif node.startswith(\"Disease::\"):\n","        return \"disease\"\n","    elif node.startswith(\"Gene::\"):\n","        return \"gene\"\n","    else:\n","        return \"other\"\n","\n","# build node_type_map and node_types_count\n","node_type_map = {}\n","node_types_count = {}\n","for n in G.nodes():\n","    t = get_node_type(n)\n","    if t not in node_types_count:\n","        node_types_count[t] = 0\n","    node_type_map[n] = (t, node_types_count[t])\n","    node_types_count[t] += 1\n","\n","treat_type_dim = 0\n","gene_gene_feat_dim = 0\n","mask_dim = 0\n","\n","for u, v, d in G.edges(data=True):\n","    if \"treat_type_onehot\" in d:\n","        treat_type_dim = len(d[\"treat_type_onehot\"])\n","    if \"gene_gene_features\" in d:\n","        gene_gene_feat_dim = len(d[\"gene_gene_features\"])\n","        mask_dim = len(d[\"gene_gene_mask\"])\n","    if treat_type_dim > 0 and gene_gene_feat_dim > 0:\n","        break\n","\n","hetero_path = \"./ml_graph_heterodata.pkl\"\n","if os.path.exists(hetero_path):\n","    print(f\"Loading existing HeteroData from {hetero_path}...\")\n","    with open(hetero_path, \"rb\") as f:\n","        data = pickle.load(f)\n","else:\n","    print(\"Building new HeteroData object...\")\n","    data = HeteroData()\n","\n","    # assign num_nodes and a dummy x to each node type\n","    for ntype, count in node_types_count.items():\n","        data[ntype].x = torch.empty((count, 0))  # No initial features, to be learned\n","        data[ntype].num_nodes = count\n","\n","    # determine edge type (just preserve the original DRKG relation type)\n","    def get_edge_type(u, v, edata):\n","        rel = edata.get(\"relation\", None)\n","        src_type, _ = node_type_map[u]\n","        dst_type, _ = node_type_map[v]\n","        if rel is None:\n","            rel = \"unknown_relation\"\n","        return (src_type, rel, dst_type)\n","\n","    edge_dict = {}\n","    edge_treat_type_features = {}\n","    edge_gene_gene_features = {}\n","    edge_gene_gene_mask = {}\n","\n","    # collect edges by exact relation type\n","    for u, v, d in G.edges(data=True):\n","        et = get_edge_type(u, v, d)\n","        if et not in edge_dict:\n","            edge_dict[et] = [[], []]\n","\n","        src_idx = node_type_map[u][1]\n","        dst_idx = node_type_map[v][1]\n","        edge_dict[et][0].append(src_idx)\n","        edge_dict[et][1].append(dst_idx)\n","\n","        # store edge features if any\n","        if \"treat_type_onehot\" in d:\n","            # This edge_type corresponds to compound->disease \"DRKG::Treats::Compound:Disease\"\n","            rel_name = et[1]  # original relation name\n","            if rel_name not in edge_treat_type_features:\n","                edge_treat_type_features[rel_name] = []\n","            edge_treat_type_features[rel_name].append(d[\"treat_type_onehot\"])\n","\n","        if \"gene_gene_features\" in d:\n","            rel_name = et[1]\n","            if rel_name not in edge_gene_gene_features:\n","                edge_gene_gene_features[rel_name] = []\n","                edge_gene_gene_mask[rel_name] = []\n","            edge_gene_gene_features[rel_name].append(d[\"gene_gene_features\"])\n","            edge_gene_gene_mask[rel_name].append(d[\"gene_gene_mask\"])\n","\n","    # assign edge_index and edge_attr to data\n","    for (srct, rel_name, dstt), (srcs, dsts) in edge_dict.items():\n","        data[(srct, rel_name, dstt)].edge_index = torch.tensor(\n","            [srcs, dsts], dtype=torch.long\n","        )\n","\n","        # treat_type features exist for this relation\n","        if rel_name in edge_treat_type_features:\n","            data[(srct, rel_name, dstt)].edge_attr = torch.tensor(\n","                edge_treat_type_features[rel_name], dtype=torch.float\n","            )\n","        # if gene-gene features exist for this relation\n","        if rel_name in edge_gene_gene_features:\n","            fmat = torch.tensor(edge_gene_gene_features[rel_name], dtype=torch.float)\n","            mmask = torch.tensor(edge_gene_gene_mask[rel_name], dtype=torch.float)\n","            # concatenate features and masks\n","            if (\n","                hasattr(data[(srct, rel_name, dstt)], \"edge_attr\")\n","                and data[(srct, rel_name, dstt)].edge_attr is not None\n","            ):\n","                # if already have treat_type, we need to concatenate\n","                # this should NOT happen since treat_type is usually for compound-disease edges,\n","                raise ValueError(\"edge attributes already exist for this relation\")\n","                # existing_attr = data[(srct, rel_name, dstt)].edge_attr\n","                # combined_attr = torch.cat([existing_attr, fmat, mmask], dim=1)\n","                # data[(srct, rel_name, dstt)].edge_attr = combined_attr\n","            else:\n","                data[(srct, rel_name, dstt)].edge_attr = torch.cat([fmat, mmask], dim=1)\n","\n","    print(f\"Saving HeteroData to {hetero_path}...\")\n","    with open(hetero_path, \"wb\") as f:\n","        pickle.dump(data, f)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tiXtHtnJx0CM","outputId":"87cd2609-d151-4aca-a97e-ab42981de602"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading existing HeteroData from ./ml_graph_heterodata.pkl...\n"]}]},{"cell_type":"code","source":["###################################################\n","# Define the HGT-based Model\n","###################################################\n","# - we use HGTConv as is for node-level message passing.\n","# - after HGTConv layers, we use node embeddings and for link prediction & incorporate edge features before final scoring.\n","\n","# future work: subclass MessagePassing to incorporate the edge features there\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# make CUDA happy\n","for ntype in data.node_types:\n","    if data[ntype].x is not None:\n","        data[ntype].x = data[ntype].x.to(device)\n","for rel in data.edge_types:\n","    data[rel].edge_index = data[rel].edge_index.to(device)\n","    if hasattr(data[rel], 'edge_attr') and data[rel].edge_attr is not None:\n","        data[rel].edge_attr = data[rel].edge_attr.to(device)\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch_geometric.nn import HGTConv\n","\n","class HGTModel(nn.Module):\n","    def __init__(self, metadata, hidden_channels, out_channels, num_layers, num_heads,\n","                 node_types_count, treat_type_dim, gene_edge_dim, dropout=0.3, jk_mode='concat'):\n","        super(HGTModel, self).__init__()\n","        self.node_types = list(node_types_count.keys())\n","        self.hidden_channels = hidden_channels\n","        self.dropout = dropout\n","        self.treat_type_dim = treat_type_dim\n","        self.gene_edge_dim = gene_edge_dim\n","        self.num_layers = num_layers\n","        self.jk_mode = jk_mode\n","\n","        # embeddings for each node type\n","        self.node_embs = nn.ModuleDict()\n","        for ntype, count in node_types_count.items():\n","            self.node_embs[ntype] = nn.Embedding(count, hidden_channels)\n","\n","        # HGTConv layers\n","        self.convs = nn.ModuleList([\n","            HGTConv(in_channels=hidden_channels, out_channels=hidden_channels,\n","                    metadata=metadata, heads=num_heads) for _ in range(num_layers)\n","        ])\n","\n","        # LayerNorm\n","        self.norms = nn.ModuleList([nn.LayerNorm(hidden_channels) for _ in range(num_layers)])\n","\n","        # JK-Net aggregator: combine layer outputs at the end\n","        if jk_mode == 'concat':\n","            self.final_lin = nn.Linear(hidden_channels * num_layers, hidden_channels)\n","        elif jk_mode in ['max', 'mean']:\n","            # No dimension change needed\n","            self.final_lin = nn.Linear(hidden_channels, hidden_channels)\n","        else:\n","            raise ValueError(\"jk_mode must be one of ['concat', 'max', 'mean']\")\n","\n","        self.lin_score = nn.Linear(hidden_channels * 2 + treat_type_dim + gene_edge_dim, out_channels)\n","\n","    def forward(self, x_dict, edge_index_dict, edge_attr_dict):\n","        # dropout\n","        for ntype in x_dict:\n","            emb = self.node_embs[ntype].weight\n","            emb = F.dropout(emb, p=self.dropout, training=self.training)\n","            x_dict[ntype] = emb\n","\n","        # keep track of outputs from each layer for JK-Net\n","        layer_outputs = []\n","\n","        # message passing\n","        for i, conv in enumerate(self.convs):\n","            prev_x_dict = {nt: x for nt, x in x_dict.items()}\n","            x_dict = conv(x_dict, edge_index_dict)\n","\n","            # residual\n","            for ntype in x_dict:\n","                x_dict[ntype] = x_dict[ntype] + prev_x_dict[ntype]\n","                x_dict[ntype] = self.norms[i](x_dict[ntype])\n","                # dropout\n","                x_dict[ntype] = F.dropout(x_dict[ntype], p=self.dropout, training=self.training)\n","\n","            # for JK-Net\n","            layer_outputs.append({nt: x.clone() for nt, x in x_dict.items()})\n","\n","        for ntype in x_dict:\n","            if self.jk_mode == 'concat':\n","                all_layers = [layer_outputs[i][ntype] for i in range(self.num_layers)]\n","                x_cat = torch.cat(all_layers, dim=-1)  # [num_nodes, hidden_channels * num_layers]\n","                x_dict[ntype] = self.final_lin(x_cat)\n","            elif self.jk_mode == 'max':\n","                all_layers = torch.stack([layer_outputs[i][ntype] for i in range(self.num_layers)], dim=0)\n","                x_max, _ = torch.max(all_layers, dim=0)\n","                x_dict[ntype] = self.final_lin(x_max)\n","            elif self.jk_mode == 'mean':\n","                all_layers = torch.stack([layer_outputs[i][ntype] for i in range(self.num_layers)], dim=0)\n","                x_mean = torch.mean(all_layers, dim=0)\n","                x_dict[ntype] = self.final_lin(x_mean)\n","\n","        return x_dict\n","\n","    def predict_link(self, x_dict, src_nodes, dst_nodes, edge_attr=None):\n","        src_ntype, src_ids = src_nodes\n","        dst_ntype, dst_ids = dst_nodes\n","        x_src = x_dict[src_ntype][src_ids]\n","        x_dst = x_dict[dst_ntype][dst_ids]\n","\n","        # concatenate augmented edge features; pad if missing\n","        needed_dim = self.treat_type_dim + self.gene_edge_dim\n","        if edge_attr is None:\n","            edge_attr = torch.zeros(x_src.size(0), needed_dim, device=x_src.device)\n","        else:\n","            current_dim = edge_attr.size(1)\n","            if current_dim < needed_dim:\n","                pad_dim = needed_dim - current_dim\n","                padding = torch.zeros(x_src.size(0), pad_dim, device=x_src.device)\n","                edge_attr = torch.cat([edge_attr, padding], dim=1)\n","\n","        combined = torch.cat([x_src, x_dst, edge_attr], dim=-1)\n","        # dropout\n","        combined = F.dropout(combined, p=self.dropout, training=self.training)\n","        scores = self.lin_score(combined)\n","        return scores\n","\n","# train, val, test treatment triples splits\n","\n","metadata = data.metadata()\n","\n","cd_rel = (\"compound\", \"DRKG::Treats::Compound:Disease\", \"disease\")\n","\n","cd_edge_index = data[cd_rel].edge_index\n","cd_edge_attr = data[cd_rel].edge_attr if hasattr(data[cd_rel], \"edge_attr\") else None\n","\n","num_cd_edges = cd_edge_index.size(1)\n","perm = torch.randperm(num_cd_edges, device=device)\n","\n","train_size = int(0.8 * num_cd_edges)\n","val_size = int(0.1 * num_cd_edges)\n","test_size = num_cd_edges - train_size - val_size\n","\n","train_edges = cd_edge_index[:, perm[:train_size]]\n","val_edges = cd_edge_index[:, perm[train_size : train_size + val_size]]\n","test_edges = cd_edge_index[:, perm[train_size + val_size :]]\n","\n","\n","def slice_edge_attr(edge_attr_full, idxs):\n","    return edge_attr_full[idxs] if edge_attr_full is not None else None\n","\n","\n","train_edge_attr = slice_edge_attr(cd_edge_attr, perm[:train_size])\n","val_edge_attr = slice_edge_attr(cd_edge_attr, perm[train_size : train_size + val_size])\n","test_edge_attr = slice_edge_attr(cd_edge_attr, perm[train_size + val_size :])\n","\n","compound_count = data[\"compound\"].num_nodes\n","disease_count = data[\"disease\"].num_nodes\n","\n","\n","def negative_sampling(num_samples):\n","    neg_src = torch.randint(0, compound_count, (num_samples,), device=device)\n","    neg_dst = torch.randint(0, disease_count, (num_samples,), device=device)\n","    return neg_src, neg_dst\n","\n","\n","neg_train_src, neg_train_dst = negative_sampling(train_edges.size(1))\n","neg_val_src, neg_val_dst = negative_sampling(val_edges.size(1))\n","neg_test_src, neg_test_dst = negative_sampling(test_edges.size(1))"],"metadata":{"id":"_B9fMGFJx0w1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["hidden_channels = 128\n","out_channels = 1\n","num_layers = 2\n","num_heads = 8\n","\n","model = HGTModel(metadata, hidden_channels, out_channels, num_layers, num_heads,\n","                 node_types_count=node_types_count,\n","                 treat_type_dim=treat_type_dim,\n","                 gene_edge_dim=gene_gene_feat_dim+mask_dim if gene_gene_feat_dim>0 else 0).to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n","\n","def compute_loss(pos_edges, neg_edges, pos_edge_attr, model, node_embs):\n","    pos_src, pos_dst = pos_edges\n","    neg_src, neg_dst = neg_edges\n","    # pos_src, pos_dst, neg_src, neg_dst are already on device\n","    idxs = torch.arange(pos_src.size(0), device=device)\n","    pos_attr = slice_edge_attr(pos_edge_attr, idxs)\n","\n","    pos_scores = model.predict_link(node_embs, ('compound', pos_src), ('disease', pos_dst), pos_attr)\n","    neg_scores = model.predict_link(node_embs, ('compound', neg_src), ('disease', neg_dst), None)\n","\n","    labels = torch.cat([torch.ones(pos_scores.size(0), 1, device=device),\n","                        torch.zeros(neg_scores.size(0), 1, device=device)], dim=0)\n","    scores = torch.cat([pos_scores, neg_scores], dim=0)\n","    loss = F.binary_cross_entropy_with_logits(scores, labels)\n","    return loss\n","\n","# Prepare once outside loop\n","x_dict = {ntype: data[ntype].x for ntype in data.node_types}\n","edge_index_dict = {rel: data[rel].edge_index for rel in data.edge_types}\n","edge_attr_dict = {rel: (data[rel].edge_attr if hasattr(data[rel], 'edge_attr') else None)\n","                  for rel in data.edge_types}\n","\n","train_losses = []\n","val_losses = []\n","\n","# Early stopping parameters\n","patience = 40  # Number of epochs to wait for improvement\n","min_delta = 1e-4  # Minimum change in validation loss to qualify as an improvement\n","best_val_loss = float(\"inf\")\n","best_model_state = None\n","patience_counter = 0\n","best_epoch = 0\n","\n","epochs = 4000\n","current_epoch = 0\n","for epoch in range(epochs):\n","    model.train()\n","    optimizer.zero_grad()\n","    x_dict = {ntype: data[ntype].x for ntype in data.node_types}\n","    edge_index_dict = {rel: data[rel].edge_index for rel in data.edge_types}\n","    edge_attr_dict = {\n","        rel: (data[rel].edge_attr if hasattr(data[rel], \"edge_attr\") else None)\n","        for rel in data.edge_types\n","    }\n","\n","    node_embs = model(x_dict, edge_index_dict, edge_attr_dict)\n","\n","    train_loss = compute_loss(\n","        train_edges, (neg_train_src, neg_train_dst), train_edge_attr, model, node_embs\n","    )\n","    train_loss.backward()\n","    optimizer.step()\n","\n","    # validation loss\n","    model.eval()\n","    with torch.no_grad():\n","        node_embs = model(x_dict, edge_index_dict, edge_attr_dict)\n","        val_loss = compute_loss(\n","            val_edges, (neg_val_src, neg_val_dst), val_edge_attr, model, node_embs\n","        )\n","\n","    train_losses.append(train_loss.item())\n","    val_losses.append(val_loss.item())\n","\n","    # early stopping check\n","    current_val_loss = val_loss.item()\n","    if current_val_loss < best_val_loss - min_delta:\n","        best_val_loss = current_val_loss\n","        best_model_state = model.state_dict()\n","        patience_counter = 0\n","        best_epoch = epoch\n","    else:\n","        patience_counter += 1\n","\n","    print(\n","        f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}\"\n","        + (f\" *\" if patience_counter == 0 else \"\")\n","    )\n","\n","    # early stopping\n","    if patience_counter >= patience:\n","        print(\n","            f\"\\nEarly stopping triggered! Best epoch was {best_epoch + 1} with validation loss: {best_val_loss:.4f}\"\n","        )\n","        # Restore best model\n","        model.load_state_dict(best_model_state)\n","        break\n","    current_epoch += 1\n","\n","if epoch == epochs - 1 and best_model_state is not None and best_epoch != epoch:\n","    print(\n","        f\"\\nTraining completed. Restoring best model from epoch {best_epoch + 1} with validation loss: {best_val_loss:.4f}\"\n","    )\n","    model.load_state_dict(best_model_state)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k1xwvxZ35cLa","outputId":"bf392330-0d4e-4803-9120-047c4d0a9193","collapsed":true},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/4000 - Train Loss: 0.7125, Val Loss: 0.6897 *\n","Epoch 2/4000 - Train Loss: 0.6995, Val Loss: 0.6821 *\n","Epoch 3/4000 - Train Loss: 0.6958, Val Loss: 0.6756 *\n","Epoch 4/4000 - Train Loss: 0.6849, Val Loss: 0.6700 *\n","Epoch 5/4000 - Train Loss: 0.6828, Val Loss: 0.6653 *\n","Epoch 6/4000 - Train Loss: 0.6806, Val Loss: 0.6611 *\n","Epoch 7/4000 - Train Loss: 0.6781, Val Loss: 0.6574 *\n","Epoch 8/4000 - Train Loss: 0.6743, Val Loss: 0.6542 *\n","Epoch 9/4000 - Train Loss: 0.6702, Val Loss: 0.6511 *\n","Epoch 10/4000 - Train Loss: 0.6685, Val Loss: 0.6482 *\n","Epoch 11/4000 - Train Loss: 0.6671, Val Loss: 0.6454 *\n","Epoch 12/4000 - Train Loss: 0.6634, Val Loss: 0.6427 *\n","Epoch 13/4000 - Train Loss: 0.6671, Val Loss: 0.6399 *\n","Epoch 14/4000 - Train Loss: 0.6580, Val Loss: 0.6369 *\n","Epoch 15/4000 - Train Loss: 0.6606, Val Loss: 0.6339 *\n","Epoch 16/4000 - Train Loss: 0.6528, Val Loss: 0.6306 *\n","Epoch 17/4000 - Train Loss: 0.6458, Val Loss: 0.6271 *\n","Epoch 18/4000 - Train Loss: 0.6461, Val Loss: 0.6232 *\n","Epoch 19/4000 - Train Loss: 0.6389, Val Loss: 0.6190 *\n","Epoch 20/4000 - Train Loss: 0.6493, Val Loss: 0.6144 *\n","Epoch 21/4000 - Train Loss: 0.6422, Val Loss: 0.6095 *\n","Epoch 22/4000 - Train Loss: 0.6418, Val Loss: 0.6041 *\n","Epoch 23/4000 - Train Loss: 0.6285, Val Loss: 0.5983 *\n","Epoch 24/4000 - Train Loss: 0.6288, Val Loss: 0.5920 *\n","Epoch 25/4000 - Train Loss: 0.6219, Val Loss: 0.5851 *\n","Epoch 26/4000 - Train Loss: 0.6097, Val Loss: 0.5776 *\n","Epoch 27/4000 - Train Loss: 0.6041, Val Loss: 0.5694 *\n","Epoch 28/4000 - Train Loss: 0.5988, Val Loss: 0.5604 *\n","Epoch 29/4000 - Train Loss: 0.5930, Val Loss: 0.5508 *\n","Epoch 30/4000 - Train Loss: 0.5851, Val Loss: 0.5407 *\n","Epoch 31/4000 - Train Loss: 0.5772, Val Loss: 0.5300 *\n","Epoch 32/4000 - Train Loss: 0.5684, Val Loss: 0.5188 *\n","Epoch 33/4000 - Train Loss: 0.5640, Val Loss: 0.5075 *\n","Epoch 34/4000 - Train Loss: 0.5455, Val Loss: 0.4960 *\n","Epoch 35/4000 - Train Loss: 0.5302, Val Loss: 0.4846 *\n","Epoch 36/4000 - Train Loss: 0.5240, Val Loss: 0.4737 *\n","Epoch 37/4000 - Train Loss: 0.5121, Val Loss: 0.4635 *\n","Epoch 38/4000 - Train Loss: 0.5044, Val Loss: 0.4544 *\n","Epoch 39/4000 - Train Loss: 0.4939, Val Loss: 0.4464 *\n","Epoch 40/4000 - Train Loss: 0.4835, Val Loss: 0.4397 *\n","Epoch 41/4000 - Train Loss: 0.4779, Val Loss: 0.4336 *\n","Epoch 42/4000 - Train Loss: 0.4655, Val Loss: 0.4276 *\n","Epoch 43/4000 - Train Loss: 0.4563, Val Loss: 0.4216 *\n","Epoch 44/4000 - Train Loss: 0.4518, Val Loss: 0.4160 *\n","Epoch 45/4000 - Train Loss: 0.4568, Val Loss: 0.4106 *\n","Epoch 46/4000 - Train Loss: 0.4450, Val Loss: 0.4058 *\n","Epoch 47/4000 - Train Loss: 0.4330, Val Loss: 0.4009 *\n","Epoch 48/4000 - Train Loss: 0.4233, Val Loss: 0.3959 *\n","Epoch 49/4000 - Train Loss: 0.4267, Val Loss: 0.3910 *\n","Epoch 50/4000 - Train Loss: 0.4130, Val Loss: 0.3868 *\n","Epoch 51/4000 - Train Loss: 0.4063, Val Loss: 0.3832 *\n","Epoch 52/4000 - Train Loss: 0.4028, Val Loss: 0.3803 *\n","Epoch 53/4000 - Train Loss: 0.4012, Val Loss: 0.3776 *\n","Epoch 54/4000 - Train Loss: 0.4050, Val Loss: 0.3747 *\n","Epoch 55/4000 - Train Loss: 0.3943, Val Loss: 0.3711 *\n","Epoch 56/4000 - Train Loss: 0.3919, Val Loss: 0.3669 *\n","Epoch 57/4000 - Train Loss: 0.3895, Val Loss: 0.3631 *\n","Epoch 58/4000 - Train Loss: 0.3909, Val Loss: 0.3602 *\n","Epoch 59/4000 - Train Loss: 0.3866, Val Loss: 0.3580 *\n","Epoch 60/4000 - Train Loss: 0.3838, Val Loss: 0.3559 *\n","Epoch 61/4000 - Train Loss: 0.3767, Val Loss: 0.3540 *\n","Epoch 62/4000 - Train Loss: 0.3750, Val Loss: 0.3525 *\n","Epoch 63/4000 - Train Loss: 0.3724, Val Loss: 0.3516 *\n","Epoch 64/4000 - Train Loss: 0.3740, Val Loss: 0.3507 *\n","Epoch 65/4000 - Train Loss: 0.3717, Val Loss: 0.3491 *\n","Epoch 66/4000 - Train Loss: 0.3667, Val Loss: 0.3466 *\n","Epoch 67/4000 - Train Loss: 0.3624, Val Loss: 0.3437 *\n","Epoch 68/4000 - Train Loss: 0.3583, Val Loss: 0.3409 *\n","Epoch 69/4000 - Train Loss: 0.3654, Val Loss: 0.3388 *\n","Epoch 70/4000 - Train Loss: 0.3624, Val Loss: 0.3369 *\n","Epoch 71/4000 - Train Loss: 0.3553, Val Loss: 0.3349 *\n","Epoch 72/4000 - Train Loss: 0.3584, Val Loss: 0.3332 *\n","Epoch 73/4000 - Train Loss: 0.3553, Val Loss: 0.3320 *\n","Epoch 74/4000 - Train Loss: 0.3506, Val Loss: 0.3314 *\n","Epoch 75/4000 - Train Loss: 0.3506, Val Loss: 0.3305 *\n","Epoch 76/4000 - Train Loss: 0.3478, Val Loss: 0.3287 *\n","Epoch 77/4000 - Train Loss: 0.3446, Val Loss: 0.3265 *\n","Epoch 78/4000 - Train Loss: 0.3506, Val Loss: 0.3243 *\n","Epoch 79/4000 - Train Loss: 0.3485, Val Loss: 0.3226 *\n","Epoch 80/4000 - Train Loss: 0.3445, Val Loss: 0.3213 *\n","Epoch 81/4000 - Train Loss: 0.3447, Val Loss: 0.3203 *\n","Epoch 82/4000 - Train Loss: 0.3426, Val Loss: 0.3199 *\n","Epoch 83/4000 - Train Loss: 0.3409, Val Loss: 0.3203\n","Epoch 84/4000 - Train Loss: 0.3368, Val Loss: 0.3200\n","Epoch 85/4000 - Train Loss: 0.3367, Val Loss: 0.3190 *\n","Epoch 86/4000 - Train Loss: 0.3398, Val Loss: 0.3172 *\n","Epoch 87/4000 - Train Loss: 0.3350, Val Loss: 0.3152 *\n","Epoch 88/4000 - Train Loss: 0.3368, Val Loss: 0.3141 *\n","Epoch 89/4000 - Train Loss: 0.3341, Val Loss: 0.3134 *\n","Epoch 90/4000 - Train Loss: 0.3358, Val Loss: 0.3128 *\n","Epoch 91/4000 - Train Loss: 0.3316, Val Loss: 0.3122 *\n","Epoch 92/4000 - Train Loss: 0.3280, Val Loss: 0.3119 *\n","Epoch 93/4000 - Train Loss: 0.3267, Val Loss: 0.3119\n","Epoch 94/4000 - Train Loss: 0.3284, Val Loss: 0.3113 *\n","Epoch 95/4000 - Train Loss: 0.3247, Val Loss: 0.3100 *\n","Epoch 96/4000 - Train Loss: 0.3228, Val Loss: 0.3085 *\n","Epoch 97/4000 - Train Loss: 0.3234, Val Loss: 0.3069 *\n","Epoch 98/4000 - Train Loss: 0.3227, Val Loss: 0.3058 *\n","Epoch 99/4000 - Train Loss: 0.3238, Val Loss: 0.3053 *\n","Epoch 100/4000 - Train Loss: 0.3220, Val Loss: 0.3049 *\n","Epoch 101/4000 - Train Loss: 0.3194, Val Loss: 0.3041 *\n","Epoch 102/4000 - Train Loss: 0.3246, Val Loss: 0.3032 *\n","Epoch 103/4000 - Train Loss: 0.3205, Val Loss: 0.3023 *\n","Epoch 104/4000 - Train Loss: 0.3196, Val Loss: 0.3017 *\n","Epoch 105/4000 - Train Loss: 0.3193, Val Loss: 0.3008 *\n","Epoch 106/4000 - Train Loss: 0.3182, Val Loss: 0.3001 *\n","Epoch 107/4000 - Train Loss: 0.3190, Val Loss: 0.2994 *\n","Epoch 108/4000 - Train Loss: 0.3157, Val Loss: 0.2988 *\n","Epoch 109/4000 - Train Loss: 0.3129, Val Loss: 0.2980 *\n","Epoch 110/4000 - Train Loss: 0.3130, Val Loss: 0.2971 *\n","Epoch 111/4000 - Train Loss: 0.3117, Val Loss: 0.2960 *\n","Epoch 112/4000 - Train Loss: 0.3069, Val Loss: 0.2954 *\n","Epoch 113/4000 - Train Loss: 0.3105, Val Loss: 0.2952 *\n","Epoch 114/4000 - Train Loss: 0.3114, Val Loss: 0.2953\n","Epoch 115/4000 - Train Loss: 0.3095, Val Loss: 0.2956\n","Epoch 116/4000 - Train Loss: 0.3174, Val Loss: 0.2951\n","Epoch 117/4000 - Train Loss: 0.3125, Val Loss: 0.2943 *\n","Epoch 118/4000 - Train Loss: 0.3088, Val Loss: 0.2939 *\n","Epoch 119/4000 - Train Loss: 0.3101, Val Loss: 0.2935 *\n","Epoch 120/4000 - Train Loss: 0.3094, Val Loss: 0.2928 *\n","Epoch 121/4000 - Train Loss: 0.3022, Val Loss: 0.2926 *\n","Epoch 122/4000 - Train Loss: 0.3075, Val Loss: 0.2923 *\n","Epoch 123/4000 - Train Loss: 0.3018, Val Loss: 0.2916 *\n","Epoch 124/4000 - Train Loss: 0.3048, Val Loss: 0.2907 *\n","Epoch 125/4000 - Train Loss: 0.3025, Val Loss: 0.2900 *\n","Epoch 126/4000 - Train Loss: 0.3023, Val Loss: 0.2896 *\n","Epoch 127/4000 - Train Loss: 0.3029, Val Loss: 0.2895 *\n","Epoch 128/4000 - Train Loss: 0.3039, Val Loss: 0.2896\n","Epoch 129/4000 - Train Loss: 0.3021, Val Loss: 0.2896\n","Epoch 130/4000 - Train Loss: 0.3007, Val Loss: 0.2899\n","Epoch 131/4000 - Train Loss: 0.3009, Val Loss: 0.2900\n","Epoch 132/4000 - Train Loss: 0.3008, Val Loss: 0.2896\n","Epoch 133/4000 - Train Loss: 0.2985, Val Loss: 0.2886 *\n","Epoch 134/4000 - Train Loss: 0.2989, Val Loss: 0.2873 *\n","Epoch 135/4000 - Train Loss: 0.2961, Val Loss: 0.2865 *\n","Epoch 136/4000 - Train Loss: 0.2992, Val Loss: 0.2860 *\n","Epoch 137/4000 - Train Loss: 0.3020, Val Loss: 0.2857 *\n","Epoch 138/4000 - Train Loss: 0.2994, Val Loss: 0.2854 *\n","Epoch 139/4000 - Train Loss: 0.2958, Val Loss: 0.2856\n","Epoch 140/4000 - Train Loss: 0.2970, Val Loss: 0.2865\n","Epoch 141/4000 - Train Loss: 0.2968, Val Loss: 0.2877\n","Epoch 142/4000 - Train Loss: 0.2975, Val Loss: 0.2884\n","Epoch 143/4000 - Train Loss: 0.2972, Val Loss: 0.2877\n","Epoch 144/4000 - Train Loss: 0.2989, Val Loss: 0.2865\n","Epoch 145/4000 - Train Loss: 0.2951, Val Loss: 0.2862\n","Epoch 146/4000 - Train Loss: 0.2915, Val Loss: 0.2865\n","Epoch 147/4000 - Train Loss: 0.2914, Val Loss: 0.2864\n","Epoch 148/4000 - Train Loss: 0.2919, Val Loss: 0.2858\n","Epoch 149/4000 - Train Loss: 0.2944, Val Loss: 0.2837 *\n","Epoch 150/4000 - Train Loss: 0.2929, Val Loss: 0.2813 *\n","Epoch 151/4000 - Train Loss: 0.2899, Val Loss: 0.2798 *\n","Epoch 152/4000 - Train Loss: 0.2905, Val Loss: 0.2796 *\n","Epoch 153/4000 - Train Loss: 0.2897, Val Loss: 0.2803\n","Epoch 154/4000 - Train Loss: 0.2937, Val Loss: 0.2813\n","Epoch 155/4000 - Train Loss: 0.2905, Val Loss: 0.2818\n","Epoch 156/4000 - Train Loss: 0.2884, Val Loss: 0.2817\n","Epoch 157/4000 - Train Loss: 0.2904, Val Loss: 0.2816\n","Epoch 158/4000 - Train Loss: 0.2917, Val Loss: 0.2816\n","Epoch 159/4000 - Train Loss: 0.2926, Val Loss: 0.2818\n","Epoch 160/4000 - Train Loss: 0.2902, Val Loss: 0.2830\n","Epoch 161/4000 - Train Loss: 0.2883, Val Loss: 0.2840\n","Epoch 162/4000 - Train Loss: 0.2915, Val Loss: 0.2829\n","Epoch 163/4000 - Train Loss: 0.2912, Val Loss: 0.2808\n","Epoch 164/4000 - Train Loss: 0.2887, Val Loss: 0.2789 *\n","Epoch 165/4000 - Train Loss: 0.2872, Val Loss: 0.2781 *\n","Epoch 166/4000 - Train Loss: 0.2898, Val Loss: 0.2780\n","Epoch 167/4000 - Train Loss: 0.2850, Val Loss: 0.2780\n","Epoch 168/4000 - Train Loss: 0.2859, Val Loss: 0.2780\n","Epoch 169/4000 - Train Loss: 0.2851, Val Loss: 0.2777 *\n","Epoch 170/4000 - Train Loss: 0.2870, Val Loss: 0.2773 *\n","Epoch 171/4000 - Train Loss: 0.2861, Val Loss: 0.2770 *\n","Epoch 172/4000 - Train Loss: 0.2847, Val Loss: 0.2771\n","Epoch 173/4000 - Train Loss: 0.2851, Val Loss: 0.2772\n","Epoch 174/4000 - Train Loss: 0.2842, Val Loss: 0.2771\n","Epoch 175/4000 - Train Loss: 0.2847, Val Loss: 0.2770\n","Epoch 176/4000 - Train Loss: 0.2857, Val Loss: 0.2768 *\n","Epoch 177/4000 - Train Loss: 0.2849, Val Loss: 0.2764 *\n","Epoch 178/4000 - Train Loss: 0.2843, Val Loss: 0.2762 *\n","Epoch 179/4000 - Train Loss: 0.2865, Val Loss: 0.2761 *\n","Epoch 180/4000 - Train Loss: 0.2829, Val Loss: 0.2760\n","Epoch 181/4000 - Train Loss: 0.2838, Val Loss: 0.2758 *\n","Epoch 182/4000 - Train Loss: 0.2803, Val Loss: 0.2756 *\n","Epoch 183/4000 - Train Loss: 0.2810, Val Loss: 0.2756\n","Epoch 184/4000 - Train Loss: 0.2810, Val Loss: 0.2754 *\n","Epoch 185/4000 - Train Loss: 0.2830, Val Loss: 0.2752 *\n","Epoch 186/4000 - Train Loss: 0.2836, Val Loss: 0.2751 *\n","Epoch 187/4000 - Train Loss: 0.2819, Val Loss: 0.2749 *\n","Epoch 188/4000 - Train Loss: 0.2799, Val Loss: 0.2747 *\n","Epoch 189/4000 - Train Loss: 0.2817, Val Loss: 0.2742 *\n","Epoch 190/4000 - Train Loss: 0.2789, Val Loss: 0.2740 *\n","Epoch 191/4000 - Train Loss: 0.2792, Val Loss: 0.2735 *\n","Epoch 192/4000 - Train Loss: 0.2793, Val Loss: 0.2730 *\n","Epoch 193/4000 - Train Loss: 0.2807, Val Loss: 0.2728 *\n","Epoch 194/4000 - Train Loss: 0.2798, Val Loss: 0.2726 *\n","Epoch 195/4000 - Train Loss: 0.2798, Val Loss: 0.2725 *\n","Epoch 196/4000 - Train Loss: 0.2775, Val Loss: 0.2725\n","Epoch 197/4000 - Train Loss: 0.2785, Val Loss: 0.2727\n","Epoch 198/4000 - Train Loss: 0.2788, Val Loss: 0.2726\n","Epoch 199/4000 - Train Loss: 0.2759, Val Loss: 0.2729\n","Epoch 200/4000 - Train Loss: 0.2813, Val Loss: 0.2727\n","Epoch 201/4000 - Train Loss: 0.2777, Val Loss: 0.2726\n","Epoch 202/4000 - Train Loss: 0.2795, Val Loss: 0.2723 *\n","Epoch 203/4000 - Train Loss: 0.2791, Val Loss: 0.2715 *\n","Epoch 204/4000 - Train Loss: 0.2758, Val Loss: 0.2711 *\n","Epoch 205/4000 - Train Loss: 0.2772, Val Loss: 0.2702 *\n","Epoch 206/4000 - Train Loss: 0.2783, Val Loss: 0.2693 *\n","Epoch 207/4000 - Train Loss: 0.2765, Val Loss: 0.2687 *\n","Epoch 208/4000 - Train Loss: 0.2776, Val Loss: 0.2687\n","Epoch 209/4000 - Train Loss: 0.2742, Val Loss: 0.2695\n","Epoch 210/4000 - Train Loss: 0.2753, Val Loss: 0.2692\n","Epoch 211/4000 - Train Loss: 0.2749, Val Loss: 0.2684 *\n","Epoch 212/4000 - Train Loss: 0.2791, Val Loss: 0.2672 *\n","Epoch 213/4000 - Train Loss: 0.2745, Val Loss: 0.2673\n","Epoch 214/4000 - Train Loss: 0.2715, Val Loss: 0.2674\n","Epoch 215/4000 - Train Loss: 0.2733, Val Loss: 0.2673\n","Epoch 216/4000 - Train Loss: 0.2732, Val Loss: 0.2683\n","Epoch 217/4000 - Train Loss: 0.2724, Val Loss: 0.2692\n","Epoch 218/4000 - Train Loss: 0.2735, Val Loss: 0.2686\n","Epoch 219/4000 - Train Loss: 0.2738, Val Loss: 0.2673\n","Epoch 220/4000 - Train Loss: 0.2723, Val Loss: 0.2670 *\n","Epoch 221/4000 - Train Loss: 0.2727, Val Loss: 0.2668 *\n","Epoch 222/4000 - Train Loss: 0.2708, Val Loss: 0.2667 *\n","Epoch 223/4000 - Train Loss: 0.2726, Val Loss: 0.2670\n","Epoch 224/4000 - Train Loss: 0.2730, Val Loss: 0.2674\n","Epoch 225/4000 - Train Loss: 0.2730, Val Loss: 0.2676\n","Epoch 226/4000 - Train Loss: 0.2699, Val Loss: 0.2673\n","Epoch 227/4000 - Train Loss: 0.2724, Val Loss: 0.2667\n","Epoch 228/4000 - Train Loss: 0.2722, Val Loss: 0.2665 *\n","Epoch 229/4000 - Train Loss: 0.2699, Val Loss: 0.2664\n","Epoch 230/4000 - Train Loss: 0.2716, Val Loss: 0.2664\n","Epoch 231/4000 - Train Loss: 0.2704, Val Loss: 0.2667\n","Epoch 232/4000 - Train Loss: 0.2701, Val Loss: 0.2665\n","Epoch 233/4000 - Train Loss: 0.2737, Val Loss: 0.2664 *\n","Epoch 234/4000 - Train Loss: 0.2721, Val Loss: 0.2660 *\n","Epoch 235/4000 - Train Loss: 0.2715, Val Loss: 0.2654 *\n","Epoch 236/4000 - Train Loss: 0.2691, Val Loss: 0.2650 *\n","Epoch 237/4000 - Train Loss: 0.2704, Val Loss: 0.2649\n","Epoch 238/4000 - Train Loss: 0.2701, Val Loss: 0.2650\n","Epoch 239/4000 - Train Loss: 0.2695, Val Loss: 0.2651\n","Epoch 240/4000 - Train Loss: 0.2681, Val Loss: 0.2651\n","Epoch 241/4000 - Train Loss: 0.2658, Val Loss: 0.2651\n","Epoch 242/4000 - Train Loss: 0.2681, Val Loss: 0.2650\n","Epoch 243/4000 - Train Loss: 0.2662, Val Loss: 0.2650\n","Epoch 244/4000 - Train Loss: 0.2680, Val Loss: 0.2652\n","Epoch 245/4000 - Train Loss: 0.2718, Val Loss: 0.2651\n","Epoch 246/4000 - Train Loss: 0.2667, Val Loss: 0.2646 *\n","Epoch 247/4000 - Train Loss: 0.2689, Val Loss: 0.2640 *\n","Epoch 248/4000 - Train Loss: 0.2657, Val Loss: 0.2637 *\n","Epoch 249/4000 - Train Loss: 0.2639, Val Loss: 0.2635 *\n","Epoch 250/4000 - Train Loss: 0.2653, Val Loss: 0.2633 *\n","Epoch 251/4000 - Train Loss: 0.2689, Val Loss: 0.2628 *\n","Epoch 252/4000 - Train Loss: 0.2654, Val Loss: 0.2623 *\n","Epoch 253/4000 - Train Loss: 0.2648, Val Loss: 0.2620 *\n","Epoch 254/4000 - Train Loss: 0.2659, Val Loss: 0.2622\n","Epoch 255/4000 - Train Loss: 0.2635, Val Loss: 0.2633\n","Epoch 256/4000 - Train Loss: 0.2687, Val Loss: 0.2636\n","Epoch 257/4000 - Train Loss: 0.2673, Val Loss: 0.2632\n","Epoch 258/4000 - Train Loss: 0.2651, Val Loss: 0.2617 *\n","Epoch 259/4000 - Train Loss: 0.2666, Val Loss: 0.2614 *\n","Epoch 260/4000 - Train Loss: 0.2635, Val Loss: 0.2612 *\n","Epoch 261/4000 - Train Loss: 0.2658, Val Loss: 0.2613\n","Epoch 262/4000 - Train Loss: 0.2652, Val Loss: 0.2620\n","Epoch 263/4000 - Train Loss: 0.2660, Val Loss: 0.2618\n","Epoch 264/4000 - Train Loss: 0.2633, Val Loss: 0.2614\n","Epoch 265/4000 - Train Loss: 0.2629, Val Loss: 0.2613\n","Epoch 266/4000 - Train Loss: 0.2636, Val Loss: 0.2608 *\n","Epoch 267/4000 - Train Loss: 0.2649, Val Loss: 0.2608\n","Epoch 268/4000 - Train Loss: 0.2665, Val Loss: 0.2612\n","Epoch 269/4000 - Train Loss: 0.2649, Val Loss: 0.2603 *\n","Epoch 270/4000 - Train Loss: 0.2650, Val Loss: 0.2590 *\n","Epoch 271/4000 - Train Loss: 0.2623, Val Loss: 0.2586 *\n","Epoch 272/4000 - Train Loss: 0.2628, Val Loss: 0.2587\n","Epoch 273/4000 - Train Loss: 0.2634, Val Loss: 0.2593\n","Epoch 274/4000 - Train Loss: 0.2628, Val Loss: 0.2604\n","Epoch 275/4000 - Train Loss: 0.2620, Val Loss: 0.2625\n","Epoch 276/4000 - Train Loss: 0.2645, Val Loss: 0.2629\n","Epoch 277/4000 - Train Loss: 0.2648, Val Loss: 0.2622\n","Epoch 278/4000 - Train Loss: 0.2589, Val Loss: 0.2621\n","Epoch 279/4000 - Train Loss: 0.2622, Val Loss: 0.2619\n","Epoch 280/4000 - Train Loss: 0.2609, Val Loss: 0.2618\n","Epoch 281/4000 - Train Loss: 0.2616, Val Loss: 0.2621\n","Epoch 282/4000 - Train Loss: 0.2643, Val Loss: 0.2612\n","Epoch 283/4000 - Train Loss: 0.2618, Val Loss: 0.2596\n","Epoch 284/4000 - Train Loss: 0.2614, Val Loss: 0.2584 *\n","Epoch 285/4000 - Train Loss: 0.2644, Val Loss: 0.2575 *\n","Epoch 286/4000 - Train Loss: 0.2629, Val Loss: 0.2575\n","Epoch 287/4000 - Train Loss: 0.2608, Val Loss: 0.2584\n","Epoch 288/4000 - Train Loss: 0.2640, Val Loss: 0.2581\n","Epoch 289/4000 - Train Loss: 0.2619, Val Loss: 0.2576\n","Epoch 290/4000 - Train Loss: 0.2597, Val Loss: 0.2575\n","Epoch 291/4000 - Train Loss: 0.2601, Val Loss: 0.2575\n","Epoch 292/4000 - Train Loss: 0.2622, Val Loss: 0.2582\n","Epoch 293/4000 - Train Loss: 0.2582, Val Loss: 0.2588\n","Epoch 294/4000 - Train Loss: 0.2592, Val Loss: 0.2593\n","Epoch 295/4000 - Train Loss: 0.2569, Val Loss: 0.2600\n","Epoch 296/4000 - Train Loss: 0.2622, Val Loss: 0.2597\n","Epoch 297/4000 - Train Loss: 0.2581, Val Loss: 0.2592\n","Epoch 298/4000 - Train Loss: 0.2615, Val Loss: 0.2589\n","Epoch 299/4000 - Train Loss: 0.2599, Val Loss: 0.2584\n","Epoch 300/4000 - Train Loss: 0.2580, Val Loss: 0.2577\n","Epoch 301/4000 - Train Loss: 0.2580, Val Loss: 0.2568 *\n","Epoch 302/4000 - Train Loss: 0.2605, Val Loss: 0.2560 *\n","Epoch 303/4000 - Train Loss: 0.2572, Val Loss: 0.2555 *\n","Epoch 304/4000 - Train Loss: 0.2560, Val Loss: 0.2552 *\n","Epoch 305/4000 - Train Loss: 0.2549, Val Loss: 0.2556\n","Epoch 306/4000 - Train Loss: 0.2596, Val Loss: 0.2557\n","Epoch 307/4000 - Train Loss: 0.2590, Val Loss: 0.2559\n","Epoch 308/4000 - Train Loss: 0.2561, Val Loss: 0.2557\n","Epoch 309/4000 - Train Loss: 0.2579, Val Loss: 0.2555\n","Epoch 310/4000 - Train Loss: 0.2552, Val Loss: 0.2556\n","Epoch 311/4000 - Train Loss: 0.2568, Val Loss: 0.2561\n","Epoch 312/4000 - Train Loss: 0.2555, Val Loss: 0.2564\n","Epoch 313/4000 - Train Loss: 0.2546, Val Loss: 0.2559\n","Epoch 314/4000 - Train Loss: 0.2630, Val Loss: 0.2550 *\n","Epoch 315/4000 - Train Loss: 0.2545, Val Loss: 0.2546 *\n","Epoch 316/4000 - Train Loss: 0.2580, Val Loss: 0.2546\n","Epoch 317/4000 - Train Loss: 0.2553, Val Loss: 0.2553\n","Epoch 318/4000 - Train Loss: 0.2558, Val Loss: 0.2554\n","Epoch 319/4000 - Train Loss: 0.2563, Val Loss: 0.2548\n","Epoch 320/4000 - Train Loss: 0.2533, Val Loss: 0.2539 *\n","Epoch 321/4000 - Train Loss: 0.2560, Val Loss: 0.2535 *\n","Epoch 322/4000 - Train Loss: 0.2540, Val Loss: 0.2532 *\n","Epoch 323/4000 - Train Loss: 0.2566, Val Loss: 0.2537\n","Epoch 324/4000 - Train Loss: 0.2576, Val Loss: 0.2543\n","Epoch 325/4000 - Train Loss: 0.2542, Val Loss: 0.2535\n","Epoch 326/4000 - Train Loss: 0.2565, Val Loss: 0.2523 *\n","Epoch 327/4000 - Train Loss: 0.2525, Val Loss: 0.2516 *\n","Epoch 328/4000 - Train Loss: 0.2537, Val Loss: 0.2516\n","Epoch 329/4000 - Train Loss: 0.2555, Val Loss: 0.2519\n","Epoch 330/4000 - Train Loss: 0.2506, Val Loss: 0.2522\n","Epoch 331/4000 - Train Loss: 0.2554, Val Loss: 0.2522\n","Epoch 332/4000 - Train Loss: 0.2514, Val Loss: 0.2522\n","Epoch 333/4000 - Train Loss: 0.2515, Val Loss: 0.2522\n","Epoch 334/4000 - Train Loss: 0.2504, Val Loss: 0.2522\n","Epoch 335/4000 - Train Loss: 0.2515, Val Loss: 0.2521\n","Epoch 336/4000 - Train Loss: 0.2520, Val Loss: 0.2521\n","Epoch 337/4000 - Train Loss: 0.2540, Val Loss: 0.2521\n","Epoch 338/4000 - Train Loss: 0.2569, Val Loss: 0.2515\n","Epoch 339/4000 - Train Loss: 0.2518, Val Loss: 0.2509 *\n","Epoch 340/4000 - Train Loss: 0.2524, Val Loss: 0.2504 *\n","Epoch 341/4000 - Train Loss: 0.2537, Val Loss: 0.2501 *\n","Epoch 342/4000 - Train Loss: 0.2506, Val Loss: 0.2497 *\n","Epoch 343/4000 - Train Loss: 0.2514, Val Loss: 0.2498\n","Epoch 344/4000 - Train Loss: 0.2514, Val Loss: 0.2505\n","Epoch 345/4000 - Train Loss: 0.2513, Val Loss: 0.2501\n","Epoch 346/4000 - Train Loss: 0.2522, Val Loss: 0.2502\n","Epoch 347/4000 - Train Loss: 0.2504, Val Loss: 0.2505\n","Epoch 348/4000 - Train Loss: 0.2526, Val Loss: 0.2508\n","Epoch 349/4000 - Train Loss: 0.2524, Val Loss: 0.2506\n","Epoch 350/4000 - Train Loss: 0.2488, Val Loss: 0.2517\n","Epoch 351/4000 - Train Loss: 0.2524, Val Loss: 0.2518\n","Epoch 352/4000 - Train Loss: 0.2494, Val Loss: 0.2503\n","Epoch 353/4000 - Train Loss: 0.2511, Val Loss: 0.2501\n","Epoch 354/4000 - Train Loss: 0.2512, Val Loss: 0.2496 *\n","Epoch 355/4000 - Train Loss: 0.2502, Val Loss: 0.2493 *\n","Epoch 356/4000 - Train Loss: 0.2498, Val Loss: 0.2503\n","Epoch 357/4000 - Train Loss: 0.2505, Val Loss: 0.2501\n","Epoch 358/4000 - Train Loss: 0.2510, Val Loss: 0.2487 *\n","Epoch 359/4000 - Train Loss: 0.2484, Val Loss: 0.2481 *\n","Epoch 360/4000 - Train Loss: 0.2492, Val Loss: 0.2482\n","Epoch 361/4000 - Train Loss: 0.2485, Val Loss: 0.2486\n","Epoch 362/4000 - Train Loss: 0.2483, Val Loss: 0.2497\n","Epoch 363/4000 - Train Loss: 0.2504, Val Loss: 0.2499\n","Epoch 364/4000 - Train Loss: 0.2490, Val Loss: 0.2490\n","Epoch 365/4000 - Train Loss: 0.2512, Val Loss: 0.2485\n","Epoch 366/4000 - Train Loss: 0.2500, Val Loss: 0.2486\n","Epoch 367/4000 - Train Loss: 0.2494, Val Loss: 0.2474 *\n","Epoch 368/4000 - Train Loss: 0.2488, Val Loss: 0.2482\n","Epoch 369/4000 - Train Loss: 0.2495, Val Loss: 0.2480\n","Epoch 370/4000 - Train Loss: 0.2507, Val Loss: 0.2454 *\n","Epoch 371/4000 - Train Loss: 0.2472, Val Loss: 0.2448 *\n","Epoch 372/4000 - Train Loss: 0.2483, Val Loss: 0.2452\n","Epoch 373/4000 - Train Loss: 0.2490, Val Loss: 0.2448\n","Epoch 374/4000 - Train Loss: 0.2488, Val Loss: 0.2465\n","Epoch 375/4000 - Train Loss: 0.2488, Val Loss: 0.2483\n","Epoch 376/4000 - Train Loss: 0.2463, Val Loss: 0.2477\n","Epoch 377/4000 - Train Loss: 0.2460, Val Loss: 0.2474\n","Epoch 378/4000 - Train Loss: 0.2479, Val Loss: 0.2478\n","Epoch 379/4000 - Train Loss: 0.2478, Val Loss: 0.2478\n","Epoch 380/4000 - Train Loss: 0.2457, Val Loss: 0.2481\n","Epoch 381/4000 - Train Loss: 0.2493, Val Loss: 0.2480\n","Epoch 382/4000 - Train Loss: 0.2468, Val Loss: 0.2469\n","Epoch 383/4000 - Train Loss: 0.2483, Val Loss: 0.2457\n","Epoch 384/4000 - Train Loss: 0.2447, Val Loss: 0.2456\n","Epoch 385/4000 - Train Loss: 0.2452, Val Loss: 0.2447 *\n","Epoch 386/4000 - Train Loss: 0.2460, Val Loss: 0.2440 *\n","Epoch 387/4000 - Train Loss: 0.2475, Val Loss: 0.2444\n","Epoch 388/4000 - Train Loss: 0.2447, Val Loss: 0.2451\n","Epoch 389/4000 - Train Loss: 0.2465, Val Loss: 0.2444\n","Epoch 390/4000 - Train Loss: 0.2454, Val Loss: 0.2439\n","Epoch 391/4000 - Train Loss: 0.2452, Val Loss: 0.2445\n","Epoch 392/4000 - Train Loss: 0.2465, Val Loss: 0.2442\n","Epoch 393/4000 - Train Loss: 0.2476, Val Loss: 0.2452\n","Epoch 394/4000 - Train Loss: 0.2443, Val Loss: 0.2461\n","Epoch 395/4000 - Train Loss: 0.2447, Val Loss: 0.2462\n","Epoch 396/4000 - Train Loss: 0.2477, Val Loss: 0.2455\n","Epoch 397/4000 - Train Loss: 0.2446, Val Loss: 0.2455\n","Epoch 398/4000 - Train Loss: 0.2461, Val Loss: 0.2447\n","Epoch 399/4000 - Train Loss: 0.2420, Val Loss: 0.2444\n","Epoch 400/4000 - Train Loss: 0.2445, Val Loss: 0.2447\n","Epoch 401/4000 - Train Loss: 0.2443, Val Loss: 0.2439 *\n","Epoch 402/4000 - Train Loss: 0.2424, Val Loss: 0.2427 *\n","Epoch 403/4000 - Train Loss: 0.2413, Val Loss: 0.2422 *\n","Epoch 404/4000 - Train Loss: 0.2426, Val Loss: 0.2416 *\n","Epoch 405/4000 - Train Loss: 0.2434, Val Loss: 0.2414 *\n","Epoch 406/4000 - Train Loss: 0.2438, Val Loss: 0.2423\n","Epoch 407/4000 - Train Loss: 0.2413, Val Loss: 0.2427\n","Epoch 408/4000 - Train Loss: 0.2461, Val Loss: 0.2426\n","Epoch 409/4000 - Train Loss: 0.2454, Val Loss: 0.2436\n","Epoch 410/4000 - Train Loss: 0.2442, Val Loss: 0.2452\n","Epoch 411/4000 - Train Loss: 0.2436, Val Loss: 0.2435\n","Epoch 412/4000 - Train Loss: 0.2437, Val Loss: 0.2433\n","Epoch 413/4000 - Train Loss: 0.2441, Val Loss: 0.2434\n","Epoch 414/4000 - Train Loss: 0.2422, Val Loss: 0.2425\n","Epoch 415/4000 - Train Loss: 0.2376, Val Loss: 0.2419\n","Epoch 416/4000 - Train Loss: 0.2395, Val Loss: 0.2415\n","Epoch 417/4000 - Train Loss: 0.2423, Val Loss: 0.2415\n","Epoch 418/4000 - Train Loss: 0.2425, Val Loss: 0.2413 *\n","Epoch 419/4000 - Train Loss: 0.2420, Val Loss: 0.2425\n","Epoch 420/4000 - Train Loss: 0.2413, Val Loss: 0.2435\n","Epoch 421/4000 - Train Loss: 0.2413, Val Loss: 0.2421\n","Epoch 422/4000 - Train Loss: 0.2390, Val Loss: 0.2417\n","Epoch 423/4000 - Train Loss: 0.2426, Val Loss: 0.2418\n","Epoch 424/4000 - Train Loss: 0.2415, Val Loss: 0.2409 *\n","Epoch 425/4000 - Train Loss: 0.2410, Val Loss: 0.2403 *\n","Epoch 426/4000 - Train Loss: 0.2401, Val Loss: 0.2410\n","Epoch 427/4000 - Train Loss: 0.2412, Val Loss: 0.2401 *\n","Epoch 428/4000 - Train Loss: 0.2413, Val Loss: 0.2392 *\n","Epoch 429/4000 - Train Loss: 0.2365, Val Loss: 0.2393\n","Epoch 430/4000 - Train Loss: 0.2408, Val Loss: 0.2394\n","Epoch 431/4000 - Train Loss: 0.2394, Val Loss: 0.2393\n","Epoch 432/4000 - Train Loss: 0.2401, Val Loss: 0.2397\n","Epoch 433/4000 - Train Loss: 0.2387, Val Loss: 0.2397\n","Epoch 434/4000 - Train Loss: 0.2389, Val Loss: 0.2395\n","Epoch 435/4000 - Train Loss: 0.2368, Val Loss: 0.2394\n","Epoch 436/4000 - Train Loss: 0.2402, Val Loss: 0.2395\n","Epoch 437/4000 - Train Loss: 0.2393, Val Loss: 0.2395\n","Epoch 438/4000 - Train Loss: 0.2385, Val Loss: 0.2396\n","Epoch 439/4000 - Train Loss: 0.2398, Val Loss: 0.2391 *\n","Epoch 440/4000 - Train Loss: 0.2405, Val Loss: 0.2387 *\n","Epoch 441/4000 - Train Loss: 0.2362, Val Loss: 0.2381 *\n","Epoch 442/4000 - Train Loss: 0.2411, Val Loss: 0.2374 *\n","Epoch 443/4000 - Train Loss: 0.2373, Val Loss: 0.2374\n","Epoch 444/4000 - Train Loss: 0.2402, Val Loss: 0.2374\n","Epoch 445/4000 - Train Loss: 0.2377, Val Loss: 0.2372 *\n","Epoch 446/4000 - Train Loss: 0.2390, Val Loss: 0.2373\n","Epoch 447/4000 - Train Loss: 0.2377, Val Loss: 0.2374\n","Epoch 448/4000 - Train Loss: 0.2398, Val Loss: 0.2384\n","Epoch 449/4000 - Train Loss: 0.2374, Val Loss: 0.2395\n","Epoch 450/4000 - Train Loss: 0.2392, Val Loss: 0.2393\n","Epoch 451/4000 - Train Loss: 0.2370, Val Loss: 0.2393\n","Epoch 452/4000 - Train Loss: 0.2392, Val Loss: 0.2389\n","Epoch 453/4000 - Train Loss: 0.2398, Val Loss: 0.2385\n","Epoch 454/4000 - Train Loss: 0.2383, Val Loss: 0.2377\n","Epoch 455/4000 - Train Loss: 0.2368, Val Loss: 0.2369 *\n","Epoch 456/4000 - Train Loss: 0.2360, Val Loss: 0.2373\n","Epoch 457/4000 - Train Loss: 0.2366, Val Loss: 0.2364 *\n","Epoch 458/4000 - Train Loss: 0.2345, Val Loss: 0.2362 *\n","Epoch 459/4000 - Train Loss: 0.2343, Val Loss: 0.2365\n","Epoch 460/4000 - Train Loss: 0.2369, Val Loss: 0.2365\n","Epoch 461/4000 - Train Loss: 0.2342, Val Loss: 0.2365\n","Epoch 462/4000 - Train Loss: 0.2340, Val Loss: 0.2371\n","Epoch 463/4000 - Train Loss: 0.2370, Val Loss: 0.2369\n","Epoch 464/4000 - Train Loss: 0.2346, Val Loss: 0.2365\n","Epoch 465/4000 - Train Loss: 0.2343, Val Loss: 0.2364\n","Epoch 466/4000 - Train Loss: 0.2375, Val Loss: 0.2362\n","Epoch 467/4000 - Train Loss: 0.2318, Val Loss: 0.2354 *\n","Epoch 468/4000 - Train Loss: 0.2341, Val Loss: 0.2350 *\n","Epoch 469/4000 - Train Loss: 0.2349, Val Loss: 0.2341 *\n","Epoch 470/4000 - Train Loss: 0.2334, Val Loss: 0.2340 *\n","Epoch 471/4000 - Train Loss: 0.2374, Val Loss: 0.2335 *\n","Epoch 472/4000 - Train Loss: 0.2379, Val Loss: 0.2350\n","Epoch 473/4000 - Train Loss: 0.2383, Val Loss: 0.2341\n","Epoch 474/4000 - Train Loss: 0.2342, Val Loss: 0.2350\n","Epoch 475/4000 - Train Loss: 0.2364, Val Loss: 0.2350\n","Epoch 476/4000 - Train Loss: 0.2360, Val Loss: 0.2346\n","Epoch 477/4000 - Train Loss: 0.2336, Val Loss: 0.2359\n","Epoch 478/4000 - Train Loss: 0.2348, Val Loss: 0.2356\n","Epoch 479/4000 - Train Loss: 0.2338, Val Loss: 0.2347\n","Epoch 480/4000 - Train Loss: 0.2342, Val Loss: 0.2355\n","Epoch 481/4000 - Train Loss: 0.2343, Val Loss: 0.2350\n","Epoch 482/4000 - Train Loss: 0.2316, Val Loss: 0.2344\n","Epoch 483/4000 - Train Loss: 0.2339, Val Loss: 0.2333 *\n","Epoch 484/4000 - Train Loss: 0.2338, Val Loss: 0.2325 *\n","Epoch 485/4000 - Train Loss: 0.2336, Val Loss: 0.2319 *\n","Epoch 486/4000 - Train Loss: 0.2344, Val Loss: 0.2319\n","Epoch 487/4000 - Train Loss: 0.2324, Val Loss: 0.2321\n","Epoch 488/4000 - Train Loss: 0.2332, Val Loss: 0.2322\n","Epoch 489/4000 - Train Loss: 0.2350, Val Loss: 0.2337\n","Epoch 490/4000 - Train Loss: 0.2319, Val Loss: 0.2343\n","Epoch 491/4000 - Train Loss: 0.2339, Val Loss: 0.2351\n","Epoch 492/4000 - Train Loss: 0.2336, Val Loss: 0.2361\n","Epoch 493/4000 - Train Loss: 0.2318, Val Loss: 0.2349\n","Epoch 494/4000 - Train Loss: 0.2327, Val Loss: 0.2340\n","Epoch 495/4000 - Train Loss: 0.2344, Val Loss: 0.2332\n","Epoch 496/4000 - Train Loss: 0.2295, Val Loss: 0.2325\n","Epoch 497/4000 - Train Loss: 0.2303, Val Loss: 0.2320\n","Epoch 498/4000 - Train Loss: 0.2321, Val Loss: 0.2310 *\n","Epoch 499/4000 - Train Loss: 0.2285, Val Loss: 0.2307 *\n","Epoch 500/4000 - Train Loss: 0.2328, Val Loss: 0.2304 *\n","Epoch 501/4000 - Train Loss: 0.2321, Val Loss: 0.2303 *\n","Epoch 502/4000 - Train Loss: 0.2326, Val Loss: 0.2309\n","Epoch 503/4000 - Train Loss: 0.2339, Val Loss: 0.2310\n","Epoch 504/4000 - Train Loss: 0.2323, Val Loss: 0.2318\n","Epoch 505/4000 - Train Loss: 0.2311, Val Loss: 0.2335\n","Epoch 506/4000 - Train Loss: 0.2339, Val Loss: 0.2333\n","Epoch 507/4000 - Train Loss: 0.2340, Val Loss: 0.2332\n","Epoch 508/4000 - Train Loss: 0.2325, Val Loss: 0.2336\n","Epoch 509/4000 - Train Loss: 0.2317, Val Loss: 0.2327\n","Epoch 510/4000 - Train Loss: 0.2338, Val Loss: 0.2318\n","Epoch 511/4000 - Train Loss: 0.2288, Val Loss: 0.2313\n","Epoch 512/4000 - Train Loss: 0.2315, Val Loss: 0.2305\n","Epoch 513/4000 - Train Loss: 0.2321, Val Loss: 0.2304\n","Epoch 514/4000 - Train Loss: 0.2310, Val Loss: 0.2307\n","Epoch 515/4000 - Train Loss: 0.2312, Val Loss: 0.2314\n","Epoch 516/4000 - Train Loss: 0.2290, Val Loss: 0.2310\n","Epoch 517/4000 - Train Loss: 0.2305, Val Loss: 0.2305\n","Epoch 518/4000 - Train Loss: 0.2274, Val Loss: 0.2312\n","Epoch 519/4000 - Train Loss: 0.2311, Val Loss: 0.2307\n","Epoch 520/4000 - Train Loss: 0.2297, Val Loss: 0.2313\n","Epoch 521/4000 - Train Loss: 0.2306, Val Loss: 0.2316\n","Epoch 522/4000 - Train Loss: 0.2336, Val Loss: 0.2308\n","Epoch 523/4000 - Train Loss: 0.2323, Val Loss: 0.2297 *\n","Epoch 524/4000 - Train Loss: 0.2280, Val Loss: 0.2294 *\n","Epoch 525/4000 - Train Loss: 0.2291, Val Loss: 0.2288 *\n","Epoch 526/4000 - Train Loss: 0.2297, Val Loss: 0.2290\n","Epoch 527/4000 - Train Loss: 0.2288, Val Loss: 0.2285 *\n","Epoch 528/4000 - Train Loss: 0.2291, Val Loss: 0.2281 *\n","Epoch 529/4000 - Train Loss: 0.2286, Val Loss: 0.2280\n","Epoch 530/4000 - Train Loss: 0.2304, Val Loss: 0.2277 *\n","Epoch 531/4000 - Train Loss: 0.2285, Val Loss: 0.2279\n","Epoch 532/4000 - Train Loss: 0.2277, Val Loss: 0.2280\n","Epoch 533/4000 - Train Loss: 0.2306, Val Loss: 0.2285\n","Epoch 534/4000 - Train Loss: 0.2281, Val Loss: 0.2290\n","Epoch 535/4000 - Train Loss: 0.2274, Val Loss: 0.2294\n","Epoch 536/4000 - Train Loss: 0.2281, Val Loss: 0.2290\n","Epoch 537/4000 - Train Loss: 0.2269, Val Loss: 0.2291\n","Epoch 538/4000 - Train Loss: 0.2279, Val Loss: 0.2292\n","Epoch 539/4000 - Train Loss: 0.2277, Val Loss: 0.2284\n","Epoch 540/4000 - Train Loss: 0.2296, Val Loss: 0.2289\n","Epoch 541/4000 - Train Loss: 0.2264, Val Loss: 0.2282\n","Epoch 542/4000 - Train Loss: 0.2253, Val Loss: 0.2268 *\n","Epoch 543/4000 - Train Loss: 0.2295, Val Loss: 0.2265 *\n","Epoch 544/4000 - Train Loss: 0.2275, Val Loss: 0.2260 *\n","Epoch 545/4000 - Train Loss: 0.2299, Val Loss: 0.2271\n","Epoch 546/4000 - Train Loss: 0.2294, Val Loss: 0.2266\n","Epoch 547/4000 - Train Loss: 0.2265, Val Loss: 0.2261\n","Epoch 548/4000 - Train Loss: 0.2282, Val Loss: 0.2261\n","Epoch 549/4000 - Train Loss: 0.2260, Val Loss: 0.2264\n","Epoch 550/4000 - Train Loss: 0.2277, Val Loss: 0.2274\n","Epoch 551/4000 - Train Loss: 0.2270, Val Loss: 0.2278\n","Epoch 552/4000 - Train Loss: 0.2252, Val Loss: 0.2269\n","Epoch 553/4000 - Train Loss: 0.2268, Val Loss: 0.2263\n","Epoch 554/4000 - Train Loss: 0.2264, Val Loss: 0.2260\n","Epoch 555/4000 - Train Loss: 0.2258, Val Loss: 0.2261\n","Epoch 556/4000 - Train Loss: 0.2248, Val Loss: 0.2271\n","Epoch 557/4000 - Train Loss: 0.2308, Val Loss: 0.2251 *\n","Epoch 558/4000 - Train Loss: 0.2258, Val Loss: 0.2243 *\n","Epoch 559/4000 - Train Loss: 0.2262, Val Loss: 0.2244\n","Epoch 560/4000 - Train Loss: 0.2276, Val Loss: 0.2249\n","Epoch 561/4000 - Train Loss: 0.2262, Val Loss: 0.2266\n","Epoch 562/4000 - Train Loss: 0.2276, Val Loss: 0.2255\n","Epoch 563/4000 - Train Loss: 0.2287, Val Loss: 0.2246\n","Epoch 564/4000 - Train Loss: 0.2250, Val Loss: 0.2249\n","Epoch 565/4000 - Train Loss: 0.2269, Val Loss: 0.2244\n","Epoch 566/4000 - Train Loss: 0.2279, Val Loss: 0.2252\n","Epoch 567/4000 - Train Loss: 0.2247, Val Loss: 0.2252\n","Epoch 568/4000 - Train Loss: 0.2244, Val Loss: 0.2243\n","Epoch 569/4000 - Train Loss: 0.2264, Val Loss: 0.2244\n","Epoch 570/4000 - Train Loss: 0.2276, Val Loss: 0.2241 *\n","Epoch 571/4000 - Train Loss: 0.2276, Val Loss: 0.2251\n","Epoch 572/4000 - Train Loss: 0.2259, Val Loss: 0.2257\n","Epoch 573/4000 - Train Loss: 0.2248, Val Loss: 0.2233 *\n","Epoch 574/4000 - Train Loss: 0.2231, Val Loss: 0.2233\n","Epoch 575/4000 - Train Loss: 0.2267, Val Loss: 0.2230 *\n","Epoch 576/4000 - Train Loss: 0.2282, Val Loss: 0.2242\n","Epoch 577/4000 - Train Loss: 0.2230, Val Loss: 0.2253\n","Epoch 578/4000 - Train Loss: 0.2260, Val Loss: 0.2242\n","Epoch 579/4000 - Train Loss: 0.2246, Val Loss: 0.2248\n","Epoch 580/4000 - Train Loss: 0.2231, Val Loss: 0.2249\n","Epoch 581/4000 - Train Loss: 0.2231, Val Loss: 0.2252\n","Epoch 582/4000 - Train Loss: 0.2265, Val Loss: 0.2250\n","Epoch 583/4000 - Train Loss: 0.2226, Val Loss: 0.2238\n","Epoch 584/4000 - Train Loss: 0.2239, Val Loss: 0.2235\n","Epoch 585/4000 - Train Loss: 0.2258, Val Loss: 0.2233\n","Epoch 586/4000 - Train Loss: 0.2268, Val Loss: 0.2232\n","Epoch 587/4000 - Train Loss: 0.2243, Val Loss: 0.2237\n","Epoch 588/4000 - Train Loss: 0.2223, Val Loss: 0.2254\n","Epoch 589/4000 - Train Loss: 0.2249, Val Loss: 0.2265\n","Epoch 590/4000 - Train Loss: 0.2234, Val Loss: 0.2263\n","Epoch 591/4000 - Train Loss: 0.2233, Val Loss: 0.2253\n","Epoch 592/4000 - Train Loss: 0.2246, Val Loss: 0.2236\n","Epoch 593/4000 - Train Loss: 0.2251, Val Loss: 0.2227 *\n","Epoch 594/4000 - Train Loss: 0.2269, Val Loss: 0.2234\n","Epoch 595/4000 - Train Loss: 0.2275, Val Loss: 0.2231\n","Epoch 596/4000 - Train Loss: 0.2258, Val Loss: 0.2258\n","Epoch 597/4000 - Train Loss: 0.2233, Val Loss: 0.2247\n","Epoch 598/4000 - Train Loss: 0.2246, Val Loss: 0.2222 *\n","Epoch 599/4000 - Train Loss: 0.2213, Val Loss: 0.2226\n","Epoch 600/4000 - Train Loss: 0.2233, Val Loss: 0.2217 *\n","Epoch 601/4000 - Train Loss: 0.2213, Val Loss: 0.2216\n","Epoch 602/4000 - Train Loss: 0.2256, Val Loss: 0.2247\n","Epoch 603/4000 - Train Loss: 0.2222, Val Loss: 0.2228\n","Epoch 604/4000 - Train Loss: 0.2233, Val Loss: 0.2211 *\n","Epoch 605/4000 - Train Loss: 0.2240, Val Loss: 0.2237\n","Epoch 606/4000 - Train Loss: 0.2237, Val Loss: 0.2225\n","Epoch 607/4000 - Train Loss: 0.2224, Val Loss: 0.2249\n","Epoch 608/4000 - Train Loss: 0.2248, Val Loss: 0.2274\n","Epoch 609/4000 - Train Loss: 0.2253, Val Loss: 0.2237\n","Epoch 610/4000 - Train Loss: 0.2239, Val Loss: 0.2226\n","Epoch 611/4000 - Train Loss: 0.2223, Val Loss: 0.2222\n","Epoch 612/4000 - Train Loss: 0.2228, Val Loss: 0.2211\n","Epoch 613/4000 - Train Loss: 0.2217, Val Loss: 0.2241\n","Epoch 614/4000 - Train Loss: 0.2291, Val Loss: 0.2213\n","Epoch 615/4000 - Train Loss: 0.2226, Val Loss: 0.2205 *\n","Epoch 616/4000 - Train Loss: 0.2209, Val Loss: 0.2226\n","Epoch 617/4000 - Train Loss: 0.2238, Val Loss: 0.2211\n","Epoch 618/4000 - Train Loss: 0.2222, Val Loss: 0.2241\n","Epoch 619/4000 - Train Loss: 0.2253, Val Loss: 0.2257\n","Epoch 620/4000 - Train Loss: 0.2243, Val Loss: 0.2220\n","Epoch 621/4000 - Train Loss: 0.2200, Val Loss: 0.2205\n","Epoch 622/4000 - Train Loss: 0.2194, Val Loss: 0.2209\n","Epoch 623/4000 - Train Loss: 0.2246, Val Loss: 0.2198 *\n","Epoch 624/4000 - Train Loss: 0.2224, Val Loss: 0.2216\n","Epoch 625/4000 - Train Loss: 0.2219, Val Loss: 0.2227\n","Epoch 626/4000 - Train Loss: 0.2237, Val Loss: 0.2204\n","Epoch 627/4000 - Train Loss: 0.2205, Val Loss: 0.2192 *\n","Epoch 628/4000 - Train Loss: 0.2199, Val Loss: 0.2192\n","Epoch 629/4000 - Train Loss: 0.2214, Val Loss: 0.2192\n","Epoch 630/4000 - Train Loss: 0.2183, Val Loss: 0.2194\n","Epoch 631/4000 - Train Loss: 0.2220, Val Loss: 0.2201\n","Epoch 632/4000 - Train Loss: 0.2240, Val Loss: 0.2200\n","Epoch 633/4000 - Train Loss: 0.2199, Val Loss: 0.2197\n","Epoch 634/4000 - Train Loss: 0.2187, Val Loss: 0.2195\n","Epoch 635/4000 - Train Loss: 0.2203, Val Loss: 0.2193\n","Epoch 636/4000 - Train Loss: 0.2193, Val Loss: 0.2193\n","Epoch 637/4000 - Train Loss: 0.2187, Val Loss: 0.2195\n","Epoch 638/4000 - Train Loss: 0.2190, Val Loss: 0.2194\n","Epoch 639/4000 - Train Loss: 0.2188, Val Loss: 0.2188 *\n","Epoch 640/4000 - Train Loss: 0.2191, Val Loss: 0.2186 *\n","Epoch 641/4000 - Train Loss: 0.2193, Val Loss: 0.2188\n","Epoch 642/4000 - Train Loss: 0.2188, Val Loss: 0.2185 *\n","Epoch 643/4000 - Train Loss: 0.2208, Val Loss: 0.2193\n","Epoch 644/4000 - Train Loss: 0.2183, Val Loss: 0.2196\n","Epoch 645/4000 - Train Loss: 0.2174, Val Loss: 0.2189\n","Epoch 646/4000 - Train Loss: 0.2216, Val Loss: 0.2186\n","Epoch 647/4000 - Train Loss: 0.2187, Val Loss: 0.2186\n","Epoch 648/4000 - Train Loss: 0.2182, Val Loss: 0.2182 *\n","Epoch 649/4000 - Train Loss: 0.2204, Val Loss: 0.2177 *\n","Epoch 650/4000 - Train Loss: 0.2187, Val Loss: 0.2172 *\n","Epoch 651/4000 - Train Loss: 0.2174, Val Loss: 0.2170 *\n","Epoch 652/4000 - Train Loss: 0.2198, Val Loss: 0.2175\n","Epoch 653/4000 - Train Loss: 0.2183, Val Loss: 0.2164 *\n","Epoch 654/4000 - Train Loss: 0.2179, Val Loss: 0.2157 *\n","Epoch 655/4000 - Train Loss: 0.2182, Val Loss: 0.2159\n","Epoch 656/4000 - Train Loss: 0.2210, Val Loss: 0.2157\n","Epoch 657/4000 - Train Loss: 0.2182, Val Loss: 0.2176\n","Epoch 658/4000 - Train Loss: 0.2206, Val Loss: 0.2174\n","Epoch 659/4000 - Train Loss: 0.2177, Val Loss: 0.2166\n","Epoch 660/4000 - Train Loss: 0.2173, Val Loss: 0.2174\n","Epoch 661/4000 - Train Loss: 0.2193, Val Loss: 0.2169\n","Epoch 662/4000 - Train Loss: 0.2161, Val Loss: 0.2183\n","Epoch 663/4000 - Train Loss: 0.2181, Val Loss: 0.2188\n","Epoch 664/4000 - Train Loss: 0.2188, Val Loss: 0.2170\n","Epoch 665/4000 - Train Loss: 0.2167, Val Loss: 0.2160\n","Epoch 666/4000 - Train Loss: 0.2150, Val Loss: 0.2161\n","Epoch 667/4000 - Train Loss: 0.2189, Val Loss: 0.2167\n","Epoch 668/4000 - Train Loss: 0.2151, Val Loss: 0.2176\n","Epoch 669/4000 - Train Loss: 0.2177, Val Loss: 0.2165\n","Epoch 670/4000 - Train Loss: 0.2175, Val Loss: 0.2149 *\n","Epoch 671/4000 - Train Loss: 0.2188, Val Loss: 0.2146 *\n","Epoch 672/4000 - Train Loss: 0.2179, Val Loss: 0.2140 *\n","Epoch 673/4000 - Train Loss: 0.2192, Val Loss: 0.2148\n","Epoch 674/4000 - Train Loss: 0.2186, Val Loss: 0.2174\n","Epoch 675/4000 - Train Loss: 0.2192, Val Loss: 0.2160\n","Epoch 676/4000 - Train Loss: 0.2144, Val Loss: 0.2147\n","Epoch 677/4000 - Train Loss: 0.2138, Val Loss: 0.2155\n","Epoch 678/4000 - Train Loss: 0.2159, Val Loss: 0.2154\n","Epoch 679/4000 - Train Loss: 0.2170, Val Loss: 0.2166\n","Epoch 680/4000 - Train Loss: 0.2161, Val Loss: 0.2173\n","Epoch 681/4000 - Train Loss: 0.2164, Val Loss: 0.2152\n","Epoch 682/4000 - Train Loss: 0.2152, Val Loss: 0.2133 *\n","Epoch 683/4000 - Train Loss: 0.2159, Val Loss: 0.2131 *\n","Epoch 684/4000 - Train Loss: 0.2148, Val Loss: 0.2133\n","Epoch 685/4000 - Train Loss: 0.2181, Val Loss: 0.2124 *\n","Epoch 686/4000 - Train Loss: 0.2145, Val Loss: 0.2140\n","Epoch 687/4000 - Train Loss: 0.2165, Val Loss: 0.2147\n","Epoch 688/4000 - Train Loss: 0.2195, Val Loss: 0.2140\n","Epoch 689/4000 - Train Loss: 0.2142, Val Loss: 0.2142\n","Epoch 690/4000 - Train Loss: 0.2145, Val Loss: 0.2150\n","Epoch 691/4000 - Train Loss: 0.2163, Val Loss: 0.2159\n","Epoch 692/4000 - Train Loss: 0.2149, Val Loss: 0.2176\n","Epoch 693/4000 - Train Loss: 0.2162, Val Loss: 0.2166\n","Epoch 694/4000 - Train Loss: 0.2179, Val Loss: 0.2136\n","Epoch 695/4000 - Train Loss: 0.2161, Val Loss: 0.2141\n","Epoch 696/4000 - Train Loss: 0.2171, Val Loss: 0.2129\n","Epoch 697/4000 - Train Loss: 0.2156, Val Loss: 0.2139\n","Epoch 698/4000 - Train Loss: 0.2160, Val Loss: 0.2169\n","Epoch 699/4000 - Train Loss: 0.2182, Val Loss: 0.2142\n","Epoch 700/4000 - Train Loss: 0.2134, Val Loss: 0.2131\n","Epoch 701/4000 - Train Loss: 0.2154, Val Loss: 0.2135\n","Epoch 702/4000 - Train Loss: 0.2143, Val Loss: 0.2133\n","Epoch 703/4000 - Train Loss: 0.2171, Val Loss: 0.2140\n","Epoch 704/4000 - Train Loss: 0.2147, Val Loss: 0.2141\n","Epoch 705/4000 - Train Loss: 0.2128, Val Loss: 0.2132\n","Epoch 706/4000 - Train Loss: 0.2099, Val Loss: 0.2127\n","Epoch 707/4000 - Train Loss: 0.2116, Val Loss: 0.2123 *\n","Epoch 708/4000 - Train Loss: 0.2116, Val Loss: 0.2122 *\n","Epoch 709/4000 - Train Loss: 0.2135, Val Loss: 0.2117 *\n","Epoch 710/4000 - Train Loss: 0.2126, Val Loss: 0.2113 *\n","Epoch 711/4000 - Train Loss: 0.2128, Val Loss: 0.2110 *\n","Epoch 712/4000 - Train Loss: 0.2129, Val Loss: 0.2106 *\n","Epoch 713/4000 - Train Loss: 0.2143, Val Loss: 0.2104 *\n","Epoch 714/4000 - Train Loss: 0.2124, Val Loss: 0.2106\n","Epoch 715/4000 - Train Loss: 0.2127, Val Loss: 0.2110\n","Epoch 716/4000 - Train Loss: 0.2104, Val Loss: 0.2110\n","Epoch 717/4000 - Train Loss: 0.2109, Val Loss: 0.2106\n","Epoch 718/4000 - Train Loss: 0.2152, Val Loss: 0.2112\n","Epoch 719/4000 - Train Loss: 0.2135, Val Loss: 0.2122\n","Epoch 720/4000 - Train Loss: 0.2144, Val Loss: 0.2136\n","Epoch 721/4000 - Train Loss: 0.2122, Val Loss: 0.2140\n","Epoch 722/4000 - Train Loss: 0.2143, Val Loss: 0.2135\n","Epoch 723/4000 - Train Loss: 0.2141, Val Loss: 0.2121\n","Epoch 724/4000 - Train Loss: 0.2129, Val Loss: 0.2109\n","Epoch 725/4000 - Train Loss: 0.2128, Val Loss: 0.2107\n","Epoch 726/4000 - Train Loss: 0.2126, Val Loss: 0.2096 *\n","Epoch 727/4000 - Train Loss: 0.2102, Val Loss: 0.2091 *\n","Epoch 728/4000 - Train Loss: 0.2141, Val Loss: 0.2088 *\n","Epoch 729/4000 - Train Loss: 0.2126, Val Loss: 0.2091\n","Epoch 730/4000 - Train Loss: 0.2118, Val Loss: 0.2094\n","Epoch 731/4000 - Train Loss: 0.2143, Val Loss: 0.2103\n","Epoch 732/4000 - Train Loss: 0.2124, Val Loss: 0.2118\n","Epoch 733/4000 - Train Loss: 0.2107, Val Loss: 0.2134\n","Epoch 734/4000 - Train Loss: 0.2114, Val Loss: 0.2146\n","Epoch 735/4000 - Train Loss: 0.2135, Val Loss: 0.2138\n","Epoch 736/4000 - Train Loss: 0.2130, Val Loss: 0.2116\n","Epoch 737/4000 - Train Loss: 0.2134, Val Loss: 0.2114\n","Epoch 738/4000 - Train Loss: 0.2142, Val Loss: 0.2102\n","Epoch 739/4000 - Train Loss: 0.2119, Val Loss: 0.2123\n","Epoch 740/4000 - Train Loss: 0.2124, Val Loss: 0.2120\n","Epoch 741/4000 - Train Loss: 0.2106, Val Loss: 0.2092\n","Epoch 742/4000 - Train Loss: 0.2101, Val Loss: 0.2084 *\n","Epoch 743/4000 - Train Loss: 0.2105, Val Loss: 0.2081 *\n","Epoch 744/4000 - Train Loss: 0.2105, Val Loss: 0.2093\n","Epoch 745/4000 - Train Loss: 0.2123, Val Loss: 0.2114\n","Epoch 746/4000 - Train Loss: 0.2121, Val Loss: 0.2094\n","Epoch 747/4000 - Train Loss: 0.2148, Val Loss: 0.2084\n","Epoch 748/4000 - Train Loss: 0.2092, Val Loss: 0.2090\n","Epoch 749/4000 - Train Loss: 0.2113, Val Loss: 0.2093\n","Epoch 750/4000 - Train Loss: 0.2123, Val Loss: 0.2098\n","Epoch 751/4000 - Train Loss: 0.2097, Val Loss: 0.2110\n","Epoch 752/4000 - Train Loss: 0.2113, Val Loss: 0.2111\n","Epoch 753/4000 - Train Loss: 0.2105, Val Loss: 0.2102\n","Epoch 754/4000 - Train Loss: 0.2090, Val Loss: 0.2089\n","Epoch 755/4000 - Train Loss: 0.2090, Val Loss: 0.2081\n","Epoch 756/4000 - Train Loss: 0.2112, Val Loss: 0.2078 *\n","Epoch 757/4000 - Train Loss: 0.2093, Val Loss: 0.2084\n","Epoch 758/4000 - Train Loss: 0.2127, Val Loss: 0.2080\n","Epoch 759/4000 - Train Loss: 0.2094, Val Loss: 0.2066 *\n","Epoch 760/4000 - Train Loss: 0.2109, Val Loss: 0.2065 *\n","Epoch 761/4000 - Train Loss: 0.2111, Val Loss: 0.2067\n","Epoch 762/4000 - Train Loss: 0.2095, Val Loss: 0.2073\n","Epoch 763/4000 - Train Loss: 0.2111, Val Loss: 0.2077\n","Epoch 764/4000 - Train Loss: 0.2098, Val Loss: 0.2078\n","Epoch 765/4000 - Train Loss: 0.2099, Val Loss: 0.2072\n","Epoch 766/4000 - Train Loss: 0.2063, Val Loss: 0.2075\n","Epoch 767/4000 - Train Loss: 0.2077, Val Loss: 0.2082\n","Epoch 768/4000 - Train Loss: 0.2080, Val Loss: 0.2096\n","Epoch 769/4000 - Train Loss: 0.2077, Val Loss: 0.2096\n","Epoch 770/4000 - Train Loss: 0.2092, Val Loss: 0.2075\n","Epoch 771/4000 - Train Loss: 0.2093, Val Loss: 0.2068\n","Epoch 772/4000 - Train Loss: 0.2094, Val Loss: 0.2065\n","Epoch 773/4000 - Train Loss: 0.2089, Val Loss: 0.2068\n","Epoch 774/4000 - Train Loss: 0.2078, Val Loss: 0.2081\n","Epoch 775/4000 - Train Loss: 0.2078, Val Loss: 0.2071\n","Epoch 776/4000 - Train Loss: 0.2092, Val Loss: 0.2054 *\n","Epoch 777/4000 - Train Loss: 0.2076, Val Loss: 0.2050 *\n","Epoch 778/4000 - Train Loss: 0.2082, Val Loss: 0.2049\n","Epoch 779/4000 - Train Loss: 0.2082, Val Loss: 0.2058\n","Epoch 780/4000 - Train Loss: 0.2071, Val Loss: 0.2068\n","Epoch 781/4000 - Train Loss: 0.2066, Val Loss: 0.2063\n","Epoch 782/4000 - Train Loss: 0.2048, Val Loss: 0.2060\n","Epoch 783/4000 - Train Loss: 0.2063, Val Loss: 0.2059\n","Epoch 784/4000 - Train Loss: 0.2092, Val Loss: 0.2065\n","Epoch 785/4000 - Train Loss: 0.2067, Val Loss: 0.2073\n","Epoch 786/4000 - Train Loss: 0.2055, Val Loss: 0.2088\n","Epoch 787/4000 - Train Loss: 0.2087, Val Loss: 0.2082\n","Epoch 788/4000 - Train Loss: 0.2085, Val Loss: 0.2069\n","Epoch 789/4000 - Train Loss: 0.2068, Val Loss: 0.2061\n","Epoch 790/4000 - Train Loss: 0.2032, Val Loss: 0.2059\n","Epoch 791/4000 - Train Loss: 0.2077, Val Loss: 0.2059\n","Epoch 792/4000 - Train Loss: 0.2056, Val Loss: 0.2059\n","Epoch 793/4000 - Train Loss: 0.2068, Val Loss: 0.2057\n","Epoch 794/4000 - Train Loss: 0.2074, Val Loss: 0.2055\n","Epoch 795/4000 - Train Loss: 0.2080, Val Loss: 0.2056\n","Epoch 796/4000 - Train Loss: 0.2064, Val Loss: 0.2062\n","Epoch 797/4000 - Train Loss: 0.2057, Val Loss: 0.2055\n","Epoch 798/4000 - Train Loss: 0.2091, Val Loss: 0.2041 *\n","Epoch 799/4000 - Train Loss: 0.2064, Val Loss: 0.2040\n","Epoch 800/4000 - Train Loss: 0.2067, Val Loss: 0.2045\n","Epoch 801/4000 - Train Loss: 0.2061, Val Loss: 0.2051\n","Epoch 802/4000 - Train Loss: 0.2060, Val Loss: 0.2054\n","Epoch 803/4000 - Train Loss: 0.2054, Val Loss: 0.2054\n","Epoch 804/4000 - Train Loss: 0.2047, Val Loss: 0.2045\n","Epoch 805/4000 - Train Loss: 0.2048, Val Loss: 0.2041\n","Epoch 806/4000 - Train Loss: 0.2055, Val Loss: 0.2037 *\n","Epoch 807/4000 - Train Loss: 0.2085, Val Loss: 0.2049\n","Epoch 808/4000 - Train Loss: 0.2041, Val Loss: 0.2057\n","Epoch 809/4000 - Train Loss: 0.2057, Val Loss: 0.2040\n","Epoch 810/4000 - Train Loss: 0.2045, Val Loss: 0.2030 *\n","Epoch 811/4000 - Train Loss: 0.2032, Val Loss: 0.2033\n","Epoch 812/4000 - Train Loss: 0.2058, Val Loss: 0.2027 *\n","Epoch 813/4000 - Train Loss: 0.2057, Val Loss: 0.2055\n","Epoch 814/4000 - Train Loss: 0.2054, Val Loss: 0.2058\n","Epoch 815/4000 - Train Loss: 0.2074, Val Loss: 0.2036\n","Epoch 816/4000 - Train Loss: 0.2082, Val Loss: 0.2052\n","Epoch 817/4000 - Train Loss: 0.2079, Val Loss: 0.2053\n","Epoch 818/4000 - Train Loss: 0.2065, Val Loss: 0.2081\n","Epoch 819/4000 - Train Loss: 0.2043, Val Loss: 0.2089\n","Epoch 820/4000 - Train Loss: 0.2069, Val Loss: 0.2040\n","Epoch 821/4000 - Train Loss: 0.2079, Val Loss: 0.2025 *\n","Epoch 822/4000 - Train Loss: 0.2043, Val Loss: 0.2023 *\n","Epoch 823/4000 - Train Loss: 0.2050, Val Loss: 0.2026\n","Epoch 824/4000 - Train Loss: 0.2035, Val Loss: 0.2057\n","Epoch 825/4000 - Train Loss: 0.2028, Val Loss: 0.2076\n","Epoch 826/4000 - Train Loss: 0.2076, Val Loss: 0.2033\n","Epoch 827/4000 - Train Loss: 0.2027, Val Loss: 0.2034\n","Epoch 828/4000 - Train Loss: 0.2063, Val Loss: 0.2030\n","Epoch 829/4000 - Train Loss: 0.2055, Val Loss: 0.2023\n","Epoch 830/4000 - Train Loss: 0.2039, Val Loss: 0.2048\n","Epoch 831/4000 - Train Loss: 0.2025, Val Loss: 0.2056\n","Epoch 832/4000 - Train Loss: 0.2042, Val Loss: 0.2023\n","Epoch 833/4000 - Train Loss: 0.2035, Val Loss: 0.2003 *\n","Epoch 834/4000 - Train Loss: 0.2076, Val Loss: 0.2000 *\n","Epoch 835/4000 - Train Loss: 0.2043, Val Loss: 0.2006\n","Epoch 836/4000 - Train Loss: 0.2038, Val Loss: 0.2014\n","Epoch 837/4000 - Train Loss: 0.2018, Val Loss: 0.2021\n","Epoch 838/4000 - Train Loss: 0.2017, Val Loss: 0.2020\n","Epoch 839/4000 - Train Loss: 0.2030, Val Loss: 0.2012\n","Epoch 840/4000 - Train Loss: 0.2022, Val Loss: 0.2008\n","Epoch 841/4000 - Train Loss: 0.2033, Val Loss: 0.2011\n","Epoch 842/4000 - Train Loss: 0.2005, Val Loss: 0.2019\n","Epoch 843/4000 - Train Loss: 0.2031, Val Loss: 0.2018\n","Epoch 844/4000 - Train Loss: 0.2031, Val Loss: 0.2009\n","Epoch 845/4000 - Train Loss: 0.2016, Val Loss: 0.2007\n","Epoch 846/4000 - Train Loss: 0.2000, Val Loss: 0.2011\n","Epoch 847/4000 - Train Loss: 0.2028, Val Loss: 0.2017\n","Epoch 848/4000 - Train Loss: 0.2046, Val Loss: 0.2018\n","Epoch 849/4000 - Train Loss: 0.2036, Val Loss: 0.2014\n","Epoch 850/4000 - Train Loss: 0.2024, Val Loss: 0.2012\n","Epoch 851/4000 - Train Loss: 0.2023, Val Loss: 0.2020\n","Epoch 852/4000 - Train Loss: 0.2044, Val Loss: 0.2009\n","Epoch 853/4000 - Train Loss: 0.2008, Val Loss: 0.1991 *\n","Epoch 854/4000 - Train Loss: 0.2029, Val Loss: 0.1989 *\n","Epoch 855/4000 - Train Loss: 0.2031, Val Loss: 0.1992\n","Epoch 856/4000 - Train Loss: 0.2003, Val Loss: 0.2001\n","Epoch 857/4000 - Train Loss: 0.2037, Val Loss: 0.2005\n","Epoch 858/4000 - Train Loss: 0.2015, Val Loss: 0.2002\n","Epoch 859/4000 - Train Loss: 0.2002, Val Loss: 0.2003\n","Epoch 860/4000 - Train Loss: 0.2019, Val Loss: 0.2003\n","Epoch 861/4000 - Train Loss: 0.2002, Val Loss: 0.1996\n","Epoch 862/4000 - Train Loss: 0.2010, Val Loss: 0.1986 *\n","Epoch 863/4000 - Train Loss: 0.1996, Val Loss: 0.1986\n","Epoch 864/4000 - Train Loss: 0.2001, Val Loss: 0.1987\n","Epoch 865/4000 - Train Loss: 0.2032, Val Loss: 0.1983 *\n","Epoch 866/4000 - Train Loss: 0.2021, Val Loss: 0.1976 *\n","Epoch 867/4000 - Train Loss: 0.2013, Val Loss: 0.1976\n","Epoch 868/4000 - Train Loss: 0.1997, Val Loss: 0.1981\n","Epoch 869/4000 - Train Loss: 0.2021, Val Loss: 0.1993\n","Epoch 870/4000 - Train Loss: 0.2009, Val Loss: 0.2009\n","Epoch 871/4000 - Train Loss: 0.1998, Val Loss: 0.2010\n","Epoch 872/4000 - Train Loss: 0.1993, Val Loss: 0.1997\n","Epoch 873/4000 - Train Loss: 0.2025, Val Loss: 0.1983\n","Epoch 874/4000 - Train Loss: 0.2007, Val Loss: 0.1974 *\n","Epoch 875/4000 - Train Loss: 0.2008, Val Loss: 0.1972 *\n","Epoch 876/4000 - Train Loss: 0.2007, Val Loss: 0.1987\n","Epoch 877/4000 - Train Loss: 0.1994, Val Loss: 0.2003\n","Epoch 878/4000 - Train Loss: 0.2000, Val Loss: 0.1980\n","Epoch 879/4000 - Train Loss: 0.1991, Val Loss: 0.1975\n","Epoch 880/4000 - Train Loss: 0.1993, Val Loss: 0.1973\n","Epoch 881/4000 - Train Loss: 0.2012, Val Loss: 0.1980\n","Epoch 882/4000 - Train Loss: 0.1978, Val Loss: 0.2003\n","Epoch 883/4000 - Train Loss: 0.1989, Val Loss: 0.1994\n","Epoch 884/4000 - Train Loss: 0.1996, Val Loss: 0.1972\n","Epoch 885/4000 - Train Loss: 0.1995, Val Loss: 0.1965 *\n","Epoch 886/4000 - Train Loss: 0.1975, Val Loss: 0.1961 *\n","Epoch 887/4000 - Train Loss: 0.2009, Val Loss: 0.1962\n","Epoch 888/4000 - Train Loss: 0.1989, Val Loss: 0.1973\n","Epoch 889/4000 - Train Loss: 0.1987, Val Loss: 0.1993\n","Epoch 890/4000 - Train Loss: 0.1960, Val Loss: 0.1986\n","Epoch 891/4000 - Train Loss: 0.1999, Val Loss: 0.1970\n","Epoch 892/4000 - Train Loss: 0.1979, Val Loss: 0.1969\n","Epoch 893/4000 - Train Loss: 0.1993, Val Loss: 0.1968\n","Epoch 894/4000 - Train Loss: 0.1968, Val Loss: 0.1978\n","Epoch 895/4000 - Train Loss: 0.1984, Val Loss: 0.1980\n","Epoch 896/4000 - Train Loss: 0.1986, Val Loss: 0.1967\n","Epoch 897/4000 - Train Loss: 0.1957, Val Loss: 0.1955 *\n","Epoch 898/4000 - Train Loss: 0.2007, Val Loss: 0.1952 *\n","Epoch 899/4000 - Train Loss: 0.1974, Val Loss: 0.1953\n","Epoch 900/4000 - Train Loss: 0.1991, Val Loss: 0.1972\n","Epoch 901/4000 - Train Loss: 0.1984, Val Loss: 0.1983\n","Epoch 902/4000 - Train Loss: 0.1988, Val Loss: 0.1965\n","Epoch 903/4000 - Train Loss: 0.1955, Val Loss: 0.1958\n","Epoch 904/4000 - Train Loss: 0.1984, Val Loss: 0.1961\n","Epoch 905/4000 - Train Loss: 0.1983, Val Loss: 0.1971\n","Epoch 906/4000 - Train Loss: 0.1995, Val Loss: 0.1980\n","Epoch 907/4000 - Train Loss: 0.1969, Val Loss: 0.1982\n","Epoch 908/4000 - Train Loss: 0.1968, Val Loss: 0.1967\n","Epoch 909/4000 - Train Loss: 0.1978, Val Loss: 0.1955\n","Epoch 910/4000 - Train Loss: 0.1985, Val Loss: 0.1953\n","Epoch 911/4000 - Train Loss: 0.1974, Val Loss: 0.1958\n","Epoch 912/4000 - Train Loss: 0.1970, Val Loss: 0.1969\n","Epoch 913/4000 - Train Loss: 0.1975, Val Loss: 0.1958\n","Epoch 914/4000 - Train Loss: 0.1951, Val Loss: 0.1951 *\n","Epoch 915/4000 - Train Loss: 0.1969, Val Loss: 0.1949 *\n","Epoch 916/4000 - Train Loss: 0.1972, Val Loss: 0.1946 *\n","Epoch 917/4000 - Train Loss: 0.1975, Val Loss: 0.1953\n","Epoch 918/4000 - Train Loss: 0.1981, Val Loss: 0.1946\n","Epoch 919/4000 - Train Loss: 0.1977, Val Loss: 0.1935 *\n","Epoch 920/4000 - Train Loss: 0.1962, Val Loss: 0.1931 *\n","Epoch 921/4000 - Train Loss: 0.1990, Val Loss: 0.1933\n","Epoch 922/4000 - Train Loss: 0.1969, Val Loss: 0.1945\n","Epoch 923/4000 - Train Loss: 0.1954, Val Loss: 0.1956\n","Epoch 924/4000 - Train Loss: 0.1933, Val Loss: 0.1941\n","Epoch 925/4000 - Train Loss: 0.1954, Val Loss: 0.1936\n","Epoch 926/4000 - Train Loss: 0.1967, Val Loss: 0.1939\n","Epoch 927/4000 - Train Loss: 0.1970, Val Loss: 0.1946\n","Epoch 928/4000 - Train Loss: 0.1999, Val Loss: 0.1941\n","Epoch 929/4000 - Train Loss: 0.1960, Val Loss: 0.1942\n","Epoch 930/4000 - Train Loss: 0.1959, Val Loss: 0.1949\n","Epoch 931/4000 - Train Loss: 0.1964, Val Loss: 0.1958\n","Epoch 932/4000 - Train Loss: 0.1952, Val Loss: 0.1957\n","Epoch 933/4000 - Train Loss: 0.1973, Val Loss: 0.1932\n","Epoch 934/4000 - Train Loss: 0.1968, Val Loss: 0.1923 *\n","Epoch 935/4000 - Train Loss: 0.1957, Val Loss: 0.1927\n","Epoch 936/4000 - Train Loss: 0.1942, Val Loss: 0.1945\n","Epoch 937/4000 - Train Loss: 0.1959, Val Loss: 0.1947\n","Epoch 938/4000 - Train Loss: 0.1945, Val Loss: 0.1939\n","Epoch 939/4000 - Train Loss: 0.1930, Val Loss: 0.1937\n","Epoch 940/4000 - Train Loss: 0.1946, Val Loss: 0.1939\n","Epoch 941/4000 - Train Loss: 0.1917, Val Loss: 0.1949\n","Epoch 942/4000 - Train Loss: 0.1924, Val Loss: 0.1954\n","Epoch 943/4000 - Train Loss: 0.1947, Val Loss: 0.1932\n","Epoch 944/4000 - Train Loss: 0.1939, Val Loss: 0.1917 *\n","Epoch 945/4000 - Train Loss: 0.1964, Val Loss: 0.1916\n","Epoch 946/4000 - Train Loss: 0.1965, Val Loss: 0.1920\n","Epoch 947/4000 - Train Loss: 0.1958, Val Loss: 0.1912 *\n","Epoch 948/4000 - Train Loss: 0.1961, Val Loss: 0.1911\n","Epoch 949/4000 - Train Loss: 0.1914, Val Loss: 0.1918\n","Epoch 950/4000 - Train Loss: 0.1966, Val Loss: 0.1928\n","Epoch 951/4000 - Train Loss: 0.1951, Val Loss: 0.1927\n","Epoch 952/4000 - Train Loss: 0.1957, Val Loss: 0.1919\n","Epoch 953/4000 - Train Loss: 0.1934, Val Loss: 0.1914\n","Epoch 954/4000 - Train Loss: 0.1927, Val Loss: 0.1917\n","Epoch 955/4000 - Train Loss: 0.1934, Val Loss: 0.1917\n","Epoch 956/4000 - Train Loss: 0.1945, Val Loss: 0.1926\n","Epoch 957/4000 - Train Loss: 0.1935, Val Loss: 0.1932\n","Epoch 958/4000 - Train Loss: 0.1939, Val Loss: 0.1935\n","Epoch 959/4000 - Train Loss: 0.1941, Val Loss: 0.1942\n","Epoch 960/4000 - Train Loss: 0.1946, Val Loss: 0.1944\n","Epoch 961/4000 - Train Loss: 0.1926, Val Loss: 0.1939\n","Epoch 962/4000 - Train Loss: 0.1940, Val Loss: 0.1916\n","Epoch 963/4000 - Train Loss: 0.1929, Val Loss: 0.1896 *\n","Epoch 964/4000 - Train Loss: 0.1921, Val Loss: 0.1894 *\n","Epoch 965/4000 - Train Loss: 0.1947, Val Loss: 0.1899\n","Epoch 966/4000 - Train Loss: 0.1953, Val Loss: 0.1908\n","Epoch 967/4000 - Train Loss: 0.1938, Val Loss: 0.1907\n","Epoch 968/4000 - Train Loss: 0.1944, Val Loss: 0.1913\n","Epoch 969/4000 - Train Loss: 0.1916, Val Loss: 0.1928\n","Epoch 970/4000 - Train Loss: 0.1973, Val Loss: 0.1932\n","Epoch 971/4000 - Train Loss: 0.1948, Val Loss: 0.1939\n","Epoch 972/4000 - Train Loss: 0.1923, Val Loss: 0.1942\n","Epoch 973/4000 - Train Loss: 0.1937, Val Loss: 0.1930\n","Epoch 974/4000 - Train Loss: 0.1916, Val Loss: 0.1918\n","Epoch 975/4000 - Train Loss: 0.1920, Val Loss: 0.1902\n","Epoch 976/4000 - Train Loss: 0.1944, Val Loss: 0.1895\n","Epoch 977/4000 - Train Loss: 0.1934, Val Loss: 0.1896\n","Epoch 978/4000 - Train Loss: 0.1927, Val Loss: 0.1911\n","Epoch 979/4000 - Train Loss: 0.1934, Val Loss: 0.1900\n","Epoch 980/4000 - Train Loss: 0.1914, Val Loss: 0.1889 *\n","Epoch 981/4000 - Train Loss: 0.1903, Val Loss: 0.1886 *\n","Epoch 982/4000 - Train Loss: 0.1945, Val Loss: 0.1886\n","Epoch 983/4000 - Train Loss: 0.1948, Val Loss: 0.1898\n","Epoch 984/4000 - Train Loss: 0.1899, Val Loss: 0.1927\n","Epoch 985/4000 - Train Loss: 0.1913, Val Loss: 0.1894\n","Epoch 986/4000 - Train Loss: 0.1909, Val Loss: 0.1886\n","Epoch 987/4000 - Train Loss: 0.1920, Val Loss: 0.1890\n","Epoch 988/4000 - Train Loss: 0.1941, Val Loss: 0.1906\n","Epoch 989/4000 - Train Loss: 0.1899, Val Loss: 0.1923\n","Epoch 990/4000 - Train Loss: 0.1882, Val Loss: 0.1905\n","Epoch 991/4000 - Train Loss: 0.1907, Val Loss: 0.1886\n","Epoch 992/4000 - Train Loss: 0.1909, Val Loss: 0.1881 *\n","Epoch 993/4000 - Train Loss: 0.1920, Val Loss: 0.1879 *\n","Epoch 994/4000 - Train Loss: 0.1895, Val Loss: 0.1900\n","Epoch 995/4000 - Train Loss: 0.1910, Val Loss: 0.1892\n","Epoch 996/4000 - Train Loss: 0.1953, Val Loss: 0.1867 *\n","Epoch 997/4000 - Train Loss: 0.1920, Val Loss: 0.1872\n","Epoch 998/4000 - Train Loss: 0.1902, Val Loss: 0.1873\n","Epoch 999/4000 - Train Loss: 0.1948, Val Loss: 0.1905\n","Epoch 1000/4000 - Train Loss: 0.1898, Val Loss: 0.1913\n","Epoch 1001/4000 - Train Loss: 0.1898, Val Loss: 0.1884\n","Epoch 1002/4000 - Train Loss: 0.1890, Val Loss: 0.1875\n","Epoch 1003/4000 - Train Loss: 0.1914, Val Loss: 0.1873\n","Epoch 1004/4000 - Train Loss: 0.1898, Val Loss: 0.1878\n","Epoch 1005/4000 - Train Loss: 0.1910, Val Loss: 0.1895\n","Epoch 1006/4000 - Train Loss: 0.1929, Val Loss: 0.1877\n","Epoch 1007/4000 - Train Loss: 0.1897, Val Loss: 0.1866\n","Epoch 1008/4000 - Train Loss: 0.1896, Val Loss: 0.1863 *\n","Epoch 1009/4000 - Train Loss: 0.1896, Val Loss: 0.1869\n","Epoch 1010/4000 - Train Loss: 0.1879, Val Loss: 0.1872\n","Epoch 1011/4000 - Train Loss: 0.1904, Val Loss: 0.1874\n","Epoch 1012/4000 - Train Loss: 0.1907, Val Loss: 0.1867\n","Epoch 1013/4000 - Train Loss: 0.1907, Val Loss: 0.1864\n","Epoch 1014/4000 - Train Loss: 0.1887, Val Loss: 0.1867\n","Epoch 1015/4000 - Train Loss: 0.1882, Val Loss: 0.1869\n","Epoch 1016/4000 - Train Loss: 0.1903, Val Loss: 0.1876\n","Epoch 1017/4000 - Train Loss: 0.1878, Val Loss: 0.1871\n","Epoch 1018/4000 - Train Loss: 0.1883, Val Loss: 0.1863\n","Epoch 1019/4000 - Train Loss: 0.1883, Val Loss: 0.1861 *\n","Epoch 1020/4000 - Train Loss: 0.1890, Val Loss: 0.1863\n","Epoch 1021/4000 - Train Loss: 0.1883, Val Loss: 0.1884\n","Epoch 1022/4000 - Train Loss: 0.1915, Val Loss: 0.1884\n","Epoch 1023/4000 - Train Loss: 0.1872, Val Loss: 0.1864\n","Epoch 1024/4000 - Train Loss: 0.1883, Val Loss: 0.1856 *\n","Epoch 1025/4000 - Train Loss: 0.1905, Val Loss: 0.1855 *\n","Epoch 1026/4000 - Train Loss: 0.1895, Val Loss: 0.1869\n","Epoch 1027/4000 - Train Loss: 0.1915, Val Loss: 0.1880\n","Epoch 1028/4000 - Train Loss: 0.1872, Val Loss: 0.1875\n","Epoch 1029/4000 - Train Loss: 0.1878, Val Loss: 0.1876\n","Epoch 1030/4000 - Train Loss: 0.1900, Val Loss: 0.1877\n","Epoch 1031/4000 - Train Loss: 0.1904, Val Loss: 0.1873\n","Epoch 1032/4000 - Train Loss: 0.1876, Val Loss: 0.1880\n","Epoch 1033/4000 - Train Loss: 0.1867, Val Loss: 0.1868\n","Epoch 1034/4000 - Train Loss: 0.1882, Val Loss: 0.1851 *\n","Epoch 1035/4000 - Train Loss: 0.1875, Val Loss: 0.1846 *\n","Epoch 1036/4000 - Train Loss: 0.1891, Val Loss: 0.1851\n","Epoch 1037/4000 - Train Loss: 0.1860, Val Loss: 0.1867\n","Epoch 1038/4000 - Train Loss: 0.1881, Val Loss: 0.1869\n","Epoch 1039/4000 - Train Loss: 0.1895, Val Loss: 0.1857\n","Epoch 1040/4000 - Train Loss: 0.1860, Val Loss: 0.1865\n","Epoch 1041/4000 - Train Loss: 0.1884, Val Loss: 0.1884\n","Epoch 1042/4000 - Train Loss: 0.1905, Val Loss: 0.1898\n","Epoch 1043/4000 - Train Loss: 0.1883, Val Loss: 0.1908\n","Epoch 1044/4000 - Train Loss: 0.1873, Val Loss: 0.1880\n","Epoch 1045/4000 - Train Loss: 0.1879, Val Loss: 0.1848\n","Epoch 1046/4000 - Train Loss: 0.1861, Val Loss: 0.1838 *\n","Epoch 1047/4000 - Train Loss: 0.1889, Val Loss: 0.1841\n","Epoch 1048/4000 - Train Loss: 0.1888, Val Loss: 0.1859\n","Epoch 1049/4000 - Train Loss: 0.1873, Val Loss: 0.1846\n","Epoch 1050/4000 - Train Loss: 0.1891, Val Loss: 0.1832 *\n","Epoch 1051/4000 - Train Loss: 0.1860, Val Loss: 0.1838\n","Epoch 1052/4000 - Train Loss: 0.1874, Val Loss: 0.1851\n","Epoch 1053/4000 - Train Loss: 0.1869, Val Loss: 0.1863\n","Epoch 1054/4000 - Train Loss: 0.1880, Val Loss: 0.1872\n","Epoch 1055/4000 - Train Loss: 0.1882, Val Loss: 0.1858\n","Epoch 1056/4000 - Train Loss: 0.1847, Val Loss: 0.1850\n","Epoch 1057/4000 - Train Loss: 0.1859, Val Loss: 0.1840\n","Epoch 1058/4000 - Train Loss: 0.1872, Val Loss: 0.1836\n","Epoch 1059/4000 - Train Loss: 0.1877, Val Loss: 0.1836\n","Epoch 1060/4000 - Train Loss: 0.1864, Val Loss: 0.1830 *\n","Epoch 1061/4000 - Train Loss: 0.1876, Val Loss: 0.1827 *\n","Epoch 1062/4000 - Train Loss: 0.1870, Val Loss: 0.1814 *\n","Epoch 1063/4000 - Train Loss: 0.1852, Val Loss: 0.1813\n","Epoch 1064/4000 - Train Loss: 0.1887, Val Loss: 0.1839\n","Epoch 1065/4000 - Train Loss: 0.1846, Val Loss: 0.1862\n","Epoch 1066/4000 - Train Loss: 0.1859, Val Loss: 0.1852\n","Epoch 1067/4000 - Train Loss: 0.1881, Val Loss: 0.1847\n","Epoch 1068/4000 - Train Loss: 0.1862, Val Loss: 0.1837\n","Epoch 1069/4000 - Train Loss: 0.1881, Val Loss: 0.1837\n","Epoch 1070/4000 - Train Loss: 0.1850, Val Loss: 0.1855\n","Epoch 1071/4000 - Train Loss: 0.1877, Val Loss: 0.1831\n","Epoch 1072/4000 - Train Loss: 0.1846, Val Loss: 0.1819\n","Epoch 1073/4000 - Train Loss: 0.1881, Val Loss: 0.1816\n","Epoch 1074/4000 - Train Loss: 0.1865, Val Loss: 0.1819\n","Epoch 1075/4000 - Train Loss: 0.1824, Val Loss: 0.1848\n","Epoch 1076/4000 - Train Loss: 0.1850, Val Loss: 0.1834\n","Epoch 1077/4000 - Train Loss: 0.1848, Val Loss: 0.1821\n","Epoch 1078/4000 - Train Loss: 0.1858, Val Loss: 0.1822\n","Epoch 1079/4000 - Train Loss: 0.1880, Val Loss: 0.1826\n","Epoch 1080/4000 - Train Loss: 0.1854, Val Loss: 0.1840\n","Epoch 1081/4000 - Train Loss: 0.1816, Val Loss: 0.1844\n","Epoch 1082/4000 - Train Loss: 0.1838, Val Loss: 0.1822\n","Epoch 1083/4000 - Train Loss: 0.1836, Val Loss: 0.1811 *\n","Epoch 1084/4000 - Train Loss: 0.1839, Val Loss: 0.1807 *\n","Epoch 1085/4000 - Train Loss: 0.1868, Val Loss: 0.1811\n","Epoch 1086/4000 - Train Loss: 0.1835, Val Loss: 0.1821\n","Epoch 1087/4000 - Train Loss: 0.1859, Val Loss: 0.1819\n","Epoch 1088/4000 - Train Loss: 0.1834, Val Loss: 0.1809\n","Epoch 1089/4000 - Train Loss: 0.1863, Val Loss: 0.1805 *\n","Epoch 1090/4000 - Train Loss: 0.1845, Val Loss: 0.1806\n","Epoch 1091/4000 - Train Loss: 0.1848, Val Loss: 0.1824\n","Epoch 1092/4000 - Train Loss: 0.1850, Val Loss: 0.1824\n","Epoch 1093/4000 - Train Loss: 0.1829, Val Loss: 0.1810\n","Epoch 1094/4000 - Train Loss: 0.1814, Val Loss: 0.1804 *\n","Epoch 1095/4000 - Train Loss: 0.1840, Val Loss: 0.1802 *\n","Epoch 1096/4000 - Train Loss: 0.1809, Val Loss: 0.1814\n","Epoch 1097/4000 - Train Loss: 0.1844, Val Loss: 0.1818\n","Epoch 1098/4000 - Train Loss: 0.1823, Val Loss: 0.1804\n","Epoch 1099/4000 - Train Loss: 0.1812, Val Loss: 0.1798 *\n","Epoch 1100/4000 - Train Loss: 0.1837, Val Loss: 0.1798\n","Epoch 1101/4000 - Train Loss: 0.1811, Val Loss: 0.1804\n","Epoch 1102/4000 - Train Loss: 0.1814, Val Loss: 0.1812\n","Epoch 1103/4000 - Train Loss: 0.1837, Val Loss: 0.1802\n","Epoch 1104/4000 - Train Loss: 0.1805, Val Loss: 0.1799\n","Epoch 1105/4000 - Train Loss: 0.1805, Val Loss: 0.1799\n","Epoch 1106/4000 - Train Loss: 0.1849, Val Loss: 0.1801\n","Epoch 1107/4000 - Train Loss: 0.1812, Val Loss: 0.1801\n","Epoch 1108/4000 - Train Loss: 0.1839, Val Loss: 0.1785 *\n","Epoch 1109/4000 - Train Loss: 0.1842, Val Loss: 0.1780 *\n","Epoch 1110/4000 - Train Loss: 0.1817, Val Loss: 0.1780\n","Epoch 1111/4000 - Train Loss: 0.1813, Val Loss: 0.1782\n","Epoch 1112/4000 - Train Loss: 0.1848, Val Loss: 0.1781\n","Epoch 1113/4000 - Train Loss: 0.1835, Val Loss: 0.1782\n","Epoch 1114/4000 - Train Loss: 0.1813, Val Loss: 0.1787\n","Epoch 1115/4000 - Train Loss: 0.1814, Val Loss: 0.1795\n","Epoch 1116/4000 - Train Loss: 0.1804, Val Loss: 0.1805\n","Epoch 1117/4000 - Train Loss: 0.1809, Val Loss: 0.1817\n","Epoch 1118/4000 - Train Loss: 0.1839, Val Loss: 0.1809\n","Epoch 1119/4000 - Train Loss: 0.1822, Val Loss: 0.1801\n","Epoch 1120/4000 - Train Loss: 0.1788, Val Loss: 0.1795\n","Epoch 1121/4000 - Train Loss: 0.1807, Val Loss: 0.1790\n","Epoch 1122/4000 - Train Loss: 0.1801, Val Loss: 0.1794\n","Epoch 1123/4000 - Train Loss: 0.1803, Val Loss: 0.1786\n","Epoch 1124/4000 - Train Loss: 0.1806, Val Loss: 0.1777 *\n","Epoch 1125/4000 - Train Loss: 0.1801, Val Loss: 0.1776 *\n","Epoch 1126/4000 - Train Loss: 0.1790, Val Loss: 0.1786\n","Epoch 1127/4000 - Train Loss: 0.1802, Val Loss: 0.1805\n","Epoch 1128/4000 - Train Loss: 0.1798, Val Loss: 0.1818\n","Epoch 1129/4000 - Train Loss: 0.1813, Val Loss: 0.1801\n","Epoch 1130/4000 - Train Loss: 0.1822, Val Loss: 0.1787\n","Epoch 1131/4000 - Train Loss: 0.1811, Val Loss: 0.1783\n","Epoch 1132/4000 - Train Loss: 0.1829, Val Loss: 0.1778\n","Epoch 1133/4000 - Train Loss: 0.1783, Val Loss: 0.1780\n","Epoch 1134/4000 - Train Loss: 0.1823, Val Loss: 0.1781\n","Epoch 1135/4000 - Train Loss: 0.1798, Val Loss: 0.1777\n","Epoch 1136/4000 - Train Loss: 0.1807, Val Loss: 0.1767 *\n","Epoch 1137/4000 - Train Loss: 0.1804, Val Loss: 0.1769\n","Epoch 1138/4000 - Train Loss: 0.1777, Val Loss: 0.1776\n","Epoch 1139/4000 - Train Loss: 0.1794, Val Loss: 0.1784\n","Epoch 1140/4000 - Train Loss: 0.1799, Val Loss: 0.1773\n","Epoch 1141/4000 - Train Loss: 0.1803, Val Loss: 0.1764 *\n","Epoch 1142/4000 - Train Loss: 0.1783, Val Loss: 0.1759 *\n","Epoch 1143/4000 - Train Loss: 0.1784, Val Loss: 0.1755 *\n","Epoch 1144/4000 - Train Loss: 0.1798, Val Loss: 0.1754 *\n","Epoch 1145/4000 - Train Loss: 0.1798, Val Loss: 0.1750 *\n","Epoch 1146/4000 - Train Loss: 0.1794, Val Loss: 0.1754\n","Epoch 1147/4000 - Train Loss: 0.1780, Val Loss: 0.1764\n","Epoch 1148/4000 - Train Loss: 0.1791, Val Loss: 0.1759\n","Epoch 1149/4000 - Train Loss: 0.1799, Val Loss: 0.1763\n","Epoch 1150/4000 - Train Loss: 0.1802, Val Loss: 0.1773\n","Epoch 1151/4000 - Train Loss: 0.1781, Val Loss: 0.1779\n","Epoch 1152/4000 - Train Loss: 0.1797, Val Loss: 0.1776\n","Epoch 1153/4000 - Train Loss: 0.1792, Val Loss: 0.1768\n","Epoch 1154/4000 - Train Loss: 0.1788, Val Loss: 0.1758\n","Epoch 1155/4000 - Train Loss: 0.1776, Val Loss: 0.1751\n","Epoch 1156/4000 - Train Loss: 0.1782, Val Loss: 0.1751\n","Epoch 1157/4000 - Train Loss: 0.1787, Val Loss: 0.1754\n","Epoch 1158/4000 - Train Loss: 0.1781, Val Loss: 0.1755\n","Epoch 1159/4000 - Train Loss: 0.1769, Val Loss: 0.1759\n","Epoch 1160/4000 - Train Loss: 0.1768, Val Loss: 0.1764\n","Epoch 1161/4000 - Train Loss: 0.1795, Val Loss: 0.1771\n","Epoch 1162/4000 - Train Loss: 0.1781, Val Loss: 0.1787\n","Epoch 1163/4000 - Train Loss: 0.1810, Val Loss: 0.1786\n","Epoch 1164/4000 - Train Loss: 0.1760, Val Loss: 0.1783\n","Epoch 1165/4000 - Train Loss: 0.1782, Val Loss: 0.1768\n","Epoch 1166/4000 - Train Loss: 0.1785, Val Loss: 0.1753\n","Epoch 1167/4000 - Train Loss: 0.1804, Val Loss: 0.1748 *\n","Epoch 1168/4000 - Train Loss: 0.1793, Val Loss: 0.1758\n","Epoch 1169/4000 - Train Loss: 0.1798, Val Loss: 0.1771\n","Epoch 1170/4000 - Train Loss: 0.1815, Val Loss: 0.1752\n","Epoch 1171/4000 - Train Loss: 0.1757, Val Loss: 0.1748\n","Epoch 1172/4000 - Train Loss: 0.1789, Val Loss: 0.1756\n","Epoch 1173/4000 - Train Loss: 0.1768, Val Loss: 0.1766\n","Epoch 1174/4000 - Train Loss: 0.1785, Val Loss: 0.1789\n","Epoch 1175/4000 - Train Loss: 0.1785, Val Loss: 0.1768\n","Epoch 1176/4000 - Train Loss: 0.1755, Val Loss: 0.1743 *\n","Epoch 1177/4000 - Train Loss: 0.1750, Val Loss: 0.1741 *\n","Epoch 1178/4000 - Train Loss: 0.1777, Val Loss: 0.1742\n","Epoch 1179/4000 - Train Loss: 0.1773, Val Loss: 0.1761\n","Epoch 1180/4000 - Train Loss: 0.1776, Val Loss: 0.1757\n","Epoch 1181/4000 - Train Loss: 0.1778, Val Loss: 0.1733 *\n","Epoch 1182/4000 - Train Loss: 0.1764, Val Loss: 0.1735\n","Epoch 1183/4000 - Train Loss: 0.1764, Val Loss: 0.1745\n","Epoch 1184/4000 - Train Loss: 0.1785, Val Loss: 0.1757\n","Epoch 1185/4000 - Train Loss: 0.1766, Val Loss: 0.1748\n","Epoch 1186/4000 - Train Loss: 0.1762, Val Loss: 0.1735\n","Epoch 1187/4000 - Train Loss: 0.1766, Val Loss: 0.1724 *\n","Epoch 1188/4000 - Train Loss: 0.1754, Val Loss: 0.1725\n","Epoch 1189/4000 - Train Loss: 0.1767, Val Loss: 0.1735\n","Epoch 1190/4000 - Train Loss: 0.1773, Val Loss: 0.1760\n","Epoch 1191/4000 - Train Loss: 0.1755, Val Loss: 0.1746\n","Epoch 1192/4000 - Train Loss: 0.1777, Val Loss: 0.1739\n","Epoch 1193/4000 - Train Loss: 0.1788, Val Loss: 0.1735\n","Epoch 1194/4000 - Train Loss: 0.1762, Val Loss: 0.1750\n","Epoch 1195/4000 - Train Loss: 0.1775, Val Loss: 0.1763\n","Epoch 1196/4000 - Train Loss: 0.1764, Val Loss: 0.1736\n","Epoch 1197/4000 - Train Loss: 0.1744, Val Loss: 0.1722 *\n","Epoch 1198/4000 - Train Loss: 0.1766, Val Loss: 0.1721\n","Epoch 1199/4000 - Train Loss: 0.1760, Val Loss: 0.1730\n","Epoch 1200/4000 - Train Loss: 0.1729, Val Loss: 0.1737\n","Epoch 1201/4000 - Train Loss: 0.1750, Val Loss: 0.1738\n","Epoch 1202/4000 - Train Loss: 0.1754, Val Loss: 0.1731\n","Epoch 1203/4000 - Train Loss: 0.1741, Val Loss: 0.1729\n","Epoch 1204/4000 - Train Loss: 0.1768, Val Loss: 0.1731\n","Epoch 1205/4000 - Train Loss: 0.1747, Val Loss: 0.1731\n","Epoch 1206/4000 - Train Loss: 0.1762, Val Loss: 0.1718 *\n","Epoch 1207/4000 - Train Loss: 0.1738, Val Loss: 0.1715 *\n","Epoch 1208/4000 - Train Loss: 0.1749, Val Loss: 0.1724\n","Epoch 1209/4000 - Train Loss: 0.1751, Val Loss: 0.1728\n","Epoch 1210/4000 - Train Loss: 0.1735, Val Loss: 0.1733\n","Epoch 1211/4000 - Train Loss: 0.1709, Val Loss: 0.1732\n","Epoch 1212/4000 - Train Loss: 0.1752, Val Loss: 0.1729\n","Epoch 1213/4000 - Train Loss: 0.1763, Val Loss: 0.1731\n","Epoch 1214/4000 - Train Loss: 0.1754, Val Loss: 0.1739\n","Epoch 1215/4000 - Train Loss: 0.1759, Val Loss: 0.1743\n","Epoch 1216/4000 - Train Loss: 0.1728, Val Loss: 0.1735\n","Epoch 1217/4000 - Train Loss: 0.1751, Val Loss: 0.1710 *\n","Epoch 1218/4000 - Train Loss: 0.1719, Val Loss: 0.1706 *\n","Epoch 1219/4000 - Train Loss: 0.1738, Val Loss: 0.1710\n","Epoch 1220/4000 - Train Loss: 0.1754, Val Loss: 0.1722\n","Epoch 1221/4000 - Train Loss: 0.1715, Val Loss: 0.1723\n","Epoch 1222/4000 - Train Loss: 0.1745, Val Loss: 0.1717\n","Epoch 1223/4000 - Train Loss: 0.1718, Val Loss: 0.1725\n","Epoch 1224/4000 - Train Loss: 0.1741, Val Loss: 0.1738\n","Epoch 1225/4000 - Train Loss: 0.1774, Val Loss: 0.1744\n","Epoch 1226/4000 - Train Loss: 0.1737, Val Loss: 0.1724\n","Epoch 1227/4000 - Train Loss: 0.1714, Val Loss: 0.1713\n","Epoch 1228/4000 - Train Loss: 0.1735, Val Loss: 0.1708\n","Epoch 1229/4000 - Train Loss: 0.1749, Val Loss: 0.1716\n","Epoch 1230/4000 - Train Loss: 0.1710, Val Loss: 0.1737\n","Epoch 1231/4000 - Train Loss: 0.1711, Val Loss: 0.1715\n","Epoch 1232/4000 - Train Loss: 0.1719, Val Loss: 0.1710\n","Epoch 1233/4000 - Train Loss: 0.1723, Val Loss: 0.1710\n","Epoch 1234/4000 - Train Loss: 0.1727, Val Loss: 0.1712\n","Epoch 1235/4000 - Train Loss: 0.1712, Val Loss: 0.1719\n","Epoch 1236/4000 - Train Loss: 0.1728, Val Loss: 0.1711\n","Epoch 1237/4000 - Train Loss: 0.1712, Val Loss: 0.1702 *\n","Epoch 1238/4000 - Train Loss: 0.1732, Val Loss: 0.1700 *\n","Epoch 1239/4000 - Train Loss: 0.1723, Val Loss: 0.1713\n","Epoch 1240/4000 - Train Loss: 0.1714, Val Loss: 0.1736\n","Epoch 1241/4000 - Train Loss: 0.1734, Val Loss: 0.1724\n","Epoch 1242/4000 - Train Loss: 0.1706, Val Loss: 0.1716\n","Epoch 1243/4000 - Train Loss: 0.1734, Val Loss: 0.1712\n","Epoch 1244/4000 - Train Loss: 0.1736, Val Loss: 0.1706\n","Epoch 1245/4000 - Train Loss: 0.1713, Val Loss: 0.1713\n","Epoch 1246/4000 - Train Loss: 0.1736, Val Loss: 0.1710\n","Epoch 1247/4000 - Train Loss: 0.1725, Val Loss: 0.1683 *\n","Epoch 1248/4000 - Train Loss: 0.1736, Val Loss: 0.1678 *\n","Epoch 1249/4000 - Train Loss: 0.1726, Val Loss: 0.1677 *\n","Epoch 1250/4000 - Train Loss: 0.1723, Val Loss: 0.1700\n","Epoch 1251/4000 - Train Loss: 0.1725, Val Loss: 0.1697\n","Epoch 1252/4000 - Train Loss: 0.1723, Val Loss: 0.1686\n","Epoch 1253/4000 - Train Loss: 0.1721, Val Loss: 0.1683\n","Epoch 1254/4000 - Train Loss: 0.1699, Val Loss: 0.1684\n","Epoch 1255/4000 - Train Loss: 0.1719, Val Loss: 0.1690\n","Epoch 1256/4000 - Train Loss: 0.1699, Val Loss: 0.1690\n","Epoch 1257/4000 - Train Loss: 0.1723, Val Loss: 0.1685\n","Epoch 1258/4000 - Train Loss: 0.1707, Val Loss: 0.1687\n","Epoch 1259/4000 - Train Loss: 0.1706, Val Loss: 0.1692\n","Epoch 1260/4000 - Train Loss: 0.1695, Val Loss: 0.1712\n","Epoch 1261/4000 - Train Loss: 0.1709, Val Loss: 0.1697\n","Epoch 1262/4000 - Train Loss: 0.1700, Val Loss: 0.1681\n","Epoch 1263/4000 - Train Loss: 0.1724, Val Loss: 0.1675 *\n","Epoch 1264/4000 - Train Loss: 0.1714, Val Loss: 0.1683\n","Epoch 1265/4000 - Train Loss: 0.1713, Val Loss: 0.1683\n","Epoch 1266/4000 - Train Loss: 0.1703, Val Loss: 0.1672 *\n","Epoch 1267/4000 - Train Loss: 0.1712, Val Loss: 0.1668 *\n","Epoch 1268/4000 - Train Loss: 0.1746, Val Loss: 0.1674\n","Epoch 1269/4000 - Train Loss: 0.1723, Val Loss: 0.1680\n","Epoch 1270/4000 - Train Loss: 0.1694, Val Loss: 0.1684\n","Epoch 1271/4000 - Train Loss: 0.1708, Val Loss: 0.1701\n","Epoch 1272/4000 - Train Loss: 0.1705, Val Loss: 0.1691\n","Epoch 1273/4000 - Train Loss: 0.1721, Val Loss: 0.1672\n","Epoch 1274/4000 - Train Loss: 0.1697, Val Loss: 0.1670\n","Epoch 1275/4000 - Train Loss: 0.1678, Val Loss: 0.1670\n","Epoch 1276/4000 - Train Loss: 0.1691, Val Loss: 0.1675\n","Epoch 1277/4000 - Train Loss: 0.1680, Val Loss: 0.1680\n","Epoch 1278/4000 - Train Loss: 0.1667, Val Loss: 0.1678\n","Epoch 1279/4000 - Train Loss: 0.1681, Val Loss: 0.1676\n","Epoch 1280/4000 - Train Loss: 0.1697, Val Loss: 0.1673\n","Epoch 1281/4000 - Train Loss: 0.1700, Val Loss: 0.1674\n","Epoch 1282/4000 - Train Loss: 0.1674, Val Loss: 0.1677\n","Epoch 1283/4000 - Train Loss: 0.1680, Val Loss: 0.1665 *\n","Epoch 1284/4000 - Train Loss: 0.1674, Val Loss: 0.1661 *\n","Epoch 1285/4000 - Train Loss: 0.1689, Val Loss: 0.1657 *\n","Epoch 1286/4000 - Train Loss: 0.1661, Val Loss: 0.1662\n","Epoch 1287/4000 - Train Loss: 0.1680, Val Loss: 0.1671\n","Epoch 1288/4000 - Train Loss: 0.1683, Val Loss: 0.1691\n","Epoch 1289/4000 - Train Loss: 0.1718, Val Loss: 0.1679\n","Epoch 1290/4000 - Train Loss: 0.1709, Val Loss: 0.1667\n","Epoch 1291/4000 - Train Loss: 0.1682, Val Loss: 0.1654 *\n","Epoch 1292/4000 - Train Loss: 0.1693, Val Loss: 0.1652 *\n","Epoch 1293/4000 - Train Loss: 0.1688, Val Loss: 0.1645 *\n","Epoch 1294/4000 - Train Loss: 0.1690, Val Loss: 0.1648\n","Epoch 1295/4000 - Train Loss: 0.1696, Val Loss: 0.1660\n","Epoch 1296/4000 - Train Loss: 0.1697, Val Loss: 0.1674\n","Epoch 1297/4000 - Train Loss: 0.1672, Val Loss: 0.1670\n","Epoch 1298/4000 - Train Loss: 0.1700, Val Loss: 0.1659\n","Epoch 1299/4000 - Train Loss: 0.1659, Val Loss: 0.1657\n","Epoch 1300/4000 - Train Loss: 0.1682, Val Loss: 0.1662\n","Epoch 1301/4000 - Train Loss: 0.1688, Val Loss: 0.1661\n","Epoch 1302/4000 - Train Loss: 0.1673, Val Loss: 0.1657\n","Epoch 1303/4000 - Train Loss: 0.1676, Val Loss: 0.1652\n","Epoch 1304/4000 - Train Loss: 0.1693, Val Loss: 0.1637 *\n","Epoch 1305/4000 - Train Loss: 0.1661, Val Loss: 0.1634 *\n","Epoch 1306/4000 - Train Loss: 0.1677, Val Loss: 0.1638\n","Epoch 1307/4000 - Train Loss: 0.1706, Val Loss: 0.1658\n","Epoch 1308/4000 - Train Loss: 0.1685, Val Loss: 0.1645\n","Epoch 1309/4000 - Train Loss: 0.1679, Val Loss: 0.1645\n","Epoch 1310/4000 - Train Loss: 0.1674, Val Loss: 0.1651\n","Epoch 1311/4000 - Train Loss: 0.1674, Val Loss: 0.1664\n","Epoch 1312/4000 - Train Loss: 0.1666, Val Loss: 0.1668\n","Epoch 1313/4000 - Train Loss: 0.1665, Val Loss: 0.1652\n","Epoch 1314/4000 - Train Loss: 0.1659, Val Loss: 0.1646\n","Epoch 1315/4000 - Train Loss: 0.1686, Val Loss: 0.1643\n","Epoch 1316/4000 - Train Loss: 0.1664, Val Loss: 0.1656\n","Epoch 1317/4000 - Train Loss: 0.1677, Val Loss: 0.1650\n","Epoch 1318/4000 - Train Loss: 0.1662, Val Loss: 0.1640\n","Epoch 1319/4000 - Train Loss: 0.1660, Val Loss: 0.1640\n","Epoch 1320/4000 - Train Loss: 0.1672, Val Loss: 0.1640\n","Epoch 1321/4000 - Train Loss: 0.1669, Val Loss: 0.1652\n","Epoch 1322/4000 - Train Loss: 0.1644, Val Loss: 0.1664\n","Epoch 1323/4000 - Train Loss: 0.1668, Val Loss: 0.1645\n","Epoch 1324/4000 - Train Loss: 0.1651, Val Loss: 0.1638\n","Epoch 1325/4000 - Train Loss: 0.1666, Val Loss: 0.1641\n","Epoch 1326/4000 - Train Loss: 0.1661, Val Loss: 0.1657\n","Epoch 1327/4000 - Train Loss: 0.1665, Val Loss: 0.1662\n","Epoch 1328/4000 - Train Loss: 0.1654, Val Loss: 0.1640\n","Epoch 1329/4000 - Train Loss: 0.1655, Val Loss: 0.1638\n","Epoch 1330/4000 - Train Loss: 0.1659, Val Loss: 0.1638\n","Epoch 1331/4000 - Train Loss: 0.1662, Val Loss: 0.1655\n","Epoch 1332/4000 - Train Loss: 0.1665, Val Loss: 0.1667\n","Epoch 1333/4000 - Train Loss: 0.1648, Val Loss: 0.1648\n","Epoch 1334/4000 - Train Loss: 0.1650, Val Loss: 0.1643\n","Epoch 1335/4000 - Train Loss: 0.1644, Val Loss: 0.1637\n","Epoch 1336/4000 - Train Loss: 0.1631, Val Loss: 0.1641\n","Epoch 1337/4000 - Train Loss: 0.1631, Val Loss: 0.1651\n","Epoch 1338/4000 - Train Loss: 0.1640, Val Loss: 0.1637\n","Epoch 1339/4000 - Train Loss: 0.1636, Val Loss: 0.1626 *\n","Epoch 1340/4000 - Train Loss: 0.1662, Val Loss: 0.1627\n","Epoch 1341/4000 - Train Loss: 0.1638, Val Loss: 0.1637\n","Epoch 1342/4000 - Train Loss: 0.1648, Val Loss: 0.1646\n","Epoch 1343/4000 - Train Loss: 0.1625, Val Loss: 0.1638\n","Epoch 1344/4000 - Train Loss: 0.1645, Val Loss: 0.1635\n","Epoch 1345/4000 - Train Loss: 0.1641, Val Loss: 0.1634\n","Epoch 1346/4000 - Train Loss: 0.1631, Val Loss: 0.1634\n","Epoch 1347/4000 - Train Loss: 0.1630, Val Loss: 0.1633\n","Epoch 1348/4000 - Train Loss: 0.1670, Val Loss: 0.1624 *\n","Epoch 1349/4000 - Train Loss: 0.1626, Val Loss: 0.1623\n","Epoch 1350/4000 - Train Loss: 0.1630, Val Loss: 0.1627\n","Epoch 1351/4000 - Train Loss: 0.1661, Val Loss: 0.1633\n","Epoch 1352/4000 - Train Loss: 0.1657, Val Loss: 0.1634\n","Epoch 1353/4000 - Train Loss: 0.1639, Val Loss: 0.1629\n","Epoch 1354/4000 - Train Loss: 0.1627, Val Loss: 0.1620 *\n","Epoch 1355/4000 - Train Loss: 0.1633, Val Loss: 0.1621\n","Epoch 1356/4000 - Train Loss: 0.1619, Val Loss: 0.1623\n","Epoch 1357/4000 - Train Loss: 0.1625, Val Loss: 0.1615 *\n","Epoch 1358/4000 - Train Loss: 0.1623, Val Loss: 0.1610 *\n","Epoch 1359/4000 - Train Loss: 0.1628, Val Loss: 0.1613\n","Epoch 1360/4000 - Train Loss: 0.1632, Val Loss: 0.1620\n","Epoch 1361/4000 - Train Loss: 0.1655, Val Loss: 0.1631\n","Epoch 1362/4000 - Train Loss: 0.1625, Val Loss: 0.1632\n","Epoch 1363/4000 - Train Loss: 0.1633, Val Loss: 0.1626\n","Epoch 1364/4000 - Train Loss: 0.1635, Val Loss: 0.1620\n","Epoch 1365/4000 - Train Loss: 0.1625, Val Loss: 0.1611\n","Epoch 1366/4000 - Train Loss: 0.1621, Val Loss: 0.1605 *\n","Epoch 1367/4000 - Train Loss: 0.1641, Val Loss: 0.1602 *\n","Epoch 1368/4000 - Train Loss: 0.1641, Val Loss: 0.1604\n","Epoch 1369/4000 - Train Loss: 0.1627, Val Loss: 0.1613\n","Epoch 1370/4000 - Train Loss: 0.1629, Val Loss: 0.1627\n","Epoch 1371/4000 - Train Loss: 0.1637, Val Loss: 0.1633\n","Epoch 1372/4000 - Train Loss: 0.1614, Val Loss: 0.1631\n","Epoch 1373/4000 - Train Loss: 0.1629, Val Loss: 0.1628\n","Epoch 1374/4000 - Train Loss: 0.1614, Val Loss: 0.1616\n","Epoch 1375/4000 - Train Loss: 0.1621, Val Loss: 0.1604\n","Epoch 1376/4000 - Train Loss: 0.1603, Val Loss: 0.1600 *\n","Epoch 1377/4000 - Train Loss: 0.1608, Val Loss: 0.1601\n","Epoch 1378/4000 - Train Loss: 0.1626, Val Loss: 0.1607\n","Epoch 1379/4000 - Train Loss: 0.1617, Val Loss: 0.1608\n","Epoch 1380/4000 - Train Loss: 0.1603, Val Loss: 0.1603\n","Epoch 1381/4000 - Train Loss: 0.1612, Val Loss: 0.1600\n","Epoch 1382/4000 - Train Loss: 0.1613, Val Loss: 0.1598 *\n","Epoch 1383/4000 - Train Loss: 0.1604, Val Loss: 0.1597\n","Epoch 1384/4000 - Train Loss: 0.1592, Val Loss: 0.1595 *\n","Epoch 1385/4000 - Train Loss: 0.1616, Val Loss: 0.1592 *\n","Epoch 1386/4000 - Train Loss: 0.1628, Val Loss: 0.1601\n","Epoch 1387/4000 - Train Loss: 0.1622, Val Loss: 0.1599\n","Epoch 1388/4000 - Train Loss: 0.1617, Val Loss: 0.1592\n","Epoch 1389/4000 - Train Loss: 0.1620, Val Loss: 0.1594\n","Epoch 1390/4000 - Train Loss: 0.1618, Val Loss: 0.1589 *\n","Epoch 1391/4000 - Train Loss: 0.1600, Val Loss: 0.1612\n","Epoch 1392/4000 - Train Loss: 0.1633, Val Loss: 0.1601\n","Epoch 1393/4000 - Train Loss: 0.1609, Val Loss: 0.1588 *\n","Epoch 1394/4000 - Train Loss: 0.1629, Val Loss: 0.1583 *\n","Epoch 1395/4000 - Train Loss: 0.1607, Val Loss: 0.1599\n","Epoch 1396/4000 - Train Loss: 0.1598, Val Loss: 0.1597\n","Epoch 1397/4000 - Train Loss: 0.1606, Val Loss: 0.1589\n","Epoch 1398/4000 - Train Loss: 0.1601, Val Loss: 0.1588\n","Epoch 1399/4000 - Train Loss: 0.1576, Val Loss: 0.1591\n","Epoch 1400/4000 - Train Loss: 0.1587, Val Loss: 0.1596\n","Epoch 1401/4000 - Train Loss: 0.1603, Val Loss: 0.1604\n","Epoch 1402/4000 - Train Loss: 0.1618, Val Loss: 0.1588\n","Epoch 1403/4000 - Train Loss: 0.1622, Val Loss: 0.1578 *\n","Epoch 1404/4000 - Train Loss: 0.1594, Val Loss: 0.1581\n","Epoch 1405/4000 - Train Loss: 0.1603, Val Loss: 0.1594\n","Epoch 1406/4000 - Train Loss: 0.1606, Val Loss: 0.1582\n","Epoch 1407/4000 - Train Loss: 0.1589, Val Loss: 0.1574 *\n","Epoch 1408/4000 - Train Loss: 0.1608, Val Loss: 0.1575\n","Epoch 1409/4000 - Train Loss: 0.1605, Val Loss: 0.1594\n","Epoch 1410/4000 - Train Loss: 0.1606, Val Loss: 0.1612\n","Epoch 1411/4000 - Train Loss: 0.1595, Val Loss: 0.1602\n","Epoch 1412/4000 - Train Loss: 0.1593, Val Loss: 0.1605\n","Epoch 1413/4000 - Train Loss: 0.1598, Val Loss: 0.1602\n","Epoch 1414/4000 - Train Loss: 0.1617, Val Loss: 0.1602\n","Epoch 1415/4000 - Train Loss: 0.1565, Val Loss: 0.1604\n","Epoch 1416/4000 - Train Loss: 0.1610, Val Loss: 0.1584\n","Epoch 1417/4000 - Train Loss: 0.1562, Val Loss: 0.1576\n","Epoch 1418/4000 - Train Loss: 0.1617, Val Loss: 0.1572 *\n","Epoch 1419/4000 - Train Loss: 0.1588, Val Loss: 0.1578\n","Epoch 1420/4000 - Train Loss: 0.1593, Val Loss: 0.1587\n","Epoch 1421/4000 - Train Loss: 0.1595, Val Loss: 0.1596\n","Epoch 1422/4000 - Train Loss: 0.1605, Val Loss: 0.1590\n","Epoch 1423/4000 - Train Loss: 0.1613, Val Loss: 0.1582\n","Epoch 1424/4000 - Train Loss: 0.1576, Val Loss: 0.1573\n","Epoch 1425/4000 - Train Loss: 0.1581, Val Loss: 0.1574\n","Epoch 1426/4000 - Train Loss: 0.1598, Val Loss: 0.1577\n","Epoch 1427/4000 - Train Loss: 0.1586, Val Loss: 0.1570 *\n","Epoch 1428/4000 - Train Loss: 0.1590, Val Loss: 0.1572\n","Epoch 1429/4000 - Train Loss: 0.1544, Val Loss: 0.1576\n","Epoch 1430/4000 - Train Loss: 0.1582, Val Loss: 0.1583\n","Epoch 1431/4000 - Train Loss: 0.1581, Val Loss: 0.1599\n","Epoch 1432/4000 - Train Loss: 0.1569, Val Loss: 0.1605\n","Epoch 1433/4000 - Train Loss: 0.1564, Val Loss: 0.1611\n","Epoch 1434/4000 - Train Loss: 0.1590, Val Loss: 0.1603\n","Epoch 1435/4000 - Train Loss: 0.1587, Val Loss: 0.1585\n","Epoch 1436/4000 - Train Loss: 0.1576, Val Loss: 0.1569 *\n","Epoch 1437/4000 - Train Loss: 0.1574, Val Loss: 0.1558 *\n","Epoch 1438/4000 - Train Loss: 0.1559, Val Loss: 0.1555 *\n","Epoch 1439/4000 - Train Loss: 0.1580, Val Loss: 0.1550 *\n","Epoch 1440/4000 - Train Loss: 0.1580, Val Loss: 0.1547 *\n","Epoch 1441/4000 - Train Loss: 0.1552, Val Loss: 0.1555\n","Epoch 1442/4000 - Train Loss: 0.1572, Val Loss: 0.1574\n","Epoch 1443/4000 - Train Loss: 0.1586, Val Loss: 0.1579\n","Epoch 1444/4000 - Train Loss: 0.1579, Val Loss: 0.1571\n","Epoch 1445/4000 - Train Loss: 0.1583, Val Loss: 0.1569\n","Epoch 1446/4000 - Train Loss: 0.1580, Val Loss: 0.1573\n","Epoch 1447/4000 - Train Loss: 0.1551, Val Loss: 0.1581\n","Epoch 1448/4000 - Train Loss: 0.1578, Val Loss: 0.1561\n","Epoch 1449/4000 - Train Loss: 0.1554, Val Loss: 0.1549\n","Epoch 1450/4000 - Train Loss: 0.1607, Val Loss: 0.1547\n","Epoch 1451/4000 - Train Loss: 0.1558, Val Loss: 0.1563\n","Epoch 1452/4000 - Train Loss: 0.1585, Val Loss: 0.1547\n","Epoch 1453/4000 - Train Loss: 0.1560, Val Loss: 0.1546 *\n","Epoch 1454/4000 - Train Loss: 0.1574, Val Loss: 0.1556\n","Epoch 1455/4000 - Train Loss: 0.1568, Val Loss: 0.1564\n","Epoch 1456/4000 - Train Loss: 0.1556, Val Loss: 0.1569\n","Epoch 1457/4000 - Train Loss: 0.1571, Val Loss: 0.1566\n","Epoch 1458/4000 - Train Loss: 0.1568, Val Loss: 0.1555\n","Epoch 1459/4000 - Train Loss: 0.1568, Val Loss: 0.1550\n","Epoch 1460/4000 - Train Loss: 0.1563, Val Loss: 0.1551\n","Epoch 1461/4000 - Train Loss: 0.1553, Val Loss: 0.1555\n","Epoch 1462/4000 - Train Loss: 0.1582, Val Loss: 0.1548\n","Epoch 1463/4000 - Train Loss: 0.1547, Val Loss: 0.1549\n","Epoch 1464/4000 - Train Loss: 0.1564, Val Loss: 0.1553\n","Epoch 1465/4000 - Train Loss: 0.1540, Val Loss: 0.1566\n","Epoch 1466/4000 - Train Loss: 0.1559, Val Loss: 0.1547\n","Epoch 1467/4000 - Train Loss: 0.1534, Val Loss: 0.1541 *\n","Epoch 1468/4000 - Train Loss: 0.1558, Val Loss: 0.1540\n","Epoch 1469/4000 - Train Loss: 0.1573, Val Loss: 0.1555\n","Epoch 1470/4000 - Train Loss: 0.1583, Val Loss: 0.1554\n","Epoch 1471/4000 - Train Loss: 0.1535, Val Loss: 0.1551\n","Epoch 1472/4000 - Train Loss: 0.1550, Val Loss: 0.1557\n","Epoch 1473/4000 - Train Loss: 0.1557, Val Loss: 0.1559\n","Epoch 1474/4000 - Train Loss: 0.1539, Val Loss: 0.1562\n","Epoch 1475/4000 - Train Loss: 0.1532, Val Loss: 0.1554\n","Epoch 1476/4000 - Train Loss: 0.1573, Val Loss: 0.1542\n","Epoch 1477/4000 - Train Loss: 0.1529, Val Loss: 0.1541\n","Epoch 1478/4000 - Train Loss: 0.1527, Val Loss: 0.1544\n","Epoch 1479/4000 - Train Loss: 0.1552, Val Loss: 0.1549\n","Epoch 1480/4000 - Train Loss: 0.1573, Val Loss: 0.1547\n","Epoch 1481/4000 - Train Loss: 0.1553, Val Loss: 0.1556\n","Epoch 1482/4000 - Train Loss: 0.1562, Val Loss: 0.1568\n","Epoch 1483/4000 - Train Loss: 0.1543, Val Loss: 0.1589\n","Epoch 1484/4000 - Train Loss: 0.1561, Val Loss: 0.1555\n","Epoch 1485/4000 - Train Loss: 0.1532, Val Loss: 0.1537 *\n","Epoch 1486/4000 - Train Loss: 0.1530, Val Loss: 0.1531 *\n","Epoch 1487/4000 - Train Loss: 0.1539, Val Loss: 0.1533\n","Epoch 1488/4000 - Train Loss: 0.1537, Val Loss: 0.1534\n","Epoch 1489/4000 - Train Loss: 0.1549, Val Loss: 0.1534\n","Epoch 1490/4000 - Train Loss: 0.1532, Val Loss: 0.1530\n","Epoch 1491/4000 - Train Loss: 0.1538, Val Loss: 0.1531\n","Epoch 1492/4000 - Train Loss: 0.1525, Val Loss: 0.1538\n","Epoch 1493/4000 - Train Loss: 0.1536, Val Loss: 0.1537\n","Epoch 1494/4000 - Train Loss: 0.1528, Val Loss: 0.1528 *\n","Epoch 1495/4000 - Train Loss: 0.1537, Val Loss: 0.1523 *\n","Epoch 1496/4000 - Train Loss: 0.1508, Val Loss: 0.1522 *\n","Epoch 1497/4000 - Train Loss: 0.1512, Val Loss: 0.1527\n","Epoch 1498/4000 - Train Loss: 0.1533, Val Loss: 0.1529\n","Epoch 1499/4000 - Train Loss: 0.1518, Val Loss: 0.1525\n","Epoch 1500/4000 - Train Loss: 0.1542, Val Loss: 0.1524\n","Epoch 1501/4000 - Train Loss: 0.1523, Val Loss: 0.1523\n","Epoch 1502/4000 - Train Loss: 0.1522, Val Loss: 0.1526\n","Epoch 1503/4000 - Train Loss: 0.1515, Val Loss: 0.1533\n","Epoch 1504/4000 - Train Loss: 0.1528, Val Loss: 0.1525\n","Epoch 1505/4000 - Train Loss: 0.1520, Val Loss: 0.1524\n","Epoch 1506/4000 - Train Loss: 0.1526, Val Loss: 0.1522\n","Epoch 1507/4000 - Train Loss: 0.1531, Val Loss: 0.1527\n","Epoch 1508/4000 - Train Loss: 0.1508, Val Loss: 0.1522\n","Epoch 1509/4000 - Train Loss: 0.1531, Val Loss: 0.1509 *\n","Epoch 1510/4000 - Train Loss: 0.1520, Val Loss: 0.1507 *\n","Epoch 1511/4000 - Train Loss: 0.1515, Val Loss: 0.1515\n","Epoch 1512/4000 - Train Loss: 0.1516, Val Loss: 0.1528\n","Epoch 1513/4000 - Train Loss: 0.1531, Val Loss: 0.1516\n","Epoch 1514/4000 - Train Loss: 0.1523, Val Loss: 0.1521\n","Epoch 1515/4000 - Train Loss: 0.1509, Val Loss: 0.1536\n","Epoch 1516/4000 - Train Loss: 0.1540, Val Loss: 0.1545\n","Epoch 1517/4000 - Train Loss: 0.1546, Val Loss: 0.1548\n","Epoch 1518/4000 - Train Loss: 0.1524, Val Loss: 0.1524\n","Epoch 1519/4000 - Train Loss: 0.1535, Val Loss: 0.1514\n","Epoch 1520/4000 - Train Loss: 0.1519, Val Loss: 0.1513\n","Epoch 1521/4000 - Train Loss: 0.1500, Val Loss: 0.1515\n","Epoch 1522/4000 - Train Loss: 0.1538, Val Loss: 0.1508\n","Epoch 1523/4000 - Train Loss: 0.1530, Val Loss: 0.1514\n","Epoch 1524/4000 - Train Loss: 0.1508, Val Loss: 0.1526\n","Epoch 1525/4000 - Train Loss: 0.1512, Val Loss: 0.1519\n","Epoch 1526/4000 - Train Loss: 0.1540, Val Loss: 0.1507\n","Epoch 1527/4000 - Train Loss: 0.1490, Val Loss: 0.1501 *\n","Epoch 1528/4000 - Train Loss: 0.1504, Val Loss: 0.1506\n","Epoch 1529/4000 - Train Loss: 0.1497, Val Loss: 0.1523\n","Epoch 1530/4000 - Train Loss: 0.1531, Val Loss: 0.1509\n","Epoch 1531/4000 - Train Loss: 0.1514, Val Loss: 0.1512\n","Epoch 1532/4000 - Train Loss: 0.1510, Val Loss: 0.1511\n","Epoch 1533/4000 - Train Loss: 0.1502, Val Loss: 0.1522\n","Epoch 1534/4000 - Train Loss: 0.1507, Val Loss: 0.1535\n","Epoch 1535/4000 - Train Loss: 0.1503, Val Loss: 0.1506\n","Epoch 1536/4000 - Train Loss: 0.1486, Val Loss: 0.1506\n","Epoch 1537/4000 - Train Loss: 0.1500, Val Loss: 0.1502\n","Epoch 1538/4000 - Train Loss: 0.1503, Val Loss: 0.1505\n","Epoch 1539/4000 - Train Loss: 0.1484, Val Loss: 0.1527\n","Epoch 1540/4000 - Train Loss: 0.1533, Val Loss: 0.1499 *\n","Epoch 1541/4000 - Train Loss: 0.1502, Val Loss: 0.1502\n","Epoch 1542/4000 - Train Loss: 0.1489, Val Loss: 0.1501\n","Epoch 1543/4000 - Train Loss: 0.1526, Val Loss: 0.1505\n","Epoch 1544/4000 - Train Loss: 0.1511, Val Loss: 0.1511\n","Epoch 1545/4000 - Train Loss: 0.1485, Val Loss: 0.1516\n","Epoch 1546/4000 - Train Loss: 0.1487, Val Loss: 0.1512\n","Epoch 1547/4000 - Train Loss: 0.1487, Val Loss: 0.1505\n","Epoch 1548/4000 - Train Loss: 0.1508, Val Loss: 0.1500\n","Epoch 1549/4000 - Train Loss: 0.1487, Val Loss: 0.1496 *\n","Epoch 1550/4000 - Train Loss: 0.1500, Val Loss: 0.1490 *\n","Epoch 1551/4000 - Train Loss: 0.1483, Val Loss: 0.1488 *\n","Epoch 1552/4000 - Train Loss: 0.1485, Val Loss: 0.1496\n","Epoch 1553/4000 - Train Loss: 0.1500, Val Loss: 0.1491\n","Epoch 1554/4000 - Train Loss: 0.1480, Val Loss: 0.1490\n","Epoch 1555/4000 - Train Loss: 0.1470, Val Loss: 0.1491\n","Epoch 1556/4000 - Train Loss: 0.1485, Val Loss: 0.1496\n","Epoch 1557/4000 - Train Loss: 0.1502, Val Loss: 0.1492\n","Epoch 1558/4000 - Train Loss: 0.1484, Val Loss: 0.1490\n","Epoch 1559/4000 - Train Loss: 0.1488, Val Loss: 0.1490\n","Epoch 1560/4000 - Train Loss: 0.1487, Val Loss: 0.1499\n","Epoch 1561/4000 - Train Loss: 0.1502, Val Loss: 0.1505\n","Epoch 1562/4000 - Train Loss: 0.1497, Val Loss: 0.1491\n","Epoch 1563/4000 - Train Loss: 0.1489, Val Loss: 0.1493\n","Epoch 1564/4000 - Train Loss: 0.1461, Val Loss: 0.1501\n","Epoch 1565/4000 - Train Loss: 0.1468, Val Loss: 0.1510\n","Epoch 1566/4000 - Train Loss: 0.1489, Val Loss: 0.1507\n","Epoch 1567/4000 - Train Loss: 0.1475, Val Loss: 0.1492\n","Epoch 1568/4000 - Train Loss: 0.1478, Val Loss: 0.1487 *\n","Epoch 1569/4000 - Train Loss: 0.1476, Val Loss: 0.1490\n","Epoch 1570/4000 - Train Loss: 0.1492, Val Loss: 0.1486 *\n","Epoch 1571/4000 - Train Loss: 0.1458, Val Loss: 0.1507\n","Epoch 1572/4000 - Train Loss: 0.1501, Val Loss: 0.1509\n","Epoch 1573/4000 - Train Loss: 0.1484, Val Loss: 0.1481 *\n","Epoch 1574/4000 - Train Loss: 0.1480, Val Loss: 0.1481\n","Epoch 1575/4000 - Train Loss: 0.1490, Val Loss: 0.1481\n","Epoch 1576/4000 - Train Loss: 0.1472, Val Loss: 0.1498\n","Epoch 1577/4000 - Train Loss: 0.1468, Val Loss: 0.1494\n","Epoch 1578/4000 - Train Loss: 0.1459, Val Loss: 0.1494\n","Epoch 1579/4000 - Train Loss: 0.1496, Val Loss: 0.1484\n","Epoch 1580/4000 - Train Loss: 0.1454, Val Loss: 0.1478 *\n","Epoch 1581/4000 - Train Loss: 0.1466, Val Loss: 0.1477\n","Epoch 1582/4000 - Train Loss: 0.1464, Val Loss: 0.1473 *\n","Epoch 1583/4000 - Train Loss: 0.1504, Val Loss: 0.1475\n","Epoch 1584/4000 - Train Loss: 0.1453, Val Loss: 0.1482\n","Epoch 1585/4000 - Train Loss: 0.1460, Val Loss: 0.1492\n","Epoch 1586/4000 - Train Loss: 0.1478, Val Loss: 0.1506\n","Epoch 1587/4000 - Train Loss: 0.1461, Val Loss: 0.1494\n","Epoch 1588/4000 - Train Loss: 0.1466, Val Loss: 0.1487\n","Epoch 1589/4000 - Train Loss: 0.1459, Val Loss: 0.1477\n","Epoch 1590/4000 - Train Loss: 0.1465, Val Loss: 0.1473\n","Epoch 1591/4000 - Train Loss: 0.1464, Val Loss: 0.1469 *\n","Epoch 1592/4000 - Train Loss: 0.1462, Val Loss: 0.1466 *\n","Epoch 1593/4000 - Train Loss: 0.1464, Val Loss: 0.1466\n","Epoch 1594/4000 - Train Loss: 0.1476, Val Loss: 0.1473\n","Epoch 1595/4000 - Train Loss: 0.1462, Val Loss: 0.1482\n","Epoch 1596/4000 - Train Loss: 0.1450, Val Loss: 0.1491\n","Epoch 1597/4000 - Train Loss: 0.1453, Val Loss: 0.1491\n","Epoch 1598/4000 - Train Loss: 0.1464, Val Loss: 0.1485\n","Epoch 1599/4000 - Train Loss: 0.1456, Val Loss: 0.1476\n","Epoch 1600/4000 - Train Loss: 0.1464, Val Loss: 0.1464 *\n","Epoch 1601/4000 - Train Loss: 0.1454, Val Loss: 0.1455 *\n","Epoch 1602/4000 - Train Loss: 0.1459, Val Loss: 0.1454 *\n","Epoch 1603/4000 - Train Loss: 0.1449, Val Loss: 0.1460\n","Epoch 1604/4000 - Train Loss: 0.1470, Val Loss: 0.1475\n","Epoch 1605/4000 - Train Loss: 0.1452, Val Loss: 0.1474\n","Epoch 1606/4000 - Train Loss: 0.1455, Val Loss: 0.1481\n","Epoch 1607/4000 - Train Loss: 0.1452, Val Loss: 0.1478\n","Epoch 1608/4000 - Train Loss: 0.1445, Val Loss: 0.1474\n","Epoch 1609/4000 - Train Loss: 0.1441, Val Loss: 0.1483\n","Epoch 1610/4000 - Train Loss: 0.1445, Val Loss: 0.1476\n","Epoch 1611/4000 - Train Loss: 0.1464, Val Loss: 0.1463\n","Epoch 1612/4000 - Train Loss: 0.1458, Val Loss: 0.1462\n","Epoch 1613/4000 - Train Loss: 0.1465, Val Loss: 0.1462\n","Epoch 1614/4000 - Train Loss: 0.1452, Val Loss: 0.1481\n","Epoch 1615/4000 - Train Loss: 0.1448, Val Loss: 0.1490\n","Epoch 1616/4000 - Train Loss: 0.1452, Val Loss: 0.1479\n","Epoch 1617/4000 - Train Loss: 0.1424, Val Loss: 0.1470\n","Epoch 1618/4000 - Train Loss: 0.1442, Val Loss: 0.1459\n","Epoch 1619/4000 - Train Loss: 0.1444, Val Loss: 0.1457\n","Epoch 1620/4000 - Train Loss: 0.1460, Val Loss: 0.1467\n","Epoch 1621/4000 - Train Loss: 0.1429, Val Loss: 0.1464\n","Epoch 1622/4000 - Train Loss: 0.1422, Val Loss: 0.1454\n","Epoch 1623/4000 - Train Loss: 0.1441, Val Loss: 0.1459\n","Epoch 1624/4000 - Train Loss: 0.1448, Val Loss: 0.1460\n","Epoch 1625/4000 - Train Loss: 0.1446, Val Loss: 0.1487\n","Epoch 1626/4000 - Train Loss: 0.1451, Val Loss: 0.1494\n","Epoch 1627/4000 - Train Loss: 0.1439, Val Loss: 0.1492\n","Epoch 1628/4000 - Train Loss: 0.1450, Val Loss: 0.1494\n","Epoch 1629/4000 - Train Loss: 0.1460, Val Loss: 0.1477\n","Epoch 1630/4000 - Train Loss: 0.1460, Val Loss: 0.1469\n","Epoch 1631/4000 - Train Loss: 0.1438, Val Loss: 0.1459\n","Epoch 1632/4000 - Train Loss: 0.1455, Val Loss: 0.1456\n","Epoch 1633/4000 - Train Loss: 0.1430, Val Loss: 0.1455\n","Epoch 1634/4000 - Train Loss: 0.1452, Val Loss: 0.1454\n","Epoch 1635/4000 - Train Loss: 0.1439, Val Loss: 0.1466\n","Epoch 1636/4000 - Train Loss: 0.1430, Val Loss: 0.1476\n","Epoch 1637/4000 - Train Loss: 0.1428, Val Loss: 0.1475\n","Epoch 1638/4000 - Train Loss: 0.1449, Val Loss: 0.1461\n","Epoch 1639/4000 - Train Loss: 0.1408, Val Loss: 0.1452 *\n","Epoch 1640/4000 - Train Loss: 0.1435, Val Loss: 0.1446 *\n","Epoch 1641/4000 - Train Loss: 0.1431, Val Loss: 0.1454\n","Epoch 1642/4000 - Train Loss: 0.1404, Val Loss: 0.1445\n","Epoch 1643/4000 - Train Loss: 0.1414, Val Loss: 0.1441 *\n","Epoch 1644/4000 - Train Loss: 0.1420, Val Loss: 0.1441\n","Epoch 1645/4000 - Train Loss: 0.1435, Val Loss: 0.1442\n","Epoch 1646/4000 - Train Loss: 0.1408, Val Loss: 0.1452\n","Epoch 1647/4000 - Train Loss: 0.1423, Val Loss: 0.1459\n","Epoch 1648/4000 - Train Loss: 0.1435, Val Loss: 0.1461\n","Epoch 1649/4000 - Train Loss: 0.1407, Val Loss: 0.1460\n","Epoch 1650/4000 - Train Loss: 0.1423, Val Loss: 0.1464\n","Epoch 1651/4000 - Train Loss: 0.1448, Val Loss: 0.1467\n","Epoch 1652/4000 - Train Loss: 0.1420, Val Loss: 0.1477\n","Epoch 1653/4000 - Train Loss: 0.1424, Val Loss: 0.1458\n","Epoch 1654/4000 - Train Loss: 0.1427, Val Loss: 0.1439 *\n","Epoch 1655/4000 - Train Loss: 0.1413, Val Loss: 0.1439\n","Epoch 1656/4000 - Train Loss: 0.1420, Val Loss: 0.1437 *\n","Epoch 1657/4000 - Train Loss: 0.1406, Val Loss: 0.1447\n","Epoch 1658/4000 - Train Loss: 0.1429, Val Loss: 0.1447\n","Epoch 1659/4000 - Train Loss: 0.1403, Val Loss: 0.1453\n","Epoch 1660/4000 - Train Loss: 0.1421, Val Loss: 0.1467\n","Epoch 1661/4000 - Train Loss: 0.1402, Val Loss: 0.1471\n","Epoch 1662/4000 - Train Loss: 0.1400, Val Loss: 0.1470\n","Epoch 1663/4000 - Train Loss: 0.1419, Val Loss: 0.1454\n","Epoch 1664/4000 - Train Loss: 0.1411, Val Loss: 0.1444\n","Epoch 1665/4000 - Train Loss: 0.1397, Val Loss: 0.1441\n","Epoch 1666/4000 - Train Loss: 0.1408, Val Loss: 0.1444\n","Epoch 1667/4000 - Train Loss: 0.1427, Val Loss: 0.1442\n","Epoch 1668/4000 - Train Loss: 0.1401, Val Loss: 0.1444\n","Epoch 1669/4000 - Train Loss: 0.1413, Val Loss: 0.1442\n","Epoch 1670/4000 - Train Loss: 0.1410, Val Loss: 0.1439\n","Epoch 1671/4000 - Train Loss: 0.1398, Val Loss: 0.1440\n","Epoch 1672/4000 - Train Loss: 0.1406, Val Loss: 0.1428 *\n","Epoch 1673/4000 - Train Loss: 0.1400, Val Loss: 0.1430\n","Epoch 1674/4000 - Train Loss: 0.1405, Val Loss: 0.1423 *\n","Epoch 1675/4000 - Train Loss: 0.1404, Val Loss: 0.1438\n","Epoch 1676/4000 - Train Loss: 0.1419, Val Loss: 0.1444\n","Epoch 1677/4000 - Train Loss: 0.1402, Val Loss: 0.1425\n","Epoch 1678/4000 - Train Loss: 0.1394, Val Loss: 0.1430\n","Epoch 1679/4000 - Train Loss: 0.1402, Val Loss: 0.1428\n","Epoch 1680/4000 - Train Loss: 0.1393, Val Loss: 0.1430\n","Epoch 1681/4000 - Train Loss: 0.1388, Val Loss: 0.1432\n","Epoch 1682/4000 - Train Loss: 0.1399, Val Loss: 0.1420 *\n","Epoch 1683/4000 - Train Loss: 0.1395, Val Loss: 0.1418 *\n","Epoch 1684/4000 - Train Loss: 0.1408, Val Loss: 0.1420\n","Epoch 1685/4000 - Train Loss: 0.1397, Val Loss: 0.1426\n","Epoch 1686/4000 - Train Loss: 0.1370, Val Loss: 0.1425\n","Epoch 1687/4000 - Train Loss: 0.1396, Val Loss: 0.1430\n","Epoch 1688/4000 - Train Loss: 0.1383, Val Loss: 0.1430\n","Epoch 1689/4000 - Train Loss: 0.1394, Val Loss: 0.1431\n","Epoch 1690/4000 - Train Loss: 0.1396, Val Loss: 0.1441\n","Epoch 1691/4000 - Train Loss: 0.1391, Val Loss: 0.1438\n","Epoch 1692/4000 - Train Loss: 0.1378, Val Loss: 0.1428\n","Epoch 1693/4000 - Train Loss: 0.1379, Val Loss: 0.1428\n","Epoch 1694/4000 - Train Loss: 0.1387, Val Loss: 0.1427\n","Epoch 1695/4000 - Train Loss: 0.1374, Val Loss: 0.1430\n","Epoch 1696/4000 - Train Loss: 0.1418, Val Loss: 0.1432\n","Epoch 1697/4000 - Train Loss: 0.1401, Val Loss: 0.1429\n","Epoch 1698/4000 - Train Loss: 0.1382, Val Loss: 0.1434\n","Epoch 1699/4000 - Train Loss: 0.1363, Val Loss: 0.1442\n","Epoch 1700/4000 - Train Loss: 0.1375, Val Loss: 0.1427\n","Epoch 1701/4000 - Train Loss: 0.1387, Val Loss: 0.1423\n","Epoch 1702/4000 - Train Loss: 0.1370, Val Loss: 0.1419\n","Epoch 1703/4000 - Train Loss: 0.1380, Val Loss: 0.1420\n","Epoch 1704/4000 - Train Loss: 0.1381, Val Loss: 0.1440\n","Epoch 1705/4000 - Train Loss: 0.1398, Val Loss: 0.1421\n","Epoch 1706/4000 - Train Loss: 0.1394, Val Loss: 0.1431\n","Epoch 1707/4000 - Train Loss: 0.1401, Val Loss: 0.1424\n","Epoch 1708/4000 - Train Loss: 0.1368, Val Loss: 0.1436\n","Epoch 1709/4000 - Train Loss: 0.1394, Val Loss: 0.1434\n","Epoch 1710/4000 - Train Loss: 0.1383, Val Loss: 0.1423\n","Epoch 1711/4000 - Train Loss: 0.1383, Val Loss: 0.1431\n","Epoch 1712/4000 - Train Loss: 0.1381, Val Loss: 0.1428\n","Epoch 1713/4000 - Train Loss: 0.1375, Val Loss: 0.1441\n","Epoch 1714/4000 - Train Loss: 0.1392, Val Loss: 0.1425\n","Epoch 1715/4000 - Train Loss: 0.1349, Val Loss: 0.1410 *\n","Epoch 1716/4000 - Train Loss: 0.1379, Val Loss: 0.1406 *\n","Epoch 1717/4000 - Train Loss: 0.1367, Val Loss: 0.1403 *\n","Epoch 1718/4000 - Train Loss: 0.1371, Val Loss: 0.1405\n","Epoch 1719/4000 - Train Loss: 0.1374, Val Loss: 0.1411\n","Epoch 1720/4000 - Train Loss: 0.1389, Val Loss: 0.1408\n","Epoch 1721/4000 - Train Loss: 0.1367, Val Loss: 0.1413\n","Epoch 1722/4000 - Train Loss: 0.1385, Val Loss: 0.1420\n","Epoch 1723/4000 - Train Loss: 0.1367, Val Loss: 0.1426\n","Epoch 1724/4000 - Train Loss: 0.1371, Val Loss: 0.1432\n","Epoch 1725/4000 - Train Loss: 0.1351, Val Loss: 0.1423\n","Epoch 1726/4000 - Train Loss: 0.1362, Val Loss: 0.1412\n","Epoch 1727/4000 - Train Loss: 0.1392, Val Loss: 0.1410\n","Epoch 1728/4000 - Train Loss: 0.1361, Val Loss: 0.1400 *\n","Epoch 1729/4000 - Train Loss: 0.1367, Val Loss: 0.1399\n","Epoch 1730/4000 - Train Loss: 0.1360, Val Loss: 0.1398 *\n","Epoch 1731/4000 - Train Loss: 0.1370, Val Loss: 0.1409\n","Epoch 1732/4000 - Train Loss: 0.1371, Val Loss: 0.1426\n","Epoch 1733/4000 - Train Loss: 0.1352, Val Loss: 0.1418\n","Epoch 1734/4000 - Train Loss: 0.1352, Val Loss: 0.1414\n","Epoch 1735/4000 - Train Loss: 0.1359, Val Loss: 0.1404\n","Epoch 1736/4000 - Train Loss: 0.1341, Val Loss: 0.1408\n","Epoch 1737/4000 - Train Loss: 0.1373, Val Loss: 0.1414\n","Epoch 1738/4000 - Train Loss: 0.1380, Val Loss: 0.1403\n","Epoch 1739/4000 - Train Loss: 0.1366, Val Loss: 0.1406\n","Epoch 1740/4000 - Train Loss: 0.1347, Val Loss: 0.1406\n","Epoch 1741/4000 - Train Loss: 0.1354, Val Loss: 0.1414\n","Epoch 1742/4000 - Train Loss: 0.1345, Val Loss: 0.1416\n","Epoch 1743/4000 - Train Loss: 0.1351, Val Loss: 0.1417\n","Epoch 1744/4000 - Train Loss: 0.1350, Val Loss: 0.1410\n","Epoch 1745/4000 - Train Loss: 0.1348, Val Loss: 0.1399\n","Epoch 1746/4000 - Train Loss: 0.1349, Val Loss: 0.1395 *\n","Epoch 1747/4000 - Train Loss: 0.1339, Val Loss: 0.1390 *\n","Epoch 1748/4000 - Train Loss: 0.1367, Val Loss: 0.1390\n","Epoch 1749/4000 - Train Loss: 0.1338, Val Loss: 0.1391\n","Epoch 1750/4000 - Train Loss: 0.1362, Val Loss: 0.1394\n","Epoch 1751/4000 - Train Loss: 0.1341, Val Loss: 0.1400\n","Epoch 1752/4000 - Train Loss: 0.1345, Val Loss: 0.1407\n","Epoch 1753/4000 - Train Loss: 0.1353, Val Loss: 0.1411\n","Epoch 1754/4000 - Train Loss: 0.1345, Val Loss: 0.1410\n","Epoch 1755/4000 - Train Loss: 0.1346, Val Loss: 0.1404\n","Epoch 1756/4000 - Train Loss: 0.1361, Val Loss: 0.1401\n","Epoch 1757/4000 - Train Loss: 0.1325, Val Loss: 0.1397\n","Epoch 1758/4000 - Train Loss: 0.1331, Val Loss: 0.1392\n","Epoch 1759/4000 - Train Loss: 0.1351, Val Loss: 0.1394\n","Epoch 1760/4000 - Train Loss: 0.1359, Val Loss: 0.1398\n","Epoch 1761/4000 - Train Loss: 0.1327, Val Loss: 0.1403\n","Epoch 1762/4000 - Train Loss: 0.1332, Val Loss: 0.1413\n","Epoch 1763/4000 - Train Loss: 0.1339, Val Loss: 0.1405\n","Epoch 1764/4000 - Train Loss: 0.1338, Val Loss: 0.1404\n","Epoch 1765/4000 - Train Loss: 0.1327, Val Loss: 0.1397\n","Epoch 1766/4000 - Train Loss: 0.1407, Val Loss: 0.1415\n","Epoch 1767/4000 - Train Loss: 0.1353, Val Loss: 0.1395\n","Epoch 1768/4000 - Train Loss: 0.1341, Val Loss: 0.1409\n","Epoch 1769/4000 - Train Loss: 0.1345, Val Loss: 0.1412\n","Epoch 1770/4000 - Train Loss: 0.1334, Val Loss: 0.1399\n","Epoch 1771/4000 - Train Loss: 0.1318, Val Loss: 0.1399\n","Epoch 1772/4000 - Train Loss: 0.1352, Val Loss: 0.1392\n","Epoch 1773/4000 - Train Loss: 0.1336, Val Loss: 0.1394\n","Epoch 1774/4000 - Train Loss: 0.1336, Val Loss: 0.1394\n","Epoch 1775/4000 - Train Loss: 0.1343, Val Loss: 0.1390\n","Epoch 1776/4000 - Train Loss: 0.1328, Val Loss: 0.1395\n","Epoch 1777/4000 - Train Loss: 0.1340, Val Loss: 0.1395\n","Epoch 1778/4000 - Train Loss: 0.1328, Val Loss: 0.1405\n","Epoch 1779/4000 - Train Loss: 0.1337, Val Loss: 0.1412\n","Epoch 1780/4000 - Train Loss: 0.1330, Val Loss: 0.1400\n","Epoch 1781/4000 - Train Loss: 0.1315, Val Loss: 0.1397\n","Epoch 1782/4000 - Train Loss: 0.1325, Val Loss: 0.1388 *\n","Epoch 1783/4000 - Train Loss: 0.1316, Val Loss: 0.1389\n","Epoch 1784/4000 - Train Loss: 0.1340, Val Loss: 0.1391\n","Epoch 1785/4000 - Train Loss: 0.1332, Val Loss: 0.1381 *\n","Epoch 1786/4000 - Train Loss: 0.1353, Val Loss: 0.1387\n","Epoch 1787/4000 - Train Loss: 0.1332, Val Loss: 0.1396\n","Epoch 1788/4000 - Train Loss: 0.1312, Val Loss: 0.1403\n","Epoch 1789/4000 - Train Loss: 0.1324, Val Loss: 0.1401\n","Epoch 1790/4000 - Train Loss: 0.1321, Val Loss: 0.1393\n","Epoch 1791/4000 - Train Loss: 0.1311, Val Loss: 0.1390\n","Epoch 1792/4000 - Train Loss: 0.1319, Val Loss: 0.1387\n","Epoch 1793/4000 - Train Loss: 0.1306, Val Loss: 0.1387\n","Epoch 1794/4000 - Train Loss: 0.1333, Val Loss: 0.1388\n","Epoch 1795/4000 - Train Loss: 0.1321, Val Loss: 0.1382\n","Epoch 1796/4000 - Train Loss: 0.1314, Val Loss: 0.1380\n","Epoch 1797/4000 - Train Loss: 0.1297, Val Loss: 0.1381\n","Epoch 1798/4000 - Train Loss: 0.1302, Val Loss: 0.1384\n","Epoch 1799/4000 - Train Loss: 0.1298, Val Loss: 0.1382\n","Epoch 1800/4000 - Train Loss: 0.1317, Val Loss: 0.1376 *\n","Epoch 1801/4000 - Train Loss: 0.1298, Val Loss: 0.1372 *\n","Epoch 1802/4000 - Train Loss: 0.1327, Val Loss: 0.1370 *\n","Epoch 1803/4000 - Train Loss: 0.1300, Val Loss: 0.1369\n","Epoch 1804/4000 - Train Loss: 0.1301, Val Loss: 0.1368 *\n","Epoch 1805/4000 - Train Loss: 0.1319, Val Loss: 0.1368\n","Epoch 1806/4000 - Train Loss: 0.1292, Val Loss: 0.1367\n","Epoch 1807/4000 - Train Loss: 0.1314, Val Loss: 0.1368\n","Epoch 1808/4000 - Train Loss: 0.1304, Val Loss: 0.1374\n","Epoch 1809/4000 - Train Loss: 0.1316, Val Loss: 0.1384\n","Epoch 1810/4000 - Train Loss: 0.1311, Val Loss: 0.1386\n","Epoch 1811/4000 - Train Loss: 0.1312, Val Loss: 0.1378\n","Epoch 1812/4000 - Train Loss: 0.1288, Val Loss: 0.1372\n","Epoch 1813/4000 - Train Loss: 0.1318, Val Loss: 0.1366 *\n","Epoch 1814/4000 - Train Loss: 0.1320, Val Loss: 0.1367\n","Epoch 1815/4000 - Train Loss: 0.1287, Val Loss: 0.1368\n","Epoch 1816/4000 - Train Loss: 0.1324, Val Loss: 0.1370\n","Epoch 1817/4000 - Train Loss: 0.1305, Val Loss: 0.1367\n","Epoch 1818/4000 - Train Loss: 0.1288, Val Loss: 0.1378\n","Epoch 1819/4000 - Train Loss: 0.1313, Val Loss: 0.1384\n","Epoch 1820/4000 - Train Loss: 0.1322, Val Loss: 0.1377\n","Epoch 1821/4000 - Train Loss: 0.1287, Val Loss: 0.1370\n","Epoch 1822/4000 - Train Loss: 0.1317, Val Loss: 0.1366\n","Epoch 1823/4000 - Train Loss: 0.1290, Val Loss: 0.1362 *\n","Epoch 1824/4000 - Train Loss: 0.1312, Val Loss: 0.1358 *\n","Epoch 1825/4000 - Train Loss: 0.1299, Val Loss: 0.1361\n","Epoch 1826/4000 - Train Loss: 0.1303, Val Loss: 0.1359\n","Epoch 1827/4000 - Train Loss: 0.1285, Val Loss: 0.1365\n","Epoch 1828/4000 - Train Loss: 0.1290, Val Loss: 0.1371\n","Epoch 1829/4000 - Train Loss: 0.1295, Val Loss: 0.1363\n","Epoch 1830/4000 - Train Loss: 0.1306, Val Loss: 0.1374\n","Epoch 1831/4000 - Train Loss: 0.1310, Val Loss: 0.1378\n","Epoch 1832/4000 - Train Loss: 0.1298, Val Loss: 0.1384\n","Epoch 1833/4000 - Train Loss: 0.1298, Val Loss: 0.1380\n","Epoch 1834/4000 - Train Loss: 0.1279, Val Loss: 0.1377\n","Epoch 1835/4000 - Train Loss: 0.1275, Val Loss: 0.1374\n","Epoch 1836/4000 - Train Loss: 0.1294, Val Loss: 0.1373\n","Epoch 1837/4000 - Train Loss: 0.1285, Val Loss: 0.1368\n","Epoch 1838/4000 - Train Loss: 0.1286, Val Loss: 0.1368\n","Epoch 1839/4000 - Train Loss: 0.1289, Val Loss: 0.1364\n","Epoch 1840/4000 - Train Loss: 0.1297, Val Loss: 0.1365\n","Epoch 1841/4000 - Train Loss: 0.1285, Val Loss: 0.1366\n","Epoch 1842/4000 - Train Loss: 0.1296, Val Loss: 0.1369\n","Epoch 1843/4000 - Train Loss: 0.1272, Val Loss: 0.1374\n","Epoch 1844/4000 - Train Loss: 0.1282, Val Loss: 0.1382\n","Epoch 1845/4000 - Train Loss: 0.1269, Val Loss: 0.1392\n","Epoch 1846/4000 - Train Loss: 0.1303, Val Loss: 0.1381\n","Epoch 1847/4000 - Train Loss: 0.1275, Val Loss: 0.1370\n","Epoch 1848/4000 - Train Loss: 0.1273, Val Loss: 0.1362\n","Epoch 1849/4000 - Train Loss: 0.1268, Val Loss: 0.1353 *\n","Epoch 1850/4000 - Train Loss: 0.1286, Val Loss: 0.1355\n","Epoch 1851/4000 - Train Loss: 0.1292, Val Loss: 0.1351 *\n","Epoch 1852/4000 - Train Loss: 0.1275, Val Loss: 0.1352\n","Epoch 1853/4000 - Train Loss: 0.1290, Val Loss: 0.1353\n","Epoch 1854/4000 - Train Loss: 0.1286, Val Loss: 0.1353\n","Epoch 1855/4000 - Train Loss: 0.1265, Val Loss: 0.1355\n","Epoch 1856/4000 - Train Loss: 0.1277, Val Loss: 0.1359\n","Epoch 1857/4000 - Train Loss: 0.1274, Val Loss: 0.1362\n","Epoch 1858/4000 - Train Loss: 0.1269, Val Loss: 0.1367\n","Epoch 1859/4000 - Train Loss: 0.1265, Val Loss: 0.1368\n","Epoch 1860/4000 - Train Loss: 0.1272, Val Loss: 0.1367\n","Epoch 1861/4000 - Train Loss: 0.1279, Val Loss: 0.1363\n","Epoch 1862/4000 - Train Loss: 0.1272, Val Loss: 0.1360\n","Epoch 1863/4000 - Train Loss: 0.1275, Val Loss: 0.1355\n","Epoch 1864/4000 - Train Loss: 0.1261, Val Loss: 0.1351\n","Epoch 1865/4000 - Train Loss: 0.1278, Val Loss: 0.1352\n","Epoch 1866/4000 - Train Loss: 0.1271, Val Loss: 0.1356\n","Epoch 1867/4000 - Train Loss: 0.1277, Val Loss: 0.1361\n","Epoch 1868/4000 - Train Loss: 0.1259, Val Loss: 0.1366\n","Epoch 1869/4000 - Train Loss: 0.1262, Val Loss: 0.1373\n","Epoch 1870/4000 - Train Loss: 0.1251, Val Loss: 0.1375\n","Epoch 1871/4000 - Train Loss: 0.1257, Val Loss: 0.1369\n","Epoch 1872/4000 - Train Loss: 0.1264, Val Loss: 0.1363\n","Epoch 1873/4000 - Train Loss: 0.1257, Val Loss: 0.1359\n","Epoch 1874/4000 - Train Loss: 0.1258, Val Loss: 0.1357\n","Epoch 1875/4000 - Train Loss: 0.1264, Val Loss: 0.1350\n","Epoch 1876/4000 - Train Loss: 0.1263, Val Loss: 0.1352\n","Epoch 1877/4000 - Train Loss: 0.1266, Val Loss: 0.1347 *\n","Epoch 1878/4000 - Train Loss: 0.1268, Val Loss: 0.1347\n","Epoch 1879/4000 - Train Loss: 0.1272, Val Loss: 0.1344 *\n","Epoch 1880/4000 - Train Loss: 0.1247, Val Loss: 0.1344\n","Epoch 1881/4000 - Train Loss: 0.1252, Val Loss: 0.1342 *\n","Epoch 1882/4000 - Train Loss: 0.1262, Val Loss: 0.1344\n","Epoch 1883/4000 - Train Loss: 0.1266, Val Loss: 0.1349\n","Epoch 1884/4000 - Train Loss: 0.1249, Val Loss: 0.1357\n","Epoch 1885/4000 - Train Loss: 0.1250, Val Loss: 0.1364\n","Epoch 1886/4000 - Train Loss: 0.1268, Val Loss: 0.1365\n","Epoch 1887/4000 - Train Loss: 0.1252, Val Loss: 0.1365\n","Epoch 1888/4000 - Train Loss: 0.1264, Val Loss: 0.1356\n","Epoch 1889/4000 - Train Loss: 0.1252, Val Loss: 0.1351\n","Epoch 1890/4000 - Train Loss: 0.1268, Val Loss: 0.1341 *\n","Epoch 1891/4000 - Train Loss: 0.1263, Val Loss: 0.1345\n","Epoch 1892/4000 - Train Loss: 0.1264, Val Loss: 0.1340 *\n","Epoch 1893/4000 - Train Loss: 0.1256, Val Loss: 0.1343\n","Epoch 1894/4000 - Train Loss: 0.1251, Val Loss: 0.1340\n","Epoch 1895/4000 - Train Loss: 0.1249, Val Loss: 0.1342\n","Epoch 1896/4000 - Train Loss: 0.1258, Val Loss: 0.1349\n","Epoch 1897/4000 - Train Loss: 0.1244, Val Loss: 0.1355\n","Epoch 1898/4000 - Train Loss: 0.1242, Val Loss: 0.1359\n","Epoch 1899/4000 - Train Loss: 0.1249, Val Loss: 0.1357\n","Epoch 1900/4000 - Train Loss: 0.1231, Val Loss: 0.1347\n","Epoch 1901/4000 - Train Loss: 0.1258, Val Loss: 0.1343\n","Epoch 1902/4000 - Train Loss: 0.1243, Val Loss: 0.1337 *\n","Epoch 1903/4000 - Train Loss: 0.1253, Val Loss: 0.1333 *\n","Epoch 1904/4000 - Train Loss: 0.1253, Val Loss: 0.1341\n","Epoch 1905/4000 - Train Loss: 0.1271, Val Loss: 0.1333\n","Epoch 1906/4000 - Train Loss: 0.1237, Val Loss: 0.1341\n","Epoch 1907/4000 - Train Loss: 0.1232, Val Loss: 0.1348\n","Epoch 1908/4000 - Train Loss: 0.1235, Val Loss: 0.1344\n","Epoch 1909/4000 - Train Loss: 0.1228, Val Loss: 0.1343\n","Epoch 1910/4000 - Train Loss: 0.1244, Val Loss: 0.1339\n","Epoch 1911/4000 - Train Loss: 0.1225, Val Loss: 0.1341\n","Epoch 1912/4000 - Train Loss: 0.1247, Val Loss: 0.1337\n","Epoch 1913/4000 - Train Loss: 0.1227, Val Loss: 0.1341\n","Epoch 1914/4000 - Train Loss: 0.1235, Val Loss: 0.1335\n","Epoch 1915/4000 - Train Loss: 0.1225, Val Loss: 0.1338\n","Epoch 1916/4000 - Train Loss: 0.1239, Val Loss: 0.1336\n","Epoch 1917/4000 - Train Loss: 0.1231, Val Loss: 0.1340\n","Epoch 1918/4000 - Train Loss: 0.1244, Val Loss: 0.1341\n","Epoch 1919/4000 - Train Loss: 0.1221, Val Loss: 0.1338\n","Epoch 1920/4000 - Train Loss: 0.1210, Val Loss: 0.1338\n","Epoch 1921/4000 - Train Loss: 0.1231, Val Loss: 0.1341\n","Epoch 1922/4000 - Train Loss: 0.1216, Val Loss: 0.1345\n","Epoch 1923/4000 - Train Loss: 0.1220, Val Loss: 0.1341\n","Epoch 1924/4000 - Train Loss: 0.1221, Val Loss: 0.1335\n","Epoch 1925/4000 - Train Loss: 0.1219, Val Loss: 0.1332 *\n","Epoch 1926/4000 - Train Loss: 0.1247, Val Loss: 0.1335\n","Epoch 1927/4000 - Train Loss: 0.1222, Val Loss: 0.1340\n","Epoch 1928/4000 - Train Loss: 0.1221, Val Loss: 0.1346\n","Epoch 1929/4000 - Train Loss: 0.1230, Val Loss: 0.1349\n","Epoch 1930/4000 - Train Loss: 0.1236, Val Loss: 0.1342\n","Epoch 1931/4000 - Train Loss: 0.1226, Val Loss: 0.1342\n","Epoch 1932/4000 - Train Loss: 0.1242, Val Loss: 0.1336\n","Epoch 1933/4000 - Train Loss: 0.1207, Val Loss: 0.1334\n","Epoch 1934/4000 - Train Loss: 0.1233, Val Loss: 0.1334\n","Epoch 1935/4000 - Train Loss: 0.1231, Val Loss: 0.1334\n","Epoch 1936/4000 - Train Loss: 0.1226, Val Loss: 0.1339\n","Epoch 1937/4000 - Train Loss: 0.1224, Val Loss: 0.1338\n","Epoch 1938/4000 - Train Loss: 0.1221, Val Loss: 0.1339\n","Epoch 1939/4000 - Train Loss: 0.1213, Val Loss: 0.1341\n","Epoch 1940/4000 - Train Loss: 0.1222, Val Loss: 0.1343\n","Epoch 1941/4000 - Train Loss: 0.1212, Val Loss: 0.1346\n","Epoch 1942/4000 - Train Loss: 0.1212, Val Loss: 0.1353\n","Epoch 1943/4000 - Train Loss: 0.1232, Val Loss: 0.1356\n","Epoch 1944/4000 - Train Loss: 0.1220, Val Loss: 0.1352\n","Epoch 1945/4000 - Train Loss: 0.1223, Val Loss: 0.1335\n","Epoch 1946/4000 - Train Loss: 0.1213, Val Loss: 0.1326 *\n","Epoch 1947/4000 - Train Loss: 0.1224, Val Loss: 0.1340\n","Epoch 1948/4000 - Train Loss: 0.1213, Val Loss: 0.1323 *\n","Epoch 1949/4000 - Train Loss: 0.1222, Val Loss: 0.1339\n","Epoch 1950/4000 - Train Loss: 0.1238, Val Loss: 0.1332\n","Epoch 1951/4000 - Train Loss: 0.1226, Val Loss: 0.1332\n","Epoch 1952/4000 - Train Loss: 0.1229, Val Loss: 0.1331\n","Epoch 1953/4000 - Train Loss: 0.1217, Val Loss: 0.1321 *\n","Epoch 1954/4000 - Train Loss: 0.1199, Val Loss: 0.1327\n","Epoch 1955/4000 - Train Loss: 0.1221, Val Loss: 0.1313 *\n","Epoch 1956/4000 - Train Loss: 0.1214, Val Loss: 0.1327\n","Epoch 1957/4000 - Train Loss: 0.1218, Val Loss: 0.1312 *\n","Epoch 1958/4000 - Train Loss: 0.1205, Val Loss: 0.1324\n","Epoch 1959/4000 - Train Loss: 0.1230, Val Loss: 0.1322\n","Epoch 1960/4000 - Train Loss: 0.1235, Val Loss: 0.1328\n","Epoch 1961/4000 - Train Loss: 0.1211, Val Loss: 0.1340\n","Epoch 1962/4000 - Train Loss: 0.1211, Val Loss: 0.1336\n","Epoch 1963/4000 - Train Loss: 0.1210, Val Loss: 0.1343\n","Epoch 1964/4000 - Train Loss: 0.1237, Val Loss: 0.1326\n","Epoch 1965/4000 - Train Loss: 0.1194, Val Loss: 0.1344\n","Epoch 1966/4000 - Train Loss: 0.1218, Val Loss: 0.1337\n","Epoch 1967/4000 - Train Loss: 0.1207, Val Loss: 0.1330\n","Epoch 1968/4000 - Train Loss: 0.1219, Val Loss: 0.1328\n","Epoch 1969/4000 - Train Loss: 0.1200, Val Loss: 0.1325\n","Epoch 1970/4000 - Train Loss: 0.1211, Val Loss: 0.1326\n","Epoch 1971/4000 - Train Loss: 0.1221, Val Loss: 0.1319\n","Epoch 1972/4000 - Train Loss: 0.1202, Val Loss: 0.1314\n","Epoch 1973/4000 - Train Loss: 0.1196, Val Loss: 0.1309 *\n","Epoch 1974/4000 - Train Loss: 0.1203, Val Loss: 0.1309\n","Epoch 1975/4000 - Train Loss: 0.1206, Val Loss: 0.1311\n","Epoch 1976/4000 - Train Loss: 0.1179, Val Loss: 0.1315\n","Epoch 1977/4000 - Train Loss: 0.1201, Val Loss: 0.1314\n","Epoch 1978/4000 - Train Loss: 0.1192, Val Loss: 0.1317\n","Epoch 1979/4000 - Train Loss: 0.1185, Val Loss: 0.1320\n","Epoch 1980/4000 - Train Loss: 0.1209, Val Loss: 0.1322\n","Epoch 1981/4000 - Train Loss: 0.1190, Val Loss: 0.1317\n","Epoch 1982/4000 - Train Loss: 0.1173, Val Loss: 0.1317\n","Epoch 1983/4000 - Train Loss: 0.1196, Val Loss: 0.1318\n","Epoch 1984/4000 - Train Loss: 0.1200, Val Loss: 0.1319\n","Epoch 1985/4000 - Train Loss: 0.1189, Val Loss: 0.1330\n","Epoch 1986/4000 - Train Loss: 0.1208, Val Loss: 0.1339\n","Epoch 1987/4000 - Train Loss: 0.1195, Val Loss: 0.1349\n","Epoch 1988/4000 - Train Loss: 0.1188, Val Loss: 0.1354\n","Epoch 1989/4000 - Train Loss: 0.1217, Val Loss: 0.1329\n","Epoch 1990/4000 - Train Loss: 0.1187, Val Loss: 0.1322\n","Epoch 1991/4000 - Train Loss: 0.1192, Val Loss: 0.1312\n","Epoch 1992/4000 - Train Loss: 0.1195, Val Loss: 0.1313\n","Epoch 1993/4000 - Train Loss: 0.1203, Val Loss: 0.1314\n","Epoch 1994/4000 - Train Loss: 0.1194, Val Loss: 0.1306 *\n","Epoch 1995/4000 - Train Loss: 0.1181, Val Loss: 0.1310\n","Epoch 1996/4000 - Train Loss: 0.1209, Val Loss: 0.1326\n","Epoch 1997/4000 - Train Loss: 0.1183, Val Loss: 0.1324\n","Epoch 1998/4000 - Train Loss: 0.1169, Val Loss: 0.1330\n","Epoch 1999/4000 - Train Loss: 0.1184, Val Loss: 0.1321\n","Epoch 2000/4000 - Train Loss: 0.1181, Val Loss: 0.1310\n","Epoch 2001/4000 - Train Loss: 0.1186, Val Loss: 0.1313\n","Epoch 2002/4000 - Train Loss: 0.1214, Val Loss: 0.1309\n","Epoch 2003/4000 - Train Loss: 0.1175, Val Loss: 0.1313\n","Epoch 2004/4000 - Train Loss: 0.1178, Val Loss: 0.1314\n","Epoch 2005/4000 - Train Loss: 0.1189, Val Loss: 0.1337\n","Epoch 2006/4000 - Train Loss: 0.1188, Val Loss: 0.1332\n","Epoch 2007/4000 - Train Loss: 0.1183, Val Loss: 0.1335\n","Epoch 2008/4000 - Train Loss: 0.1197, Val Loss: 0.1323\n","Epoch 2009/4000 - Train Loss: 0.1182, Val Loss: 0.1311\n","Epoch 2010/4000 - Train Loss: 0.1173, Val Loss: 0.1318\n","Epoch 2011/4000 - Train Loss: 0.1191, Val Loss: 0.1310\n","Epoch 2012/4000 - Train Loss: 0.1177, Val Loss: 0.1317\n","Epoch 2013/4000 - Train Loss: 0.1189, Val Loss: 0.1311\n","Epoch 2014/4000 - Train Loss: 0.1192, Val Loss: 0.1320\n","Epoch 2015/4000 - Train Loss: 0.1178, Val Loss: 0.1344\n","Epoch 2016/4000 - Train Loss: 0.1181, Val Loss: 0.1347\n","Epoch 2017/4000 - Train Loss: 0.1175, Val Loss: 0.1345\n","Epoch 2018/4000 - Train Loss: 0.1195, Val Loss: 0.1336\n","Epoch 2019/4000 - Train Loss: 0.1186, Val Loss: 0.1324\n","Epoch 2020/4000 - Train Loss: 0.1165, Val Loss: 0.1316\n","Epoch 2021/4000 - Train Loss: 0.1174, Val Loss: 0.1308\n","Epoch 2022/4000 - Train Loss: 0.1176, Val Loss: 0.1312\n","Epoch 2023/4000 - Train Loss: 0.1198, Val Loss: 0.1308\n","Epoch 2024/4000 - Train Loss: 0.1228, Val Loss: 0.1345\n","Epoch 2025/4000 - Train Loss: 0.1201, Val Loss: 0.1322\n","Epoch 2026/4000 - Train Loss: 0.1176, Val Loss: 0.1334\n","Epoch 2027/4000 - Train Loss: 0.1180, Val Loss: 0.1354\n","Epoch 2028/4000 - Train Loss: 0.1211, Val Loss: 0.1321\n","Epoch 2029/4000 - Train Loss: 0.1159, Val Loss: 0.1332\n","Epoch 2030/4000 - Train Loss: 0.1185, Val Loss: 0.1309\n","Epoch 2031/4000 - Train Loss: 0.1156, Val Loss: 0.1316\n","Epoch 2032/4000 - Train Loss: 0.1181, Val Loss: 0.1321\n","Epoch 2033/4000 - Train Loss: 0.1184, Val Loss: 0.1301 *\n","Epoch 2034/4000 - Train Loss: 0.1160, Val Loss: 0.1313\n","Epoch 2035/4000 - Train Loss: 0.1167, Val Loss: 0.1320\n","Epoch 2036/4000 - Train Loss: 0.1154, Val Loss: 0.1330\n","Epoch 2037/4000 - Train Loss: 0.1174, Val Loss: 0.1351\n","Epoch 2038/4000 - Train Loss: 0.1186, Val Loss: 0.1332\n","Epoch 2039/4000 - Train Loss: 0.1170, Val Loss: 0.1319\n","Epoch 2040/4000 - Train Loss: 0.1155, Val Loss: 0.1308\n","Epoch 2041/4000 - Train Loss: 0.1161, Val Loss: 0.1302\n","Epoch 2042/4000 - Train Loss: 0.1169, Val Loss: 0.1304\n","Epoch 2043/4000 - Train Loss: 0.1169, Val Loss: 0.1300\n","Epoch 2044/4000 - Train Loss: 0.1173, Val Loss: 0.1306\n","Epoch 2045/4000 - Train Loss: 0.1155, Val Loss: 0.1314\n","Epoch 2046/4000 - Train Loss: 0.1171, Val Loss: 0.1316\n","Epoch 2047/4000 - Train Loss: 0.1153, Val Loss: 0.1323\n","Epoch 2048/4000 - Train Loss: 0.1160, Val Loss: 0.1313\n","Epoch 2049/4000 - Train Loss: 0.1170, Val Loss: 0.1317\n","Epoch 2050/4000 - Train Loss: 0.1154, Val Loss: 0.1312\n","Epoch 2051/4000 - Train Loss: 0.1167, Val Loss: 0.1298 *\n","Epoch 2052/4000 - Train Loss: 0.1160, Val Loss: 0.1306\n","Epoch 2053/4000 - Train Loss: 0.1162, Val Loss: 0.1303\n","Epoch 2054/4000 - Train Loss: 0.1156, Val Loss: 0.1307\n","Epoch 2055/4000 - Train Loss: 0.1162, Val Loss: 0.1314\n","Epoch 2056/4000 - Train Loss: 0.1144, Val Loss: 0.1313\n","Epoch 2057/4000 - Train Loss: 0.1156, Val Loss: 0.1317\n","Epoch 2058/4000 - Train Loss: 0.1158, Val Loss: 0.1318\n","Epoch 2059/4000 - Train Loss: 0.1157, Val Loss: 0.1317\n","Epoch 2060/4000 - Train Loss: 0.1154, Val Loss: 0.1310\n","Epoch 2061/4000 - Train Loss: 0.1142, Val Loss: 0.1303\n","Epoch 2062/4000 - Train Loss: 0.1135, Val Loss: 0.1302\n","Epoch 2063/4000 - Train Loss: 0.1145, Val Loss: 0.1299\n","Epoch 2064/4000 - Train Loss: 0.1144, Val Loss: 0.1299\n","Epoch 2065/4000 - Train Loss: 0.1127, Val Loss: 0.1301\n","Epoch 2066/4000 - Train Loss: 0.1162, Val Loss: 0.1306\n","Epoch 2067/4000 - Train Loss: 0.1149, Val Loss: 0.1304\n","Epoch 2068/4000 - Train Loss: 0.1143, Val Loss: 0.1305\n","Epoch 2069/4000 - Train Loss: 0.1139, Val Loss: 0.1305\n","Epoch 2070/4000 - Train Loss: 0.1163, Val Loss: 0.1304\n","Epoch 2071/4000 - Train Loss: 0.1151, Val Loss: 0.1297\n","Epoch 2072/4000 - Train Loss: 0.1133, Val Loss: 0.1296 *\n","Epoch 2073/4000 - Train Loss: 0.1145, Val Loss: 0.1298\n","Epoch 2074/4000 - Train Loss: 0.1153, Val Loss: 0.1294 *\n","Epoch 2075/4000 - Train Loss: 0.1139, Val Loss: 0.1294\n","Epoch 2076/4000 - Train Loss: 0.1130, Val Loss: 0.1298\n","Epoch 2077/4000 - Train Loss: 0.1143, Val Loss: 0.1309\n","Epoch 2078/4000 - Train Loss: 0.1135, Val Loss: 0.1314\n","Epoch 2079/4000 - Train Loss: 0.1143, Val Loss: 0.1317\n","Epoch 2080/4000 - Train Loss: 0.1126, Val Loss: 0.1318\n","Epoch 2081/4000 - Train Loss: 0.1131, Val Loss: 0.1310\n","Epoch 2082/4000 - Train Loss: 0.1128, Val Loss: 0.1307\n","Epoch 2083/4000 - Train Loss: 0.1175, Val Loss: 0.1308\n","Epoch 2084/4000 - Train Loss: 0.1123, Val Loss: 0.1310\n","Epoch 2085/4000 - Train Loss: 0.1120, Val Loss: 0.1309\n","Epoch 2086/4000 - Train Loss: 0.1156, Val Loss: 0.1310\n","Epoch 2087/4000 - Train Loss: 0.1133, Val Loss: 0.1311\n","Epoch 2088/4000 - Train Loss: 0.1116, Val Loss: 0.1310\n","Epoch 2089/4000 - Train Loss: 0.1132, Val Loss: 0.1308\n","Epoch 2090/4000 - Train Loss: 0.1128, Val Loss: 0.1302\n","Epoch 2091/4000 - Train Loss: 0.1125, Val Loss: 0.1296\n","Epoch 2092/4000 - Train Loss: 0.1119, Val Loss: 0.1294\n","Epoch 2093/4000 - Train Loss: 0.1128, Val Loss: 0.1288 *\n","Epoch 2094/4000 - Train Loss: 0.1138, Val Loss: 0.1287\n","Epoch 2095/4000 - Train Loss: 0.1134, Val Loss: 0.1287\n","Epoch 2096/4000 - Train Loss: 0.1118, Val Loss: 0.1290\n","Epoch 2097/4000 - Train Loss: 0.1118, Val Loss: 0.1297\n","Epoch 2098/4000 - Train Loss: 0.1121, Val Loss: 0.1310\n","Epoch 2099/4000 - Train Loss: 0.1125, Val Loss: 0.1319\n","Epoch 2100/4000 - Train Loss: 0.1132, Val Loss: 0.1315\n","Epoch 2101/4000 - Train Loss: 0.1128, Val Loss: 0.1307\n","Epoch 2102/4000 - Train Loss: 0.1119, Val Loss: 0.1301\n","Epoch 2103/4000 - Train Loss: 0.1134, Val Loss: 0.1299\n","Epoch 2104/4000 - Train Loss: 0.1142, Val Loss: 0.1297\n","Epoch 2105/4000 - Train Loss: 0.1131, Val Loss: 0.1293\n","Epoch 2106/4000 - Train Loss: 0.1129, Val Loss: 0.1298\n","Epoch 2107/4000 - Train Loss: 0.1116, Val Loss: 0.1307\n","Epoch 2108/4000 - Train Loss: 0.1110, Val Loss: 0.1310\n","Epoch 2109/4000 - Train Loss: 0.1121, Val Loss: 0.1302\n","Epoch 2110/4000 - Train Loss: 0.1114, Val Loss: 0.1298\n","Epoch 2111/4000 - Train Loss: 0.1127, Val Loss: 0.1288\n","Epoch 2112/4000 - Train Loss: 0.1118, Val Loss: 0.1290\n","Epoch 2113/4000 - Train Loss: 0.1129, Val Loss: 0.1291\n","Epoch 2114/4000 - Train Loss: 0.1120, Val Loss: 0.1289\n","Epoch 2115/4000 - Train Loss: 0.1126, Val Loss: 0.1291\n","Epoch 2116/4000 - Train Loss: 0.1110, Val Loss: 0.1293\n","Epoch 2117/4000 - Train Loss: 0.1127, Val Loss: 0.1304\n","Epoch 2118/4000 - Train Loss: 0.1148, Val Loss: 0.1333\n","Epoch 2119/4000 - Train Loss: 0.1136, Val Loss: 0.1308\n","Epoch 2120/4000 - Train Loss: 0.1134, Val Loss: 0.1307\n","Epoch 2121/4000 - Train Loss: 0.1115, Val Loss: 0.1298\n","Epoch 2122/4000 - Train Loss: 0.1112, Val Loss: 0.1289\n","Epoch 2123/4000 - Train Loss: 0.1114, Val Loss: 0.1288\n","Epoch 2124/4000 - Train Loss: 0.1109, Val Loss: 0.1283 *\n","Epoch 2125/4000 - Train Loss: 0.1099, Val Loss: 0.1284\n","Epoch 2126/4000 - Train Loss: 0.1140, Val Loss: 0.1283\n","Epoch 2127/4000 - Train Loss: 0.1136, Val Loss: 0.1285\n","Epoch 2128/4000 - Train Loss: 0.1095, Val Loss: 0.1287\n","Epoch 2129/4000 - Train Loss: 0.1090, Val Loss: 0.1291\n","Epoch 2130/4000 - Train Loss: 0.1096, Val Loss: 0.1296\n","Epoch 2131/4000 - Train Loss: 0.1100, Val Loss: 0.1296\n","Epoch 2132/4000 - Train Loss: 0.1121, Val Loss: 0.1299\n","Epoch 2133/4000 - Train Loss: 0.1110, Val Loss: 0.1295\n","Epoch 2134/4000 - Train Loss: 0.1115, Val Loss: 0.1286\n","Epoch 2135/4000 - Train Loss: 0.1108, Val Loss: 0.1282\n","Epoch 2136/4000 - Train Loss: 0.1096, Val Loss: 0.1277 *\n","Epoch 2137/4000 - Train Loss: 0.1097, Val Loss: 0.1280\n","Epoch 2138/4000 - Train Loss: 0.1101, Val Loss: 0.1277\n","Epoch 2139/4000 - Train Loss: 0.1091, Val Loss: 0.1280\n","Epoch 2140/4000 - Train Loss: 0.1105, Val Loss: 0.1283\n","Epoch 2141/4000 - Train Loss: 0.1102, Val Loss: 0.1294\n","Epoch 2142/4000 - Train Loss: 0.1104, Val Loss: 0.1314\n","Epoch 2143/4000 - Train Loss: 0.1116, Val Loss: 0.1300\n","Epoch 2144/4000 - Train Loss: 0.1101, Val Loss: 0.1294\n","Epoch 2145/4000 - Train Loss: 0.1096, Val Loss: 0.1302\n","Epoch 2146/4000 - Train Loss: 0.1107, Val Loss: 0.1300\n","Epoch 2147/4000 - Train Loss: 0.1099, Val Loss: 0.1304\n","Epoch 2148/4000 - Train Loss: 0.1093, Val Loss: 0.1303\n","Epoch 2149/4000 - Train Loss: 0.1094, Val Loss: 0.1300\n","Epoch 2150/4000 - Train Loss: 0.1100, Val Loss: 0.1299\n","Epoch 2151/4000 - Train Loss: 0.1104, Val Loss: 0.1297\n","Epoch 2152/4000 - Train Loss: 0.1092, Val Loss: 0.1300\n","Epoch 2153/4000 - Train Loss: 0.1094, Val Loss: 0.1296\n","Epoch 2154/4000 - Train Loss: 0.1095, Val Loss: 0.1297\n","Epoch 2155/4000 - Train Loss: 0.1090, Val Loss: 0.1296\n","Epoch 2156/4000 - Train Loss: 0.1079, Val Loss: 0.1296\n","Epoch 2157/4000 - Train Loss: 0.1087, Val Loss: 0.1298\n","Epoch 2158/4000 - Train Loss: 0.1098, Val Loss: 0.1298\n","Epoch 2159/4000 - Train Loss: 0.1067, Val Loss: 0.1298\n","Epoch 2160/4000 - Train Loss: 0.1105, Val Loss: 0.1295\n","Epoch 2161/4000 - Train Loss: 0.1088, Val Loss: 0.1291\n","Epoch 2162/4000 - Train Loss: 0.1089, Val Loss: 0.1288\n","Epoch 2163/4000 - Train Loss: 0.1092, Val Loss: 0.1286\n","Epoch 2164/4000 - Train Loss: 0.1091, Val Loss: 0.1285\n","Epoch 2165/4000 - Train Loss: 0.1074, Val Loss: 0.1284\n","Epoch 2166/4000 - Train Loss: 0.1087, Val Loss: 0.1290\n","Epoch 2167/4000 - Train Loss: 0.1093, Val Loss: 0.1318\n","Epoch 2168/4000 - Train Loss: 0.1103, Val Loss: 0.1312\n","Epoch 2169/4000 - Train Loss: 0.1088, Val Loss: 0.1301\n","Epoch 2170/4000 - Train Loss: 0.1090, Val Loss: 0.1289\n","Epoch 2171/4000 - Train Loss: 0.1091, Val Loss: 0.1291\n","Epoch 2172/4000 - Train Loss: 0.1096, Val Loss: 0.1301\n","Epoch 2173/4000 - Train Loss: 0.1090, Val Loss: 0.1296\n","Epoch 2174/4000 - Train Loss: 0.1104, Val Loss: 0.1297\n","Epoch 2175/4000 - Train Loss: 0.1084, Val Loss: 0.1298\n","Epoch 2176/4000 - Train Loss: 0.1072, Val Loss: 0.1303\n","\n","Early stopping triggered! Best epoch was 2136 with validation loss: 0.1277\n"]}]},{"cell_type":"code","source":["# write train and val loss to disk\n","with open(\"train_val_losses.csv\", \"w\") as f:\n","    f.write(\"epoch,train_loss,val_loss\\n\")\n","    for i, (tr_l, vl_l) in enumerate(zip(train_losses, val_losses), start=1):\n","        f.write(f\"{i},{tr_l},{vl_l}\\n\")\n","\n","import matplotlib.pyplot as plt\n","# plot train and val loss\n","plt.figure()\n","plt.plot(range(1, current_epoch+2), train_losses, label='Train Loss')\n","plt.plot(range(1, current_epoch+2), val_losses, label='Val Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.title(f'HGT {hidden_channels} dim, {num_layers} layers, {num_heads} heads')\n","plt.legend()\n","plt.savefig(\"loss_plot.png\", dpi=300)\n","plt.close()\n","\n","# evaluate on test set\n","model.eval()\n","with torch.no_grad():\n","    node_embs = model(x_dict, edge_index_dict, edge_attr_dict)\n","    test_loss = compute_loss(test_edges, (neg_test_src, neg_test_dst), test_edge_attr, model, node_embs)\n","print(f\"Test Loss: {test_loss.item():.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DI7NmkJp5d6z","outputId":"3952d4e7-0d13-4894-f1d1-f747d8cb68b6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Test Loss: 0.1335\n"]}]},{"cell_type":"code","source":["# save the trained model\n","model_path = \"trained_hgt_model.pt\"\n","print(f\"\\nSaving trained model to {model_path}...\")\n","torch.save(\n","    {\n","        \"epoch\": best_epoch,\n","        \"model_state_dict\": model.state_dict(),\n","        \"optimizer_state_dict\": optimizer.state_dict(),\n","        \"loss\": train_loss,\n","        \"hidden_channels\": hidden_channels,\n","        \"out_channels\": out_channels,\n","        \"num_layers\": num_layers,\n","        \"num_heads\": num_heads,\n","        \"node_types_count\": node_types_count,\n","        \"treat_type_dim\": treat_type_dim,\n","        \"gene_edge_dim\": gene_gene_feat_dim + mask_dim if gene_gene_feat_dim > 0 else 0,\n","    },\n","    model_path,\n",")\n","\n","\n","def load_trained_model(model_path, device):\n","    # Load the checkpoint\n","    checkpoint = torch.load(model_path, map_location=device)\n","\n","    # Initialize model with same parameters\n","    model = HGTModel(\n","        metadata=data.metadata(),\n","        hidden_channels=checkpoint[\"hidden_channels\"],\n","        out_channels=checkpoint[\"out_channels\"],\n","        num_layers=checkpoint[\"num_layers\"],\n","        num_heads=checkpoint[\"num_heads\"],\n","        node_types_count=checkpoint[\"node_types_count\"],\n","        treat_type_dim=checkpoint[\"treat_type_dim\"],\n","        gene_edge_dim=checkpoint[\"gene_edge_dim\"],\n","    )\n","\n","    # Load the state dict\n","    model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    model = model.to(device)\n","\n","    # If you need to continue training, you can also load optimizer state\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n","\n","    return model, optimizer, checkpoint[\"epoch\"]\n","\n","\n","\"\"\"\n","# load the trained model\n","model_path = get_file_path(\"trained_hgt_model.pt\")\n","if os.path.exists(model_path):\n","    model, optimizer, start_epoch = load_trained_model(model_path, device)\n","    # model.eval()\n","else:\n","    print(\"No trained model found.\")\n","\"\"\""],"metadata":{"id":"Az1d8t8G7VaR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###################################################\n","# score all compound->disease pairs for diseases in list and rank compounds by score.\n","###################################################\n","\n","COV_disease_list = [\n","    \"Disease::SARS-CoV2 E\",\n","    \"Disease::SARS-CoV2 M\",\n","    \"Disease::SARS-CoV2 N\",\n","    \"Disease::SARS-CoV2 Spike\",\n","    \"Disease::SARS-CoV2 nsp1\",\n","    \"Disease::SARS-CoV2 nsp10\",\n","    \"Disease::SARS-CoV2 nsp11\",\n","    \"Disease::SARS-CoV2 nsp12\",\n","    \"Disease::SARS-CoV2 nsp13\",\n","    \"Disease::SARS-CoV2 nsp14\",\n","    \"Disease::SARS-CoV2 nsp15\",\n","    \"Disease::SARS-CoV2 nsp2\",\n","    \"Disease::SARS-CoV2 nsp4\",\n","    \"Disease::SARS-CoV2 nsp5\",\n","    \"Disease::SARS-CoV2 nsp5_C145A\",\n","    \"Disease::SARS-CoV2 nsp6\",\n","    \"Disease::SARS-CoV2 nsp7\",\n","    \"Disease::SARS-CoV2 nsp8\",\n","    \"Disease::SARS-CoV2 nsp9\",\n","    \"Disease::SARS-CoV2 orf10\",\n","    \"Disease::SARS-CoV2 orf3a\",\n","    \"Disease::SARS-CoV2 orf3b\",\n","    \"Disease::SARS-CoV2 orf6\",\n","    \"Disease::SARS-CoV2 orf7a\",\n","    \"Disease::SARS-CoV2 orf8\",\n","    \"Disease::SARS-CoV2 orf9b\",\n","    \"Disease::SARS-CoV2 orf9c\",\n","    \"Disease::MESH:D045169\",\n","    \"Disease::MESH:D045473\",\n","    \"Disease::MESH:D001351\",\n","    \"Disease::MESH:D065207\",\n","    \"Disease::MESH:D028941\",\n","    \"Disease::MESH:D058957\",\n","    \"Disease::MESH:D006517\",\n","]\n","\n","control_disease_list = [\"Disease::MESH:D015451\", \"Disease::MESH:D053158\", \"Disease::MESH:D001474\"]\n","\n","model.eval()\n","with torch.no_grad():\n","    x_dict = {ntype: data[ntype].x for ntype in data.node_types}\n","    edge_index_dict = {rel: data[rel].edge_index for rel in data.edge_types}\n","    edge_attr_dict = {\n","        rel: (data[rel].edge_attr if hasattr(data[rel], \"edge_attr\") else None)\n","        for rel in data.edge_types\n","    }\n","    node_embs = model(x_dict, edge_index_dict, edge_attr_dict)\n","\n","    # Map disease names to their indices\n","    disease_name_to_id = {}\n","    for n, (t, idx) in node_type_map.items():\n","        if t == \"disease\":\n","            disease_name_to_id[n] = idx\n","\n","    # Map compound indices for convenience\n","    compound_ids = torch.arange(data[\"compound\"].num_nodes, device=device)\n","\n","    for disease_name in COV_disease_list + control_disease_list:\n","        if disease_name not in disease_name_to_id:\n","            # Disease not in graph\n","            print(f\"Disease {disease_name} not found in the graph.\")\n","            continue\n","\n","        disease_id = disease_name_to_id[disease_name]\n","        # Score all compound->disease pairs\n","        # No treat_type edge_attr available for \"all pairs\", so we just don't use it\n","        scores = model.predict_link(\n","            node_embs,\n","            (\"compound\", compound_ids),\n","            (\"disease\", torch.full_like(compound_ids, disease_id)),\n","            None,\n","        )\n","\n","        probs = torch.sigmoid(scores).squeeze()\n","        # Rank compounds by score\n","        topk = 10\n","        top_scores, top_indices = torch.topk(probs, topk)\n","        print(f\"Top {topk} candidate compounds for {disease_name}:\")\n","        # Need to map compound_idx back to compound name\n","        # We'll build a reverse map for compounds:\n","        compound_id_to_name = {}\n","        for n, (t, idx) in node_type_map.items():\n","            if t == \"compound\":\n","                compound_id_to_name[idx] = n\n","\n","        for rank, (score_val, c_idx) in enumerate(\n","            zip(top_scores.tolist(), top_indices.tolist()), start=1\n","        ):\n","            compound_name = compound_id_to_name[c_idx]\n","            print(f\"{rank}: {compound_name} (score={score_val:.4f})\")\n","\n","###################################################\n","# The printed results give a procedure to identify candidate compounds for given COV diseases.\n","# We trained a model and then, for each disease in COV_disease_list, we predict compound rankings.\n","#\n","# This code sets up the entire training pipeline, uses an HGT model, and shows how to get predictions.\n","###################################################"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_VPx61cw1wyK","outputId":"d1b2d073-fa7c-4568-a61f-b9a6d8dbce45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Top 10 candidate compounds for Disease::SARS-CoV2 E:\n","1: Compound::MESH:D013256 (score=0.9601)\n","2: Compound::MESH:D000305 (score=0.9571)\n","3: Compound::MESH:D010406 (score=0.9529)\n","4: Compound::CHEBI:35341 (score=0.9445)\n","5: Compound::DB00207 (score=0.9310)\n","6: Compound::CHEBI:50858 (score=0.9206)\n","7: Compound::DB00563 (score=0.9204)\n","8: Compound::DB00104 (score=0.9181)\n","9: Compound::DB09140 (score=0.9086)\n","10: Compound::DB00741 (score=0.8956)\n","Top 10 candidate compounds for Disease::SARS-CoV2 M:\n","1: Compound::MESH:D013256 (score=0.9381)\n","2: Compound::MESH:D000305 (score=0.9336)\n","3: Compound::MESH:D010406 (score=0.9273)\n","4: Compound::CHEBI:35341 (score=0.9146)\n","5: Compound::DB00207 (score=0.8947)\n","6: Compound::CHEBI:50858 (score=0.8796)\n","7: Compound::DB00563 (score=0.8793)\n","8: Compound::DB00104 (score=0.8759)\n","9: Compound::DB09140 (score=0.8624)\n","10: Compound::DB00741 (score=0.8438)\n","Top 10 candidate compounds for Disease::SARS-CoV2 N:\n","1: Compound::MESH:D013256 (score=0.9209)\n","2: Compound::MESH:D000305 (score=0.9152)\n","3: Compound::MESH:D010406 (score=0.9073)\n","4: Compound::CHEBI:35341 (score=0.8916)\n","5: Compound::DB00207 (score=0.8671)\n","6: Compound::CHEBI:50858 (score=0.8486)\n","7: Compound::DB00563 (score=0.8483)\n","8: Compound::DB00104 (score=0.8442)\n","9: Compound::DB09140 (score=0.8278)\n","10: Compound::DB00741 (score=0.8057)\n","Top 10 candidate compounds for Disease::SARS-CoV2 Spike:\n","1: Compound::MESH:D013256 (score=0.9755)\n","2: Compound::MESH:D000305 (score=0.9736)\n","3: Compound::MESH:D010406 (score=0.9710)\n","4: Compound::CHEBI:35341 (score=0.9657)\n","5: Compound::DB00207 (score=0.9571)\n","6: Compound::CHEBI:50858 (score=0.9504)\n","7: Compound::DB00563 (score=0.9503)\n","8: Compound::DB00104 (score=0.9488)\n","9: Compound::DB09140 (score=0.9427)\n","10: Compound::DB00741 (score=0.9341)\n","Top 10 candidate compounds for Disease::SARS-CoV2 nsp1:\n","1: Compound::MESH:D013256 (score=0.9599)\n","2: Compound::MESH:D000305 (score=0.9569)\n","3: Compound::MESH:D010406 (score=0.9527)\n","4: Compound::CHEBI:35341 (score=0.9442)\n","5: Compound::DB00207 (score=0.9306)\n","6: Compound::CHEBI:50858 (score=0.9202)\n","7: Compound::DB00563 (score=0.9200)\n","8: Compound::DB00104 (score=0.9177)\n","9: Compound::DB09140 (score=0.9082)\n","10: Compound::DB00741 (score=0.8951)\n","Top 10 candidate compounds for Disease::SARS-CoV2 nsp10:\n","1: Compound::MESH:D013256 (score=0.9668)\n","2: Compound::MESH:D000305 (score=0.9642)\n","3: Compound::MESH:D010406 (score=0.9607)\n","4: Compound::CHEBI:35341 (score=0.9536)\n","5: Compound::DB00207 (score=0.9422)\n","6: Compound::CHEBI:50858 (score=0.9334)\n","7: Compound::DB00563 (score=0.9332)\n","8: Compound::DB00104 (score=0.9312)\n","9: Compound::DB09140 (score=0.9232)\n","10: Compound::DB00741 (score=0.9120)\n","Top 10 candidate compounds for Disease::SARS-CoV2 nsp11:\n","1: Compound::MESH:D013256 (score=0.9292)\n","2: Compound::MESH:D000305 (score=0.9240)\n","3: Compound::MESH:D010406 (score=0.9169)\n","4: Compound::CHEBI:35341 (score=0.9027)\n","5: Compound::DB00207 (score=0.8803)\n","6: Compound::CHEBI:50858 (score=0.8634)\n","7: Compound::DB00563 (score=0.8631)\n","8: Compound::DB00104 (score=0.8593)\n","9: Compound::DB09140 (score=0.8443)\n","10: Compound::DB00741 (score=0.8238)\n","Top 10 candidate compounds for Disease::SARS-CoV2 nsp12:\n","1: Compound::MESH:D013256 (score=0.9361)\n","2: Compound::MESH:D000305 (score=0.9314)\n","3: Compound::MESH:D010406 (score=0.9249)\n","4: Compound::CHEBI:35341 (score=0.9119)\n","5: Compound::DB00207 (score=0.8914)\n","6: Compound::CHEBI:50858 (score=0.8758)\n","7: Compound::DB00563 (score=0.8755)\n","8: Compound::DB00104 (score=0.8721)\n","9: Compound::DB09140 (score=0.8582)\n","10: Compound::DB00741 (score=0.8392)\n","Top 10 candidate compounds for Disease::SARS-CoV2 nsp13:\n","1: Compound::MESH:D013256 (score=0.9356)\n","2: Compound::MESH:D000305 (score=0.9309)\n","3: Compound::MESH:D010406 (score=0.9244)\n","4: Compound::CHEBI:35341 (score=0.9113)\n","5: Compound::DB00207 (score=0.8907)\n","6: Compound::CHEBI:50858 (score=0.8750)\n","7: Compound::DB00563 (score=0.8747)\n","8: Compound::DB00104 (score=0.8713)\n","9: Compound::DB09140 (score=0.8573)\n","10: Compound::DB00741 (score=0.8382)\n","Top 10 candidate compounds for Disease::SARS-CoV2 nsp14:\n","1: Compound::MESH:D013256 (score=0.9724)\n","2: Compound::MESH:D000305 (score=0.9703)\n","3: Compound::MESH:D010406 (score=0.9673)\n","4: Compound::CHEBI:35341 (score=0.9614)\n","5: Compound::DB00207 (score=0.9518)\n","6: Compound::CHEBI:50858 (score=0.9443)\n","7: Compound::DB00563 (score=0.9442)\n","8: Compound::DB00104 (score=0.9425)\n","9: Compound::DB09140 (score=0.9357)\n","10: Compound::DB00741 (score=0.9262)\n","Top 10 candidate compounds for Disease::SARS-CoV2 nsp15:\n","1: Compound::MESH:D013256 (score=0.9006)\n","2: Compound::MESH:D000305 (score=0.8936)\n","3: Compound::MESH:D010406 (score=0.8840)\n","4: Compound::CHEBI:35341 (score=0.8649)\n","5: Compound::DB00207 (score=0.8355)\n","6: Compound::CHEBI:50858 (score=0.8136)\n","7: Compound::DB00563 (score=0.8132)\n","8: Compound::DB00104 (score=0.8084)\n","9: Compound::DB09140 (score=0.7892)\n","10: Compound::DB00741 (score=0.7635)\n","Top 10 candidate compounds for Disease::SARS-CoV2 nsp2:\n","1: Compound::MESH:D013256 (score=0.9055)\n","2: Compound::MESH:D000305 (score=0.8988)\n","3: Compound::MESH:D010406 (score=0.8896)\n","4: Compound::CHEBI:35341 (score=0.8713)\n","5: Compound::DB00207 (score=0.8430)\n","6: Compound::CHEBI:50858 (score=0.8219)\n","7: Compound::DB00563 (score=0.8215)\n","8: Compound::DB00104 (score=0.8169)\n","9: Compound::DB09140 (score=0.7984)\n","10: Compound::DB00741 (score=0.7735)\n","Top 10 candidate compounds for Disease::SARS-CoV2 nsp4:\n","1: Compound::MESH:D013256 (score=0.9474)\n","2: Compound::MESH:D000305 (score=0.9435)\n","3: Compound::MESH:D010406 (score=0.9381)\n","4: Compound::CHEBI:35341 (score=0.9272)\n","5: Compound::DB00207 (score=0.9099)\n","6: Compound::CHEBI:50858 (score=0.8966)\n","7: Compound::DB00563 (score=0.8964)\n","8: Compound::DB00104 (score=0.8935)\n","9: Compound::DB09140 (score=0.8816)\n","10: Compound::DB00741 (score=0.8652)\n","Top 10 candidate compounds for Disease::SARS-CoV2 nsp5:\n","1: Compound::MESH:D013256 (score=0.9573)\n","2: Compound::MESH:D000305 (score=0.9541)\n","3: Compound::MESH:D010406 (score=0.9497)\n","4: Compound::CHEBI:35341 (score=0.9407)\n","5: Compound::DB00207 (score=0.9263)\n","6: Compound::CHEBI:50858 (score=0.9153)\n","7: Compound::DB00563 (score=0.9151)\n","8: Compound::DB00104 (score=0.9127)\n","9: Compound::DB09140 (score=0.9027)\n","10: Compound::DB00741 (score=0.8888)\n","Top 10 candidate compounds for Disease::SARS-CoV2 nsp5_C145A:\n","1: Compound::MESH:D013256 (score=0.9661)\n","2: Compound::MESH:D000305 (score=0.9635)\n","3: Compound::MESH:D010406 (score=0.9599)\n","4: Compound::CHEBI:35341 (score=0.9526)\n","5: Compound::DB00207 (score=0.9410)\n","6: Compound::CHEBI:50858 (score=0.9320)\n","7: Compound::DB00563 (score=0.9319)\n","8: Compound::DB00104 (score=0.9298)\n","9: Compound::DB09140 (score=0.9216)\n","10: Compound::DB00741 (score=0.9103)\n","Top 10 candidate compounds for Disease::SARS-CoV2 nsp6:\n","1: Compound::MESH:D013256 (score=0.9701)\n","2: Compound::MESH:D000305 (score=0.9678)\n","3: Compound::MESH:D010406 (score=0.9646)\n","4: Compound::CHEBI:35341 (score=0.9582)\n","5: Compound::DB00207 (score=0.9479)\n","6: Compound::CHEBI:50858 (score=0.9399)\n","7: Compound::DB00563 (score=0.9397)\n","8: Compound::DB00104 (score=0.9379)\n","9: Compound::DB09140 (score=0.9306)\n","10: Compound::DB00741 (score=0.9204)\n","Top 10 candidate compounds for Disease::SARS-CoV2 nsp7:\n","1: Compound::MESH:D013256 (score=0.9182)\n","2: Compound::MESH:D000305 (score=0.9123)\n","3: Compound::MESH:D010406 (score=0.9042)\n","4: Compound::CHEBI:35341 (score=0.8880)\n","5: Compound::DB00207 (score=0.8628)\n","6: Compound::CHEBI:50858 (score=0.8439)\n","7: Compound::DB00563 (score=0.8435)\n","8: Compound::DB00104 (score=0.8394)\n","9: Compound::DB09140 (score=0.8226)\n","10: Compound::DB00741 (score=0.8000)\n","Top 10 candidate compounds for Disease::SARS-CoV2 nsp8:\n","1: Compound::MESH:D013256 (score=0.9487)\n","2: Compound::MESH:D000305 (score=0.9449)\n","3: Compound::MESH:D010406 (score=0.9396)\n","4: Compound::CHEBI:35341 (score=0.9290)\n","5: Compound::DB00207 (score=0.9121)\n","6: Compound::CHEBI:50858 (score=0.8991)\n","7: Compound::DB00563 (score=0.8989)\n","8: Compound::DB00104 (score=0.8960)\n","9: Compound::DB09140 (score=0.8844)\n","10: Compound::DB00741 (score=0.8683)\n","Top 10 candidate compounds for Disease::SARS-CoV2 nsp9:\n","1: Compound::MESH:D013256 (score=0.9739)\n","2: Compound::MESH:D000305 (score=0.9719)\n","3: Compound::MESH:D010406 (score=0.9691)\n","4: Compound::CHEBI:35341 (score=0.9634)\n","5: Compound::DB00207 (score=0.9543)\n","6: Compound::CHEBI:50858 (score=0.9472)\n","7: Compound::DB00563 (score=0.9471)\n","8: Compound::DB00104 (score=0.9455)\n","9: Compound::DB09140 (score=0.9390)\n","10: Compound::DB00741 (score=0.9300)\n","Top 10 candidate compounds for Disease::SARS-CoV2 orf10:\n","1: Compound::MESH:D013256 (score=0.9049)\n","2: Compound::MESH:D000305 (score=0.8982)\n","3: Compound::MESH:D010406 (score=0.8889)\n","4: Compound::CHEBI:35341 (score=0.8706)\n","5: Compound::DB00207 (score=0.8421)\n","6: Compound::CHEBI:50858 (score=0.8209)\n","7: Compound::DB00563 (score=0.8205)\n","8: Compound::DB00104 (score=0.8159)\n","9: Compound::DB09140 (score=0.7973)\n","10: Compound::DB00741 (score=0.7723)\n","Top 10 candidate compounds for Disease::SARS-CoV2 orf3a:\n","1: Compound::MESH:D013256 (score=0.9380)\n","2: Compound::MESH:D000305 (score=0.9334)\n","3: Compound::MESH:D010406 (score=0.9271)\n","4: Compound::CHEBI:35341 (score=0.9145)\n","5: Compound::DB00207 (score=0.8945)\n","6: Compound::CHEBI:50858 (score=0.8793)\n","7: Compound::DB00563 (score=0.8790)\n","8: Compound::DB00104 (score=0.8757)\n","9: Compound::DB09140 (score=0.8621)\n","10: Compound::DB00741 (score=0.8435)\n","Top 10 candidate compounds for Disease::SARS-CoV2 orf3b:\n","1: Compound::MESH:D013256 (score=0.9342)\n","2: Compound::MESH:D000305 (score=0.9294)\n","3: Compound::MESH:D010406 (score=0.9227)\n","4: Compound::CHEBI:35341 (score=0.9094)\n","5: Compound::DB00207 (score=0.8884)\n","6: Compound::CHEBI:50858 (score=0.8725)\n","7: Compound::DB00563 (score=0.8722)\n","8: Compound::DB00104 (score=0.8686)\n","9: Compound::DB09140 (score=0.8544)\n","10: Compound::DB00741 (score=0.8350)\n","Top 10 candidate compounds for Disease::SARS-CoV2 orf6:\n","1: Compound::MESH:D013256 (score=0.9508)\n","2: Compound::MESH:D000305 (score=0.9471)\n","3: Compound::MESH:D010406 (score=0.9420)\n","4: Compound::CHEBI:35341 (score=0.9317)\n","5: Compound::DB00207 (score=0.9154)\n","6: Compound::CHEBI:50858 (score=0.9029)\n","7: Compound::DB00563 (score=0.9027)\n","8: Compound::DB00104 (score=0.8999)\n","9: Compound::DB09140 (score=0.8886)\n","10: Compound::DB00741 (score=0.8731)\n","Top 10 candidate compounds for Disease::SARS-CoV2 orf7a:\n","1: Compound::MESH:D013256 (score=0.8995)\n","2: Compound::MESH:D000305 (score=0.8924)\n","3: Compound::MESH:D010406 (score=0.8827)\n","4: Compound::CHEBI:35341 (score=0.8635)\n","5: Compound::DB00207 (score=0.8337)\n","6: Compound::CHEBI:50858 (score=0.8117)\n","7: Compound::DB00563 (score=0.8113)\n","8: Compound::DB00104 (score=0.8064)\n","9: Compound::DB09140 (score=0.7871)\n","10: Compound::DB00741 (score=0.7612)\n","Top 10 candidate compounds for Disease::SARS-CoV2 orf8:\n","1: Compound::MESH:D013256 (score=0.9112)\n","2: Compound::MESH:D000305 (score=0.9049)\n","3: Compound::MESH:D010406 (score=0.8961)\n","4: Compound::CHEBI:35341 (score=0.8788)\n","5: Compound::DB00207 (score=0.8518)\n","6: Compound::CHEBI:50858 (score=0.8317)\n","7: Compound::DB00563 (score=0.8313)\n","8: Compound::DB00104 (score=0.8269)\n","9: Compound::DB09140 (score=0.8091)\n","10: Compound::DB00741 (score=0.7852)\n","Top 10 candidate compounds for Disease::SARS-CoV2 orf9b:\n","1: Compound::MESH:D013256 (score=0.9091)\n","2: Compound::MESH:D000305 (score=0.9027)\n","3: Compound::MESH:D010406 (score=0.8937)\n","4: Compound::CHEBI:35341 (score=0.8761)\n","5: Compound::DB00207 (score=0.8486)\n","6: Compound::CHEBI:50858 (score=0.8281)\n","7: Compound::DB00563 (score=0.8278)\n","8: Compound::DB00104 (score=0.8233)\n","9: Compound::DB09140 (score=0.8052)\n","10: Compound::DB00741 (score=0.7809)\n","Top 10 candidate compounds for Disease::SARS-CoV2 orf9c:\n","1: Compound::MESH:D013256 (score=0.9287)\n","2: Compound::MESH:D000305 (score=0.9235)\n","3: Compound::MESH:D010406 (score=0.9163)\n","4: Compound::CHEBI:35341 (score=0.9020)\n","5: Compound::DB00207 (score=0.8795)\n","6: Compound::CHEBI:50858 (score=0.8625)\n","7: Compound::DB00563 (score=0.8622)\n","8: Compound::DB00104 (score=0.8584)\n","9: Compound::DB09140 (score=0.8433)\n","10: Compound::DB00741 (score=0.8227)\n","Top 10 candidate compounds for Disease::MESH:D045169:\n","1: Compound::MESH:D013256 (score=0.9799)\n","2: Compound::MESH:D000305 (score=0.9783)\n","3: Compound::MESH:D010406 (score=0.9762)\n","4: Compound::CHEBI:35341 (score=0.9718)\n","5: Compound::DB00207 (score=0.9647)\n","6: Compound::CHEBI:50858 (score=0.9591)\n","7: Compound::DB00563 (score=0.9590)\n","8: Compound::DB00104 (score=0.9578)\n","9: Compound::DB09140 (score=0.9527)\n","10: Compound::DB00741 (score=0.9455)\n","Top 10 candidate compounds for Disease::MESH:D045473:\n","1: Compound::MESH:D013256 (score=0.9458)\n","2: Compound::MESH:D000305 (score=0.9417)\n","3: Compound::MESH:D010406 (score=0.9361)\n","4: Compound::CHEBI:35341 (score=0.9249)\n","5: Compound::DB00207 (score=0.9072)\n","6: Compound::CHEBI:50858 (score=0.8936)\n","7: Compound::DB00563 (score=0.8933)\n","8: Compound::DB00104 (score=0.8903)\n","9: Compound::DB09140 (score=0.8781)\n","10: Compound::DB00741 (score=0.8614)\n","Top 10 candidate compounds for Disease::MESH:D001351:\n","1: Compound::MESH:D013256 (score=0.9410)\n","2: Compound::MESH:D000305 (score=0.9367)\n","3: Compound::MESH:D010406 (score=0.9306)\n","4: Compound::CHEBI:35341 (score=0.9186)\n","5: Compound::DB00207 (score=0.8994)\n","6: Compound::CHEBI:50858 (score=0.8849)\n","7: Compound::DB00563 (score=0.8846)\n","8: Compound::DB00104 (score=0.8814)\n","9: Compound::DB09140 (score=0.8683)\n","10: Compound::DB00741 (score=0.8504)\n","Top 10 candidate compounds for Disease::MESH:D065207:\n","1: Compound::MESH:D013256 (score=0.9297)\n","2: Compound::MESH:D000305 (score=0.9246)\n","3: Compound::MESH:D010406 (score=0.9175)\n","4: Compound::CHEBI:35341 (score=0.9034)\n","5: Compound::DB00207 (score=0.8812)\n","6: Compound::CHEBI:50858 (score=0.8643)\n","7: Compound::DB00563 (score=0.8640)\n","8: Compound::DB00104 (score=0.8603)\n","9: Compound::DB09140 (score=0.8454)\n","10: Compound::DB00741 (score=0.8250)\n","Top 10 candidate compounds for Disease::MESH:D028941:\n","1: Compound::MESH:D013256 (score=0.9628)\n","2: Compound::MESH:D000305 (score=0.9600)\n","3: Compound::MESH:D010406 (score=0.9561)\n","4: Compound::CHEBI:35341 (score=0.9482)\n","5: Compound::DB00207 (score=0.9355)\n","6: Compound::CHEBI:50858 (score=0.9258)\n","7: Compound::DB00563 (score=0.9256)\n","8: Compound::DB00104 (score=0.9234)\n","9: Compound::DB09140 (score=0.9145)\n","10: Compound::DB00741 (score=0.9022)\n","Top 10 candidate compounds for Disease::MESH:D058957:\n","1: Compound::MESH:D013256 (score=0.9165)\n","2: Compound::MESH:D000305 (score=0.9105)\n","3: Compound::MESH:D010406 (score=0.9022)\n","4: Compound::CHEBI:35341 (score=0.8858)\n","5: Compound::DB00207 (score=0.8601)\n","6: Compound::CHEBI:50858 (score=0.8409)\n","7: Compound::DB00563 (score=0.8405)\n","8: Compound::DB00104 (score=0.8363)\n","9: Compound::DB09140 (score=0.8193)\n","10: Compound::DB00741 (score=0.7963)\n","Top 10 candidate compounds for Disease::MESH:D006517:\n","1: Compound::MESH:D013256 (score=0.9197)\n","2: Compound::MESH:D000305 (score=0.9139)\n","3: Compound::MESH:D010406 (score=0.9059)\n","4: Compound::CHEBI:35341 (score=0.8900)\n","5: Compound::DB00207 (score=0.8652)\n","6: Compound::CHEBI:50858 (score=0.8465)\n","7: Compound::DB00563 (score=0.8462)\n","8: Compound::DB00104 (score=0.8421)\n","9: Compound::DB09140 (score=0.8255)\n","10: Compound::DB00741 (score=0.8032)\n","Top 10 candidate compounds for Disease::MESH:D015451:\n","1: Compound::MESH:D013256 (score=1.0000)\n","2: Compound::MESH:D000305 (score=1.0000)\n","3: Compound::MESH:D010406 (score=1.0000)\n","4: Compound::CHEBI:35341 (score=1.0000)\n","5: Compound::DB00207 (score=0.9999)\n","6: Compound::CHEBI:50858 (score=0.9999)\n","7: Compound::DB00563 (score=0.9999)\n","8: Compound::DB00104 (score=0.9999)\n","9: Compound::DB09140 (score=0.9999)\n","10: Compound::DB00741 (score=0.9999)\n","Top 10 candidate compounds for Disease::MESH:D053158:\n","1: Compound::MESH:D013256 (score=0.9996)\n","2: Compound::MESH:D000305 (score=0.9996)\n","3: Compound::MESH:D010406 (score=0.9996)\n","4: Compound::CHEBI:35341 (score=0.9995)\n","5: Compound::DB00207 (score=0.9993)\n","6: Compound::CHEBI:50858 (score=0.9992)\n","7: Compound::DB00563 (score=0.9992)\n","8: Compound::DB00104 (score=0.9992)\n","9: Compound::DB09140 (score=0.9991)\n","10: Compound::DB00741 (score=0.9989)\n","Top 10 candidate compounds for Disease::MESH:D001474:\n","1: Compound::MESH:D013256 (score=0.9950)\n","2: Compound::MESH:D000305 (score=0.9946)\n","3: Compound::MESH:D010406 (score=0.9940)\n","4: Compound::CHEBI:35341 (score=0.9929)\n","5: Compound::DB00207 (score=0.9911)\n","6: Compound::CHEBI:50858 (score=0.9896)\n","7: Compound::DB00563 (score=0.9896)\n","8: Compound::DB00104 (score=0.9893)\n","9: Compound::DB09140 (score=0.9879)\n","10: Compound::DB00741 (score=0.9860)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","\n","# known treatments and phases\n","\n","def parse_phase(phase_str):\n","    \"\"\"Parse phase string or int into a numeric value.\"\"\"\n","    if isinstance(phase_str, (int, float)):\n","        return float(phase_str)\n","    if not isinstance(phase_str, str) or phase_str == \"Not Available\":\n","        return None\n","\n","    # Try parsing comma-separated phases\n","    parts = phase_str.split(\",\")\n","    numeric_parts = []\n","    for p in parts:\n","        p = p.strip()\n","        try:\n","            numeric_parts.append(float(p))\n","        except ValueError:\n","            continue\n","\n","    return sum(numeric_parts) / len(numeric_parts) if numeric_parts else None\n","\n","known_treatments = {d: {} for d in COV_disease_list}\n","trial_data = []\n","\n","# collect treatment and trial data\n","for c_node, attrs in G.nodes(data=True):\n","    if c_node.startswith(\"Compound::\"):\n","        treatment_info = attrs.get(\"treatment_data\", {})\n","\n","        # store treatment info if available\n","        if treatment_info:\n","            for d in COV_disease_list:\n","                known_treatments[d][c_node] = treatment_info\n","\n","        # extract and store trial phase data\n","        ect_data = treatment_info.get(\"External Clinical Trials\", {})\n","        if ect_data:\n","            phase = ect_data.get(\"Phase\", \"Not Available\")\n","            parsed_phase = parse_phase(phase)\n","            if parsed_phase is not None:\n","                trial_data.append(\n","                    {\n","                        \"compound\": c_node,\n","                        \"phase\": parsed_phase,\n","                        \"phase_raw\": phase,\n","                        \"is_experimental\": \"Experimental Unapproved\" in treatment_info,\n","                    }\n","                )\n","\n","trial_df = pd.DataFrame(trial_data)\n","median_phase = trial_df[\"phase\"].median() if not trial_df.empty else 0.0\n","\n","###################################################\n","# evaluate model predictions\n","###################################################\n","\n","\n","def evaluate_predictions(\n","    COV_disease_list,\n","    model,\n","    data,\n","    node_type_map,\n","    known_treatments,\n","    trial_df,\n","    median_phase,\n","    device,\n","    topk=10,\n","):\n","    model.eval()\n","    with torch.no_grad():\n","        x_dict = {ntype: data[ntype].x for ntype in data.node_types}\n","        edge_index_dict = {rel: data[rel].edge_index for rel in data.edge_types}\n","        edge_attr_dict = {\n","            rel: (data[rel].edge_attr if hasattr(data[rel], \"edge_attr\") else None)\n","            for rel in data.edge_types\n","        }\n","        node_embs = model(x_dict, edge_index_dict, edge_attr_dict)\n","\n","        # Maps\n","        disease_name_to_id = {\n","            n: idx for n, (t, idx) in node_type_map.items() if t == \"disease\"\n","        }\n","        compound_id_to_name = {\n","            idx: n for n, (t, idx) in node_type_map.items() if t == \"compound\"\n","        }\n","\n","        compound_ids = torch.arange(data[\"compound\"].num_nodes, device=device)\n","\n","        total_correct = 0\n","        total_predictions = 0\n","        total_weighted_correct = 0.0\n","        total_weighted = 0.0\n","\n","        trial_lookup = {}\n","        if not trial_df.empty:\n","            for _, row in trial_df.iterrows():\n","                trial_lookup[row[\"compound\"]] = {\n","                    \"phase\": row[\"phase\"],\n","                    \"is_experimental\": row[\"is_experimental\"],\n","                }\n","\n","        for disease_name in COV_disease_list:\n","            if disease_name not in disease_name_to_id:\n","                continue\n","\n","            disease_id = disease_name_to_id[disease_name]\n","            scores = model.predict_link(\n","                node_embs,\n","                (\"compound\", compound_ids),\n","                (\"disease\", torch.full_like(compound_ids, disease_id)),\n","                None,\n","            )\n","\n","            probs = torch.sigmoid(scores).squeeze()\n","            top_scores, top_indices = torch.topk(probs, topk)\n","\n","            top_scores = top_scores.cpu().tolist()\n","            top_indices = top_indices.cpu().tolist()\n","\n","            known_for_disease = known_treatments.get(disease_name, {})\n","\n","            for score_val, c_idx in zip(top_scores, top_indices):\n","                compound_name = compound_id_to_name[c_idx]\n","                total_predictions += 1\n","                correct = compound_name in known_for_disease\n","\n","                # compute weight starting with base score\n","                weight = score_val\n","\n","                # apply trial data modifiers if available\n","                trial_info = trial_lookup.get(compound_name, {})\n","                if trial_info:\n","                    if trial_info[\"is_experimental\"]:\n","                        weight *= 1.5\n","\n","                    phase = trial_info.get(\"phase\", median_phase)\n","                    weight *= 1 + phase / 10.0\n","\n","                if correct:\n","                    total_correct += 1\n","                    total_weighted_correct += weight\n","                total_weighted += weight\n","\n","        accuracy = total_correct / total_predictions if total_predictions > 0 else 0.0\n","        weighted_accuracy = (\n","            total_weighted_correct / total_weighted if total_weighted > 0 else 0.0\n","        )\n","\n","        print(f\"Accuracy: {accuracy:.4f}\")\n","        print(f\"Weighted Accuracy: {weighted_accuracy:.4f}\")\n","\n","        return accuracy, weighted_accuracy\n","\n","\n","###################################################\n","accuracy, weighted_accuracy = evaluate_predictions(\n","    COV_disease_list,\n","    model,\n","    data,\n","    node_type_map,\n","    known_treatments,\n","    trial_df,\n","    median_phase,\n","    device,\n","    topk=20,\n",")\n","###################################################"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jkC6gtyk1xUz","outputId":"c480e6f6-53ea-4cd7-c61f-6705925b1ae0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.4500\n","Weighted Accuracy: 0.5277\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"pEbiy2xys4oh"},"execution_count":null,"outputs":[]}]}